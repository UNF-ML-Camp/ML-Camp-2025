{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beskrivelse\n",
    "Her gives det det samme kode som i Eksemple_CNN.ipynb. Modelen der kunne når et test accuracy på 95-96%. Her skal vi undersøg om vi kan lave modellen bedre, eller dårligere, og se hvordan at ændre de forskellige parametere har effekt på resultatet. Først starter vi med at kopiere hjælpe kode filerne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    !cp -r \"/content/drive/MyDrive/Colab Notebooks/ML-Camp-2025_CNN_Dev/ConvNets/utils\" .\n",
    "except ImportError:\n",
    "    print(\"Not in colab. Skipping copying the utilities files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derefter importerer vi pakker og overfører datasætet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from utils.train import train, plot_training_logs\n",
    "from utils.test import test\n",
    "from utils.options import Hyperparameters, name_generator\n",
    "\n",
    "# Tjekke om der er GPU ellers bruge CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Denne transform funktion gives til datasættet for at billederne kommer ud i den rigtig format, som er matricer med værdier mellem 0 og 1.\n",
    "# Vi normalisere pixlerne fra [0, 255] til [0, 1] fordi mest ML algoritmer er bygget til at arbejde bedst med normaliseret data.\n",
    "def image_transform(img):\n",
    "    return torchvision.transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "# Overfører CIFAR10 træning og test datasætene fra pytorch\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=image_transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=image_transform)\n",
    "\n",
    "# Splitter træning sætet til en træning og validering sæt.\n",
    "# val_set_ratio bestemmer hvor meget af sættet bliver brugt til validering.\n",
    "val_set_ratio = 0.1\n",
    "train_set, val_set = random_split(train_set, [int(len(train_set)*(1-val_set_ratio)), int(len(train_set)*val_set_ratio)])\n",
    "\n",
    "# Tjekker størrelsen af billederne og hvor mange klasser der er\n",
    "print(\"Images shape:\", train_set[0][0].shape)\n",
    "print(\"Number of classes:\", len(np.unique(test_set.targets)))\n",
    "\n",
    "# Model / data parameter\n",
    "num_classes = 10\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "# Laver dataloadere der samler vores data i batches og shuffler dem hvis vi vil gerne\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Køre den initial model igen så du kan sammenlign med den senere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 12, 12)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"base_model\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgaver: Undersøg forskellige modeller\n",
    "Igennem disse opgaver vil du finde instruktioner i øverest, og så vil du finde steder hvor der står \"fix\" eller \"NOTE:, som kan hjælpe dig med at kigge efter hvor du skal ændre i koden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution lagene\n",
    "Nu skal du ændre parameter i modellen og se hvis de har en effekt på performance (val_accuracy) og træning hastighed (ms/step). Første prøv at ændre antal af convolution lag. Den første convolution lag med dens relu aktivering er blevet fjernet. Fix koden og se hvad der sker med validering accuracy. **Husk at opdatere input størrelsen for conv2 og kerne størrelse + stride for conv3**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        #self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(input_channels, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 13, 13)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        #x = self.conv1(x)\n",
    "        #x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"no_conv1\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu er den anden convolution lag fjernet istedet med dens relu aktivering. **Husk at opdatere kerne størrelse + stride for conv3**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 26, 26)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.conv2(x)\n",
    "        #x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecture.to(device)ne fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"no_conv2\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad hvis du sætter et ekstra convolution lag. Initialisere en ny Conv2D og sæt den efter MaxPooling2D. Du kan selv vælge parametrene af laget, og **husk at opdaterer input størrelse + kerne størrelse + stride for conv3**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        # NOTE: Indsæt en ekstra convolution lag her. I kan kalde den self.conv_extra eller hvad som helst\n",
    "        self.convExtra = nn.Conv2d(64, 96, 5, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(96, 128, 8, 8)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        # NOTE: Pass x igennem den ny convolution lag her\n",
    "        x = self.convExtra(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"ekstra_conv\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign hastighed og accuracy forskel mellem de tre sidste modeller og den basal model. Var de bedre eller dårligere? Hurtigere eller langsomere? Hvorfor tror du det?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling lagene\n",
    "Nu kigger vi på max pooling. Den ene max pooling lag er blevet fjernet fra modellen. Fix koden og se hvad der sker med hastighed og accuracy. **Husk at opdaterer kerne størrelse + stride for conv3**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 24, 24)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"no_max_pool\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu prøve at sætte et ekstre max pooling lag efter conv1 og dens aktivering. **Husk at opdaterer kerne størrelse + stride for conv3**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, 5)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        # NOTE: Lav en max_pool2d her\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"extra_max_pool\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad hvis vi bruger større kerner? Prøv kerne størrelse på 4 istedet for 2 i de to max pooling lage. Sådan at billederne bliver mindre. **Husk at opdaterer kerne størrelse + stride for conv3**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 4)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 4)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"extra_max_pool4\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign de sidste tre modeler der bruger forskellige max pooling lag med den initial model. Hvorfor tror du der er forskel i træning hastighed og accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivering funktionet\n",
    "Aktivering funktionerne er brugt til at gør sikkert at modelen kan også lære mønstre på ulineart data, da en model uden aktivering kan kun finde lineart afhængigheder mellem dataen. Hvis man fjerner alle aktivering funktioner, så bliver det til linær aktiverin, da ingen aktivering er bare $f(x)=x$, som er linear aktiveringsfunktion.\n",
    "\n",
    "Man kan også skift aktivering funktion til en anden ved at bruge et andet funktion end F.relu. Man kan finde resten af funktionerne [her](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions).\n",
    "\n",
    "Træn en model der bruger kun linear aktivering (ingen aktivering), fjern ikke F.log_softmax, fordi den bruges til at lave netværkets output vektor til en sandsynlighedsvektor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 12, 12)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        # NOTE: Fjern de linjer som kører relu aktivering\n",
    "        x = self.conv1(x)\n",
    "        #x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        #x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        #x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"linear_model\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der findes også andre ikke lineart funktioner, prøv sigmoid og tanh aktivering funktionerne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 12, 12)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        # NOTE: Skift F.relu med F.sigmoid eller F.tanh\n",
    "        x = self.conv1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"different_activation1\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 12, 12)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        # NOTE: Skift F.relu med F.tanh eller F.sigmoid\n",
    "        x = self.conv1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.tanh(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"different_activation2\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign de forskellige aktiveringsfunktioner. Skal man altid bruge et non-lineart aktiveringsfunktion? Får man hurtigere træning ved at bruge lineart aktivering istedet for ikke lineart aktivering? Er de andre aktivering funktioner så god som RELU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgave: Undersøg træning parameter\n",
    "Det er ikke kun modellen der er vigtig for at få god resultater. Selve træning processen er også vigtigt.\n",
    "\n",
    "Epochen bestemer hvor langt modellen skal træn, for mange epocher kan resulterer i overfitting, og for færre epocher resulterer i underfitting.\n",
    "\n",
    "Batch størrelsen bestemmer hvor mange data punkter man bruge til at adjusterer vægtene med og hvor mange steps man tager per epoch som bestemmer hvor hurtigt vægtene ændres. Større batches resulterer i mere præcise vægt ændringer, men kan også resultarer i langsommere træning.\n",
    "\n",
    "Der også findes mange forskellige optimering algoritmer der opdaterer vægtene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antal Epocher\n",
    "Nu skal du undersøg om de førrig modeller underfitter. Prøv at øve antal epocher og se hvordan accuracy ændres. Pas på med at gå alt for stort, da flere epocher tager mere tid til at træne prøv at ikke gå over 80:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 12, 12)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 50,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"different_epochs\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Et måde at finde en god epoch mængde er at starte med et stort epoch tal, og så se hvornår validering accuracy starter med at ændrer sig for lidt mellem hvert epoch (fx. 0.005). Dette strategi kaldes for early stopping, og den bruges for at gøre sikker at modellen træner ikke mere end den har bruge for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad fandt du til at være en god mængde af epocher? Giver det mening at bruge det ekstra tid til at for den mængde øvede accuracy man får? Hvad hvis din model tog 10 minutter eller en time per epoch? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andre optimering algoritmer\n",
    "Lige nu bruges der SGD som optimering algoritm. Der andre optimizers i pytorch som kan ses [her](https://pytorch.org/docs/stable/optim.html#algorithms) prøv at bruge adam istedet for SGD, som er nyere og er den mest brugt inden for ML lige nu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 12, 12)\n",
    "        self.conv4 = nn.Conv2d(128, num_classes, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.mean(dim=(2,3))\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.Adam\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"adam_optimizer\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Var Adam bedre end SGD? Har Adam bruge for så mange Epochs som SGD?\n",
    "\n",
    "Prøv at træne modellen for 50 epochs. Brug ML-flow for at se om Adam overfitter datasættet. Det kan man se ved at kigge på hvordan træning loss og validering loss udvikler sig under træning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgave: En Sidste Model\n",
    "Nu har du prøvet mange forskellige modeller. Prøv at træne en model med de parametere du tror vil virke bedste og træn det. Når du tror at du har trænet den bedste model, så køre test kodeblokken og se hvor god din model kan detekterer hånd skrevet nummer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_channels = 1, num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # NOTE: Initialiserer model lag (Husk at slette raise linjen når du færdig)\n",
    "        raise NotImplementedError(\"Implementer Netværksarkitektur.\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        # NOTE: Implementere forward pass (Husk at slette raise linjen når du færdig)\n",
    "        raise NotImplementedError(\"Implementer forward pass\")\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag (Husk at slette raise linjen når du færdig)\n",
    "        self.input_shape = input_shape\n",
    "        raise NotImplementedError(\"Implementer Netværksarkitektur.\")\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        # Implementere forward pass (Husk at slette raise linjen når du færdig)\n",
    "        raise NotImplementedError(\"Implementer forward pass\")\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 20,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = \"Final_model\",\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "logs = train(train_loader, val_loader, model)\n",
    "\n",
    "Fig, ax = plot_training_logs(logs)\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VENT!\n",
    "Husk at man burde kun bruge test datasætet en gang til sidste. Er du sikker at din nuværende model er den du vil gerne teste på?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"saved_models/Final_model_best.pt\"\n",
    "model = torch.jit.load(model_path, map_location=device)\n",
    "test(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvor meget bedre er denne model end den initial model i Eksemple_CNN.ipynb?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
