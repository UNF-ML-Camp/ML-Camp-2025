{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beskrivelse\n",
    "Her gives det det samme kode som i Eksemple_CNN.ipynb. Modelen der kunne når et test accuracy på 95%. Her skal vi undersøg om vi kan lave modellen bedre, eller dårligere, og se hvordan at ændre de forskellige parametere har effekt på resultatet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Først importerer vi pakker og overfører datasætet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from CNN_utils.train import train\n",
    "from CNN_utils.test import test\n",
    "from CNN_utils.options import Hyperparameters, name_generator\n",
    "\n",
    "# Denne transform funktion gives til datasættet for at billederne kommer ud i den rigtig format, som er matricer med værdier mellem 0 og 1.\n",
    "# Vi normalisere pixlerne fra [0, 255] til [0, 1] fordi mest ML algoritmer er bygget til at arbejde bedst med normaliseret data.\n",
    "def image_transform(img):\n",
    "    return torchvision.transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "# Overfører CIFAR10 træning og test datasætene fra pytorch\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=image_transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=image_transform)\n",
    "\n",
    "# Splitter træning sætet til en træning og validering sæt.\n",
    "# val_set_ratio bestemmer hvor meget af sættet bliver brugt til validering.\n",
    "val_set_ratio = 0.1\n",
    "train_set, val_set = random_split(train_set, [int(len(train_set)*(1-val_set_ratio)), int(len(train_set)*val_set_ratio)])\n",
    "\n",
    "# Tjekker størrelsen af billederne og hvor mange klasser der er\n",
    "print(\"Images shape:\", train_set[0][0].shape)\n",
    "print(\"Number of classes:\", len(np.unique(test_set.targets)))\n",
    "\n",
    "# Model / data parameter\n",
    "num_classes = 10\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "# Laver dataloadere der samler vores data i batches og shuffler dem hvis vi vil gerne\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Køre den initial model igen så du kan sammenlign med den senere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgaver: Undersøg forskellige modeller\n",
    "### Convolution lagene\n",
    "Nu skal du ændre parameter i modellen og se hvis de har en effekt på performance (val_accuracy) og træning hastighed (ms/step). Første prøv at ændre antal af convolution lag. Fjern den første convolution lag med dens relu aktivering og se hvad der sker med validering accuracy. **Husk at opdatere input størrelsen for conv2 og fc1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(fix, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(fix, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu prøve at fjerne den anden convolution lag istedet med dens relu aktivering. **Husk at opdatere input størrelsen for fc1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(fix, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad hvis du sætter et ekstra convolution lag. Initialisere en ny Conv2D og sæt den efter MaxPooling2D. Du kan selv vælge parametrene af laget, og **husk at opdaterer input størrelsen for fc1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(fix, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign hastighed og accuracy forskel mellem de tre sidste modeller og den basal model. Var de bedre eller dårligere? Hurtigere eller langsomere? Hvorfor tror du det?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling lagene\n",
    "Nu kigger vi på max pooling. Først prøv at fjern alle max pooling lagene og se hvad der sker med hastighed og accuracy. **Husk at opdaterer input størrelsen for fc1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(fix, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu prøve at set et ekstre max pooling lag efter conv1 og dens aktivering. **Husk at opdaterer input størrelsen for fc1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(fix, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad hvis vi bruger større kerner? Prøv kerne størrelse på 4 istedet for 2 i de to max pooling lage. Sådan at billederne bliver mindre. **Husk at opdaterer input størrelsen for fc1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(fix, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign de sidste tre modeler der bruger forskellige max pooling lag med den initial model. Hvorfor tror du der er forskel i træning hastighed og accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivering funktionet\n",
    "Aktivering funktionerne er brugt til at gør sikkert at modelen kan også lære mønstre på ulineart data, da en model uden aktivering kan kun finde lineart afhængigheder mellem dataen. Hvis man fjerner alle aktivering funktioner, så bliver det til linær aktiverin, da ingen aktivering er bare $f(x)=x$, som er linear aktiveringsfunktion.\n",
    "\n",
    "Man kan også skift aktivering funktion til en anden ved at bruge et andet funktion end F.relu. Man kan finde resten af funktionerne [her](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions).\n",
    "\n",
    "Træn en model der bruger kun linear aktivering (ingen aktivering), fjern ikke F.log_softmax, fordi den bruges til at lave netværkets output vektor til en sandsynlighedsvektor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der findes også andre ikke lineart funktioner, prøv sigmoid og tanh aktivering funktionerne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign de forskellige aktiveringsfunktioner. Skal man altid bruge et non-lineart aktiveringsfunktion? Får man hurtigere træning ved at bruge lineart aktivering istedet for ikke lineart aktivering? Er de andre aktivering funktioner så god som RELU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgave: Undersøg træning parameter\n",
    "Det er ikke kun modellen der er vigtig for at få god resultater. Selve træning processen er også vigtigt.\n",
    "\n",
    "Epochen bestemer hvor langt modellen skal træn, for mange epocher kan resulterer i overfitting, og for færre epocher resulterer i underfitting.\n",
    "\n",
    "Batch størrelsen bestemmer hvor mange data punkter man bruge til at adjusterer vægtene med og hvor mange steps man tager per epoch som bestemmer hvor hurtigt vægtene ændres. Større batches resulterer i mere præcise vægt ændringer, men kan også resultarer i langsommere træning.\n",
    "\n",
    "Der også findes mange forskellige optimering algoritmer der opdaterer vægtene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antal Epocher\n",
    "Nu skal du undersøg om de førrig modeller underfitter. Prøv at øve antal epocher og se hvordan accuracy ændres. Pas på med at gå alt for stort, da flere epocher tager mere tid til at træne prøv at ikke gå over 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5, # Dis one\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Et måde at finde en god epoch mængde er at starte med et stort epoch tal, og så se hvornår validering accuracy starter med at ændrer sig for lidt mellem hvert epoch (fx. 0.1). Dette strategi kaldes for early stopping, og den bruges for at gøre sikker at modellen træner ikke mere end den har bruge for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad fandt du til at være en god mængde af epocher? Giver det mening at bruge det ekstra tid til at for den mængde øvede accuracy man får? Hvad hvis din model tog 10 minutter eller en time per epoch? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andre optimering algoritmer\n",
    "Lige nu bruges der SGD som optimering algoritm. Der andre optimizers i pytorch som kan ses [her](https://pytorch.org/docs/stable/optim.html#algorithms) prøv at bruge adam istedet for SGD, som er nyere og er den mest brugt inden for ML lige nu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD, # Dis one\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Var Adam bedre end SGD? Har Adam bruge for så mange Epochs som SGD?\n",
    "\n",
    "Prøv at træne modellen for 20 epochs. Brug ML-flow for at se om Adam overfitter datasættet. Det kan man se ved at kigge på hvordan træning loss og validering loss udvikler sig under træning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgave: En Sidste Model\n",
    "Nu har du prøvet mange forskellige modeller. Prøv at træne en model med de parametere du tror vil virke bedste og træn det. Når du tror at du har trænet den bedste model, så køre test kodeblokken og se hvor god din model kan detekterer hånd skrevet nummer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag (Husk at slette raise linjen når du færdig)\n",
    "        self.input_shape = input_shape\n",
    "        raise NotImplementedError(\"Implementer Netværksarkitektur.\")\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        # Implementere forward pass (Husk at slette raise linjen når du færdig)\n",
    "        raise NotImplementedError(\"Implementer forward pass\")\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VENT!\n",
    "Husk at man burde kun bruge test datasætet en gang til sidste. Er du sikker at din nuværende model er den du vil gerne teste på?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9908439517021179\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvor meget bedre er denne model end den initial model i Eksemple_CNN.ipynb?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
