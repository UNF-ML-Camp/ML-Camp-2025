{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opgaver til Neurale Netværk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from time import perf_counter\n",
    "np.random.seed(2024)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 1: Low-level implementering af et feedforward neuralt netværk\n",
    "I kapitel 4 i kompendiet introduceres feedforward neurale netværk. I denne opgave skal du implementere et simpelt feedforward neuralt netværk fra bunden ved hjælp af numpy. Bagefter vil vi bruge mere effektive biblioteker som PyTorch til at implementere neurale netværk.\n",
    "\n",
    "#### Opgave 1.1: Initiering af et neuralt netværk\n",
    "Nedenunder er givet en kode struktur for at initialisere et neuralt netværk. Udfyld de manglende linjer i koden, således at netværket kan initialiseres med de ønskede antal lag og antal neuroner i hvert lag. Vi antager at alle neuroner skal initieres med tilfældige vægte.\n",
    "\n",
    "For at afprøve din implementering, skal du bede den endelige kode om at initialisere netværket vist i figur 4.2 i kompendiet. Dette netværk har 2 lag, hvor det første lag har 3 neuroner, og det andet lag har 1 neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vægte(dim_in: int, dim_out: int):\n",
    "    W = np.random.randn(dim_in, dim_out)\n",
    "    b = np.random.randn(dim_out)\n",
    "    return W, b\n",
    "\n",
    "def init_NN(X_dim: int, L: list[int]):\n",
    "    vægte = []\n",
    "    bias = []\n",
    "\n",
    "    for i, lag in enumerate(L):\n",
    "        if i == 0:\n",
    "            W, b = init_vægte(X_dim, lag)\n",
    "        else:\n",
    "            W, b = init_vægte(L[i-1], lag)\n",
    "        vægte.append(W)\n",
    "        bias.append(b)\n",
    "\n",
    "    return vægte, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_NN_params(vægte: list, biases: list):\n",
    "    print(f\"Netværket har {len(vægte)} lag, med hhv. {', '.join([f'{vægte[i].shape[1]} neuroner i lag ({i+1})' for i in range(len(vægte))])}\")\n",
    "    print()\n",
    "    for i, (vægt, bias) in enumerate(zip(vægte, biases)):\n",
    "        print(f'W^({i+1}):')\n",
    "        print(vægt)\n",
    "        print(f'b^({i+1}):')\n",
    "        print(bias)\n",
    "        print()\n",
    "\n",
    "print_NN_params(*init_NN(X_dim= ???, L = ???))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 1.2: Aktiveringsfunktioner\n",
    "Nedenfor er givet en kode struktur for at implementere nogle af de mest almindelige aktiveringsfunktioner. Udfyld de manglende linjer i koden, således at aktiveringsfunktionerne kan bruges i netværket. Når du mener du har implementeret aktiveringsfunktionerne korrekt, kan du prøve at sammenligne dine resultat-plots i bunden med plotsne på slides. Ligner dine plots dem i kompendiet?\n",
    "\n",
    "*Hint: Hvis du har brug for hjælp til at implementere aktiveringsfunktionerne, kan du kigge i slides*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(z: np.ndarray) -> np.ndarray:\n",
    "    return # TODO: Implementer ReLu\n",
    "\n",
    "def tanh(z: np.ndarray) -> np.ndarray:\n",
    "    return # TODO: Implementer tanh\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return # TODO: Implementer sigmoid\n",
    "\n",
    "def leaky_ReLu(z: np.ndarray, alpha: float = 0.1) -> np.ndarray:\n",
    "    return # TODO: Implementer leaky ReLu\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    return # TODO: Implementer softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aktiverings_funktioner = [tanh, sigmoid, ReLu, leaky_ReLu]\n",
    "fig, ax = plt.subplots(1, len(aktiverings_funktioner), figsize=(15, 4), sharey=True, layout='tight')\n",
    "ax[0].set_ylabel('$f(z)$')\n",
    "z = np.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "for i, f in enumerate(aktiverings_funktioner):\n",
    "    ax[i].plot(z, f(z))\n",
    "    ax[i].plot(z, f(z, return_derivative=True), '--', color='tab:blue')\n",
    "    ax[i].set_title(f.__name__)\n",
    "    ax[i].plot(z, np.zeros_like(z), 'k--', linewidth=0.5)\n",
    "    ax[i].plot(np.zeros_like(z), z, 'k--', linewidth=0.5)\n",
    "    ax[i].set_ylim([-2, 2])\n",
    "    ax[i].set_xlim([-5, 5])\n",
    "    ax[i].grid(True)\n",
    "    ax[i].set_xlabel('$z$')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 1.3: Forward pass\n",
    "Vi er nu klar til at koble netværket sammen. I kompendiet benævnte vi grunden bag at feed forward netværk hedder feed forward; Informationen rejser forward igennem netværket. Nedenfor er givet en kode struktur for at implementere et **forward pass** i et neuralt netværk. Udfyld de manglende linjer i koden, således at forward pass kan udføres. Når du mener du har implementeret forward pass korrekt, kan du prøve at køre understående kode, der initialisere et netværk og benytter de forskellige aktiveringsfunktioner og udfører et forward pass med inputtet\n",
    "\n",
    "$$\\mathbf{X}=\\begin{bmatrix} 0.5 & 0.1 \\\\ 0.4 & 2.1 \\\\ -1.0 & -0.8 \\\\ 0.6 & -1.9 \\end{bmatrix}$$\n",
    "\n",
    "Hvilket output får du? Hvis du tænker over dimensionerne på inputtet, dimensionerne på outputtet og de forskellige aktiveringsfunktioner, kan du så forklare hvorfor du får det output du får og hvad outputtet fortæller dig?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X: np.ndarray, vægte: list, biases: list, aktiverings_funks: list) -> np.ndarray:\n",
    "    a = X\n",
    "    for i, (W, b, f) in enumerate(zip(vægte, biases, aktiverings_funks)):\n",
    "        #TODO: Lav hver iteration i feed-forward netværket\n",
    "        pass\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.2, 0.8], [2.5, 3], [0.3, -0.9],[4, 3.5]])\n",
    "vægte, biases = init_NN(X_dim=2, L=[3, 3, 3, 3, 2])\n",
    "fs = [tanh, ReLu, leaky_ReLu, ReLu, softmax]\n",
    "print(forward(X, vægte, biases, fs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 1.4: Klassifikation\n",
    "I denne opgave skal du implementere en klassifikationsfunktion, som kan bruges til at klassificere outputtet fra netværket. Koden er givet nedenfor. Udfyld de manglende linjer i koden, således at klassifikationsfunktionen kan bruges til at klassificere outputtet fra netværket. Når du mener du har implementeret klassifikationsfunktionen korrekt, kan du prøve at køre koden derunder. Hvilke af de fire datapunkter i $\\mathbf{X}$ klassificeres som klasse 1 og hvilke klassificeres som klasse 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(prob: np.ndarray) -> np.ndarray:\n",
    "    return np.argmax(prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = forward(X, vægte, biases, fs)\n",
    "y = get_label(y_prob)\n",
    "print(f\"Klassifikation: {y}\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ikke overraskende er det neurale netværk ikke særlig god til at klassificere data, da vi ikke har trænet netværket endnu. Det er ikke trænet. Det vil vi senere kigge på."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 1.5: Loss funktion\n",
    "For at kunne træne netværket, må vi starte med at implementere loss functionen. Nedenfor er givet en kode struktur for at implementere en række forskellige loss funktioner som de blev set til forelæsningen:\n",
    "* Mean-squared error (MSE)\n",
    "* Mean-absolute error (MAE)\n",
    "* Categorical Cross Entropy (CCE)\n",
    "* Binary Cross Entropy (BCE)\n",
    "\n",
    "Udfyld de manglende linjer i koden, således at loss funktionerne kan bruges i netværket. Hvilken loss funktion til hvilken type problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return #TODO implementer MSE\n",
    "\n",
    "def MAE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return #TODO implementer MAE\n",
    "\n",
    "def CCE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return #TODO implementer CCE\n",
    "\n",
    "def BCE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return #TODO implementer BCE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 1.6: Fuldt eksempel\n",
    "Prøv i følgende at ændre lidt på vægtene og beregn loss med CCE. Hvor lille loss kan du få?\n",
    "\n",
    "Først er der lidt kode til at plotte og lave data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(n_datapunkter: int = 3000, n_klasser: int = 3, n_dim: int = 2, støj: float = 0.9):\n",
    "    for klasse in range(n_klasser):\n",
    "        X_ = np.random.normal(klasse, støj, (n_datapunkter // n_klasser, n_dim))\n",
    "        y_ = np.full(X_.shape[0], klasse)\n",
    "        if klasse == 0:\n",
    "            X = X_\n",
    "            y = y_\n",
    "        else:\n",
    "            X = np.vstack([X, X_])\n",
    "            y = np.hstack([y, y_])\n",
    "\n",
    "    idx = np.random.permutation(X.shape[0])\n",
    "    X, y = X[idx], y[idx]\n",
    "    X_train, y_train = X[:int(0.6 * X.shape[0])], y[:int(0.6 * X.shape[0])]\n",
    "    X_val, y_val = X[int(0.6 * X.shape[0]):int(0.8 * X.shape[0])], y[int(0.6 * X.shape[0]):int(0.8 * X.shape[0])]\n",
    "    X_test, y_test = X[int(0.8 * X.shape[0]):], y[int(0.8 * X.shape[0]):]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test  \n",
    "\n",
    "def plot_data(X: np.ndarray, y: np.ndarray, axs: plt.Axes = None):\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    for klasse in np.unique(y):\n",
    "        axs.scatter(X[y == klasse, 0], X[y == klasse, 1], label=f'Klasse {klasse}')\n",
    "    axs.legend()\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = data_generator()\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, x, y in zip(range(3), [X_train, X_val, X_test], [y_train, y_val, y_test]):\n",
    "    plot_data(x, y, ax[i])\n",
    "    ax[i].set_title(['Træningsdata', 'Valideringsdata', 'Testdata'][i])\n",
    "    ax[i].set_xlabel('$x_1$')\n",
    "    ax[i].set_ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dim = X_train.shape[1]\n",
    "L = [10, 10, 10, 3]\n",
    "vægte, biases = init_NN(X_dim, L)\n",
    "fs = [ReLu, leaky_ReLu, ReLu, softmax]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sæt vægte her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train, X_val, y_val, X_test, y_test indeholder træning, test og valideringsdata \n",
    "\n",
    "loss = pass # Beregn loss med CCE\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opgave 2: Implementering af et neuralt netværk med PyTorch\n",
    "\n",
    "Det ovenstående eksempel var en god øvelse i at forstå hvordan neurale netværk fungerer, men i praksis vil vi ofte bruge mere effektive biblioteker til at implementere neurale netværk. I denne opgave skal du implementere det samme neurale netværk med PyTorch. Du skal bruge PyTorch til at initialisere et neuralt netværk, udføre et forward pass, beregne loss, udføre et backward pass og opdatere vægtene i netværket. Du skal bruge PyTorch til at initialisere et neuralt netværk, udføre et forward pass, beregne loss, udføre et backward pass og opdatere vægtene i netværket.\n",
    "\n",
    "Vi bruger det samme data som forrige opgave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long()),\n",
    "    batch_size= BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long()),\n",
    "    batch_size= BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long()),\n",
    "    batch_size= BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 2.1\n",
    "\n",
    "I følgende kan i se et eksempel på et simpelt neuralt netværk. Dette er implementeret ved hjælp af PyTorch biblioteket. I følgende opgaver vil i blive bedt om at lave nogle ændringer i dette netværk for at ændre dets struktur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definer netværket\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # Loss funktionen\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # De to lag der bruges. Det første argument er hvor mange neuroner der er i det forrige lag. \n",
    "        # Det andet argument er hvor mange der skal være i det nuværrende lag\n",
    "        self.l1 = nn.Linear(num_features,6)\n",
    "        self.l2 = nn.Linear(6,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hvordan netværket skal køres:\n",
    "        x = self.l1(x) # Lag 1\n",
    "        x = F.relu(x) # Aktiveringsfunktion for lag 1\n",
    "        x = self.l2(x) # Lag 2\n",
    "        output = F.softmax(x, dim=1) # Aktiveringsfunktion for lag 2 (softmax)\n",
    "        return output\n",
    "\n",
    "#Hyperparameters\n",
    "num_classes = torch.from_numpy(y_train).unique().shape[0]\n",
    "num_features = X_train.shape[1]\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nedenfor følger træningsløkken. I bør ikke ændre noget i denne. Det vigtigeste her er at funktionen træner jeres model og viser hvordan den klare sig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader: torch.utils.data.DataLoader, val_loader: torch.utils.data.DataLoader, model):\n",
    "    \"\"\"\n",
    "    Træner modellen\n",
    "    Args:\n",
    "    train_loader (torch.utils.data.DataLoader): DataLoader for træningsdata\n",
    "    val_loader (torch.utils.data.DataLoader): DataLoader for valideringsdata\n",
    "    model (torch.nn.Module): Netværksarkitektur\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "    optimizer = optim.SGD(model.parameters())\n",
    "    his_acc = []\n",
    "    his_val_acc = []\n",
    "    his_loss = []\n",
    "    his_val_loss = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        # Sæt model til træning\n",
    "        model.train()\n",
    "        start_time = perf_counter()\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.float().to(device), y.long().to(device)\n",
    "\n",
    "            # Genstart gradienter\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat_prob = model(X)\n",
    "            y_hat = torch.argmax(y_hat_prob, dim=1).long()\n",
    "            \n",
    "            # Beregn loss, accuracy, og validation accuracy\n",
    "            loss = model.criterion(y_hat_prob, y)\n",
    "            losses.append(loss.item())\n",
    "            accuracy = torch.sum(y_hat == y) / len(y)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "            # Backward pass og opdatering af vægte\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(val_loader):\n",
    "                X, y = X.float().to(device), y.long().to(device)\n",
    "                y_hat_prob = model(X)\n",
    "                val_loss = model.criterion(y_hat_prob, y)\n",
    "                val_losses.append(val_loss.item())\n",
    "                val_accuracy = torch.sum(torch.argmax(y_hat_prob, dim=1) == y) / len(y)\n",
    "                val_accuracies.append(val_accuracy)\n",
    "\n",
    "            end_time = perf_counter()\n",
    "            his_acc.append(sum(accuracies) / len(accuracies))\n",
    "            his_val_acc.append(sum(val_accuracies) / len(val_accuracies))\n",
    "\n",
    "            his_loss.append(sum(losses) / len(losses))\n",
    "            his_val_loss.append(sum(val_losses) / len(val_losses))\n",
    "            print(f\"[{epoch+1} / {epochs} {end_time-start_time:.2f}s] Training - Loss: {sum(losses) / len(losses):3f} Accuracy: {sum(accuracies) / len(accuracies):3f} | Validation - Loss: {sum(val_losses) / len(val_losses):3f} Accuracy: {sum(val_accuracies) / len(val_accuracies):3f}\")\n",
    "    plt.figure()\n",
    "    plt.plot(range(epochs), his_acc, 'r', range(epochs), his_val_acc, 'b')\n",
    "    plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "    plt.xlabel('Epochs'), plt.ylabel('Acc')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(epochs), his_loss, 'r', range(epochs), his_val_loss, 'b')\n",
    "    plt.legend(['Train Loss','Validation Loss'])\n",
    "    plt.xlabel('Epochs'), plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan nu træne hvores simple model og se hvordan den klare sig på datasættet. Kør koden og se hvordna den træningen forløber. Se om i kan finde hvor mange epochs som modellen skal køre for at nogenlunde stibilisere i loss og accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "net = Net(num_features, num_classes)\n",
    "train(train_loader,val_loader,net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 2.2\n",
    "\n",
    "På følgende netværk, ændrer antallet af neuroner i det midterste lag til at være 15 neuroner, og se hvordan det ændre på modellens loss og performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # Loss funktionen\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # De to lag der bruges. Det første argument er hvor mange neuroner der er i det forrige lag. \n",
    "        # Det andet argument er hvor mange der skal være i det nuværrende lag\n",
    "        self.l1 = nn.Linear(num_features,6)\n",
    "        self.l2 = nn.Linear(6,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hvordan netværket skal køres:\n",
    "        x = self.l1(x) # Lag 1\n",
    "        x = F.relu(x) # Aktiveringsfunktion for lag 1\n",
    "        x = self.l2(x) # Lag 2\n",
    "        output = F.softmax(x, dim=1) # Aktiveringsfunktion for lag 2 (softmax)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(num_features, num_classes)\n",
    "# Skriv koden til at træne her\n",
    "epochs = 500\n",
    "train(train_loader,val_loader,net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 2.3\n",
    "På følgende netværk, ændre aktiveringsfunktionen til at være en sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # Loss funktionen\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # De to lag der bruges. Det første argument er hvor mange neuroner der er i det forrige lag. \n",
    "        # Det andet argument er hvor mange der skal være i det nuværrende lag\n",
    "        self.l1 = nn.Linear(num_features,6)\n",
    "        self.l2 = nn.Linear(6,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hvordan netværket skal køres:\n",
    "        x = self.l1(x) # Lag 1\n",
    "        x = F.relu(x) # Aktiveringsfunktion for lag 1\n",
    "        x = self.l2(x) # Lag 2\n",
    "        output = F.softmax(x, dim=1) # Aktiveringsfunktion for lag 2 (softmax)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skriv koden til at træne her"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 2.4\n",
    "På følgende netværk tilføj et ekstra lag. Husk at tilføje en aktiveringsfunktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # Loss funktionen\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # De to lag der bruges. Det første argument er hvor mange neuroner der er i det forrige lag. \n",
    "        # Det andet argument er hvor mange der skal være i det nuværrende lag\n",
    "        self.l1 = nn.Linear(num_features,6)\n",
    "        self.l2 = nn.Linear(6,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hvordan netværket skal køres:\n",
    "        x = self.l1(x) # Lag 1\n",
    "        x = F.relu(x) # Aktiveringsfunktion for lag 1\n",
    "        x = self.l2(x) # Lag 2\n",
    "        output = F.softmax(x, dim=1) # Aktiveringsfunktion for lag 2 (softmax)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skriv koden til at træne her"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 2.5\n",
    "Implementer at følgende netværk tager en ekstra parameter L. Dette skal være antallet af lag i netværket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, L):\n",
    "        super(Net, self).__init__()\n",
    "        # Loss funktionen\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # De to lag der bruges. Det første argument er hvor mange neuroner der er i det forrige lag. \n",
    "        # Det andet argument er hvor mange der skal være i det nuværrende lag\n",
    "        self.l1 = nn.Linear(num_features,6)\n",
    "        self.l2 = nn.Linear(6,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hvordan netværket skal køres:\n",
    "        x = self.l1(x) # Lag 1\n",
    "        x = F.relu(x) # Aktiveringsfunktion for lag 1\n",
    "        x = self.l2(x) # Lag 2\n",
    "        output = F.softmax(x, dim=1) # Aktiveringsfunktion for lag 2 (softmax)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skriv koden til at træne her"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave 2.6\n",
    "Lav en større model. Den skal have 6 lag med følgende struktur:\n",
    "1. `num_features` neuroner og ReLU aktiveringsfunktion\n",
    "2. 17 neuroner og sigmoid akriveringsfunktion\n",
    "3. 7 neuroner og ReLU aktiveringsfunktion\n",
    "4. 32 neuroner og tanh aktiveringsfunktion\n",
    "5. 16 neuroner og ReLU aktiveringsfunktion\n",
    "6. `num_classes` neuroner og softmax aktiveringsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        # Loss funktionen\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # De to lag der bruges. Det første argument er hvor mange neuroner der er i det forrige lag. \n",
    "        # Det andet argument er hvor mange der skal være i det nuværrende lag\n",
    "        self.l1 = nn.Linear(num_features,6)\n",
    "        self.l2 = nn.Linear(6,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hvordan netværket skal køres:\n",
    "        x = self.l1(x) # Lag 1\n",
    "        x = F.relu(x) # Aktiveringsfunktion for lag 1\n",
    "        x = self.l2(x) # Lag 2\n",
    "        output = F.softmax(x, dim=1) # Aktiveringsfunktion for lag 2 (softmax)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skriv koden til at træne her"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
