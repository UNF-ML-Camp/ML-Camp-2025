{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e86a212",
   "metadata": {},
   "source": [
    "## Del 2: Lineær regression i Python\n",
    "\n",
    "Nu har I set lineær regression som matematik. Det er faktisk ikke alt for svært at skrive op i Python..., husk at formlen for en prediciton i lineær regression er følgende:\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$$\n",
    "\n",
    "Dette er selvfølgelig lidt forskelligt fra det rigtige (orakel) som genererer dataen:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$$\n",
    "\n",
    "Men den kan vi jo som ML udviklere ikke finde... så wgaf?\n",
    "\n",
    "Vi kan dog finde et **estimat** for de rigtige vægte. Det er altså estimatoren som minimerer den kvadrerede fejl $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2$, denne findes for lineær regression ved følgende formel:\n",
    "\n",
    "$$\\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "**Igennem meget af denne opgave, kommer vi til at arbejde med data vi selv genererer. Dette er måske lidt af et taget eksempel, men det er for at give en bedre intuition for hvornår lineær regression rent faktisk virker.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a34b3",
   "metadata": {},
   "source": [
    "## Opgave 1. Data generering\n",
    "\n",
    "**Som sagt, så bruger vi genereret data i disse opgaver, da det kan give os en lidt bedre ide for hvad rollen af en model er, i hvert fald i lineær regression.**\n",
    "\n",
    "**1.1 Undersøg dataen der er genereret med standardinstillingerne. Hvordan ligger den sig rundt om orakelfunktionen?**\n",
    "\n",
    "**1.2 Variér støjen i dataen, hvordan påvirker en anden mean værdi og en anden varians henholdsvis den genereret data? Hvad er \"værst\" for en model og hvorfor?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40fce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, from_, to, oracle, noise_mean=0, noise_var=1):\n",
    "    # Generer noget et dimensionelt data (vores features)\n",
    "    x_vals = np.random.uniform(low=from_, high=to, size=n)\n",
    "\n",
    "    # Apply vores oracle funktion alle features, derefter tilføj støj\n",
    "    y_vals = np.array(list(map(oracle, x_vals)))\n",
    "    noise = np.random.normal(loc=noise_mean, scale=noise_var, size=n)\n",
    "\n",
    "    y_vals = y_vals + noise\n",
    "    \n",
    "    return x_vals, y_vals\n",
    "\n",
    "def split_train_test(x_vals, y_vals, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split datasættet til træning og test tilfældigt\n",
    "    En lidt mere udspekluret måde end der bliver brugt ellers...\n",
    "    \"\"\"\n",
    "    n = x_vals.shape[0]\n",
    "    indices = np.arange(n)\n",
    "\n",
    "    # Shuffle indekser for at sikre tilfældighed\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Find størrelse af træningssættet\n",
    "    train_size = int(n * train_ratio)\n",
    "    train_indices = indices[:train_size]\n",
    "\n",
    "    # Resten af punkterne må være testsættet\n",
    "    test_indices = indices[train_size:]\n",
    "\n",
    "    # Opdel vores datasæt i train og test\n",
    "    train_xs = x_vals[train_indices]\n",
    "    train_ys = y_vals[train_indices]\n",
    "\n",
    "    test_xs = x_vals[test_indices]\n",
    "    test_ys = y_vals[test_indices]\n",
    "\n",
    "    return train_xs, train_ys, test_xs, test_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksempel på brug af at generere dataen\n",
    "\n",
    "a = 2\n",
    "b = 5\n",
    "def orakel(x):\n",
    "    # Orakelfunktion, ændre denne hvis du vil ændre på oraklet\n",
    "    return x * a + b\n",
    "\n",
    "# Simpel standard normalfordelt støj, ændre på dette hvis du ønsker andet støj\n",
    "mu = 0\n",
    "sigma = 1\n",
    "\n",
    "# Generer data med funktionen\n",
    "features, y = generate_data(100, 0, 4, oracle=orakel, noise_mean=mu, noise_var=sigma)\n",
    "\n",
    "# Opdel i træning og i test \n",
    "train_features, train_y, test_features, test_y = split_train_test(features, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(features, y, oracle=None):\n",
    "    \"\"\"\n",
    "    Funktion til at plotte vores data inklusiv vores orakelfunktion\n",
    "    \"\"\"\n",
    "    if orakel is not None:\n",
    "        xs = np.linspace(np.min(features), np.max(features), num=features.shape[0])\n",
    "        true_values = np.array(list(map(oracle, xs)))\n",
    "        plt.plot(xs, true_values, label=\"Sande Funktion\", color='red')\n",
    "\n",
    "\n",
    "    plt.scatter(features, y, label='Data punkter', color='blue')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Genereret data\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_data(features=train_features, y=train_y, oracle=orakel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff1f82",
   "metadata": {},
   "source": [
    "## Opgave 2: Lineær regression\n",
    "\n",
    "**At lave lineær regression er såååh nemt som at benytte sig af en formel til at minimere mean squared error. Det er basically alt vi kan gøre med det**\n",
    "\n",
    "**2.1: Udfyld linear_regression funktionen så den korrekt udregner de rigtige vægte til lineær regression**\n",
    "\n",
    "**2.2: Find ud af hvilken effekt en skæring med y aksen har ved at prøve at sætte intercept til at være False**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**2.3: Ændr i orakel funktionen så den bliver af højere orden, prøv så at bruge lineær regression til at løse problemet igen**\n",
    "\n",
    "**2.4: Overvej om lineær regression også kan bruges hvis orakel funktionen indeholder enten $\\exp{x}$ eller $\\sin{x}$. HINT: Kan vi gøre noget ved vores features?**\n",
    "\n",
    "$\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(features, y, intercept=True, higher_orders=[]):\n",
    "    \"\"\"\n",
    "    Funktion der, givet vores features og targets (y), finder den mest optimale\n",
    "    lineær regressionsløsning\n",
    "\n",
    "    Du kan inkludere højere ordener såsom [2] hvis du ønsker at gøre den mere kompleks\n",
    "    \"\"\"\n",
    "    n_samples = features.shape[0]\n",
    "\n",
    "    # Laver nyt navn for at vise vi har med vores data matrix at gøre\n",
    "    X = features[:, None]\n",
    "\n",
    "    # Tilføj højere ordener (X^2, X^3, osv.) \n",
    "    for order in higher_orders:\n",
    "        if isinstance(order, int) and order > 1:\n",
    "            higher_vector = X ** order\n",
    "            X = np.hstack((X, higher_vector))\n",
    "\n",
    "    # Tilføj skæring med y-aksen - også kaldt 'bias'\n",
    "    if intercept:\n",
    "        one_vector = np.ones((n_samples, 1))\n",
    "        X = np.hstack((one_vector, X))\n",
    "\n",
    "    # Udregn lineær regressions bud på de bedste vægte\n",
    "    # TODO: Find de bedste vægte\n",
    "    best_weights = ...\n",
    "\n",
    "    return best_weights, X  \n",
    "\n",
    "# Få de bedste vægte til lineær regression...\n",
    "best_weights, X = linear_regression(train_features, train_y, True, [2])\n",
    "\n",
    "print(f\"De bedste vægte er {best_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fdb360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anvend de bedste vægte til at lave predictions...\n",
    "plot_data(features=train_features, y=train_y, oracle=orakel)\n",
    "\n",
    "# Lav nogle 'dummy features' til bare at vise prediction linjen som den ville være\n",
    "dummy_features = np.linspace(np.min(train_features), np.max(train_features), train_features.shape[0])\n",
    "_, dummy_X = linear_regression(dummy_features, train_y, True, [2])\n",
    "\n",
    "# Lav predictions på disse 'dummy features'\n",
    "predictions = dummy_X @ best_weights\n",
    "\n",
    "plt.plot(dummy_features, predictions, label=f'Prediction {best_weights}', color='Red')\n",
    "plt.scatter(test_features, test_y, label='Test punkter', color='blue')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Genereret data\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-camp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
