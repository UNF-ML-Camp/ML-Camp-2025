{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a534f355",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "**Decision trees er all well and good, men der hvor *the real money(tm) er, er i *Random Forest* modeller. Algoritmen for dette er meget simpel:**\n",
    "\n",
    "1. **Tr칝n en helt masse decision trees p친 din data (gerne ikke s칝rligt dybe decision trees)**\n",
    "2. **Lav en demokratisk afstemning mellem disse decision trees om hvad et nyt datapunkt er**\n",
    "\n",
    "**Dette er vist rimelig simpelt nedenunder:**\n",
    "\n",
    "![](images/random_forest.png)\n",
    "\n",
    "**I denne opgave vil vi pr칮ve at lave predictions p친 et par datas칝t af forskellig kompleksitet, f칮rst med et enkelt decision tree, og s친 med en random forest model.**\n",
    "\n",
    "**Dette er desv칝rre der hvor ML bev칝ger sig v칝k fra det vi kan se p친 en sej m친de, og henimod stats p친 en sk칝rm, men bear with us!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6dc76",
   "metadata": {},
   "source": [
    "## Opgave 1: Random forest og decision trees\n",
    "\n",
    "**Der er et par datas칝t samlet her fra forskellige sources:**\n",
    "\n",
    "- Insurance: [https://www.kaggle.com/datasets/mirichoi0218/insurance](https://www.kaggle.com/datasets/mirichoi0218/insurance)\n",
    "- Iris: [https://www.kaggle.com/datasets/uciml/iris](https://www.kaggle.com/datasets/uciml/iris)\n",
    "\n",
    "**1.1: Lige nu er alle cellerne nedenunder sat til at lave tr칝ne en decision tree regressor or en random forest regressor p친 insurance datas칝ttet. Skift dette fra dette datas칝t til en af de andre. Her skal du nok overveje de f칮lgende ting for at f친 koden til at virke:**\n",
    "\n",
    "- **1. Er det klassifikation eller regression?**\n",
    "- **2. Hvad hedder target kolonnen?**\n",
    "- **3. Er der nogle kolonner som ikke er numeriske? Alts친 derfor kategoriske?**\n",
    "\n",
    "**1.2: Random forest plejer at have OK performance, men den v칝lges prim칝rt fordi den er *explainable*, alts친 den kan give et pr칝cist estimat for hvor sikker den er p친 en given prediction. Forklar hvorfor dette er tilf칝ldet**\n",
    "\n",
    " <span style=\"color:red\">L칒SNINGSFORSLAG: Med mange decision trees der \"stemmer\" internt om hvad et datapunkt, kan du f친 et rimelig godt estimat for hvor sikker hele modellen er. Jo flere tr칝er som er enige om samme beslutning, jo mere sikker m친 hele modellen v칝re.</span>.\n",
    "\n",
    "**1.3: Overvej tilf칝lde her hvor et enkeklt decision tree m친ske ville give mere mening end en hel random forest**\n",
    "\n",
    " <span style=\"color:red\">L칒SNINGSFORSLAG: Allerede med insurance datas칝ttet klarer b친de random forest og decision trees sig godt nok... Hvis compute er en stor ting man er bange for, kunne man overveje at bruge decision trees. Dette kan dog lede til problemer med for meget varians...</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede66538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66179e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age     sex     bmi  children smoker     region      charges\n",
      "0   19  female  27.900         0    yes  southwest  16884.92400\n",
      "1   18    male  33.770         1     no  southeast   1725.55230\n",
      "2   28    male  33.000         3     no  southeast   4449.46200\n",
      "3   33    male  22.705         0     no  northwest  21984.47061\n",
      "4   32    male  28.880         0     no  northwest   3866.85520\n"
     ]
    }
   ],
   "source": [
    "# Definer alle de datas칝t vi har og om de klassifikation eller ej\n",
    "iris = \"Iris.csv\" # Dette er et klassifikationsproblem!\n",
    "insurance = \"insurance.csv\"# Dette er et regressionsproblem!\n",
    "\n",
    "# V칝lg en af datas칝ttene her...\n",
    "current_dataset = insurance\n",
    "\n",
    "data = pd.read_csv(f\"../data/{current_dataset}\")\n",
    "\n",
    "# Print starten af dataen\n",
    "print(data.head())\n",
    "\n",
    "# V칝lg om det er klassifikation eller ej\n",
    "is_classification = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617e2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F친 alle ikke-numeriske kolonner til at v칝re numeriske\n",
    "categorical_columns = [\"sex\", \"smoker\", \"region\"]\n",
    "target_column = \"charges\"\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "\n",
    "# Opdel i targets og features\n",
    "X = data.drop(columns=[target_column])\n",
    "y = data[target_column]\n",
    "\n",
    "# Definer st칮rrelsen af tests칝ttet...\n",
    "test_size = 0.2\n",
    "\n",
    "# Opdel i tr칝ning og test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c84d9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tilf칝ldet hvis vi er i gang med at lave klassifikation\n",
    "if is_classification is True:\n",
    "    # Definer en decision tree model:\n",
    "    # NB: Du m친 gerne definere ting s친som criterion og max_depth, men man beh칮ver ikke\n",
    "    # Da ellers s친 bare f친r en default v칝rdi hver is칝r\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "\n",
    "    # Definer en random forest classifer\n",
    "    # Her kan man ogs친 bestemme en masse hyperparametre, den vigtigste er dog\n",
    "    n_estimators = 100\n",
    "    # Som bestemmer hvor mange decision trees vi bruger\n",
    "    rf_model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "\n",
    "# Tilf칝ldet af at vi ikke skal klassificere noget...\n",
    "elif is_classification is False:    \n",
    "    # Her benytter vi os af decision tree regressors i stedet for...\n",
    "    # Der g칝lder lidt nogle andre regler, men principperne er i bund og grund det samme\n",
    "    dt_model = DecisionTreeRegressor()\n",
    "\n",
    "    n_estimators = 100\n",
    "    rf_model = RandomForestRegressor(n_estimators=n_estimators)\n",
    "\n",
    "# Tr칝n modellerne\n",
    "dt_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Lav predictions p친 tests칝ttet\n",
    "dt_preds = dt_model.predict(X_test)\n",
    "rf_preds = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7237fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "游늵 Model Comparison:\n",
      "           Model  R Score         RMSE          MAE\n",
      "0  Decision Tree  0.748932  6033.912952  2722.919084\n",
      "1  Random Forest  0.837805  4849.791079  2770.178396\n"
     ]
    }
   ],
   "source": [
    "# Lav fuld rapport af accuracies og s친dan...\n",
    "if is_classification is True:\n",
    "    dt_accuracy = accuracy_score(y_test, dt_preds)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "\n",
    "    print(\"Decision Tree Classifier\")\n",
    "    print(\"Accuracy:\", dt_accuracy)\n",
    "    print(classification_report(y_test, dt_preds))\n",
    "\n",
    "    print(\"\\nRandom Forest Classifier\")\n",
    "    print(\"Accuracy:\", rf_accuracy)\n",
    "    print(classification_report(y_test, rf_preds))\n",
    "\n",
    "    # Optional: Compare in a simple table\n",
    "    comparison = pd.DataFrame({\n",
    "        \"Model\": [\"Decision Tree\", \"Random Forest\"],\n",
    "        \"Accuracy\": [dt_accuracy, rf_accuracy]\n",
    "    })\n",
    "\n",
    "    print(\"\\n游늵 Model Comparison:\")\n",
    "    print(comparison)\n",
    "\n",
    "\n",
    "\n",
    "elif is_classification is False:\n",
    "    from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error\n",
    "    comparison = pd.DataFrame({\n",
    "        \"Model\": [\"Decision Tree\", \"Random Forest\"],\n",
    "        \"R Score\": [r2_score(y_test, dt_preds), r2_score(y_test, rf_preds)],\n",
    "        \"RMSE\": [root_mean_squared_error(y_test, dt_preds), root_mean_squared_error(y_test, rf_preds)],\n",
    "        \"MAE\": [mean_absolute_error(y_test, dt_preds), mean_absolute_error(y_test, rf_preds)]\n",
    "    })\n",
    "\n",
    "    print(\"\\n游늵 Model Comparison:\")\n",
    "    print(comparison)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-camp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
