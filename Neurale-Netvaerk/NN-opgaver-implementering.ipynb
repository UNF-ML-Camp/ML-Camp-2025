{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32ca156",
   "metadata": {},
   "source": [
    "# Opgave Pytorch\n",
    "## Bagtanker\n",
    "\n",
    "Her er opgaverne som skal give jer en introduktion til hvor i kan implementere et simplt neuralt netv√¶rk ved hj√¶lp af pytorch.\n",
    "\n",
    "Der kommer til at v√¶re noget general struktur i koden for at i hurtigere kan komme igang med at lave nogle netv√¶rk og pr√∏ve p√• dataen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e35cda",
   "metadata": {},
   "source": [
    "## Pakker\n",
    "Her er alle pakker som i burde skulle bruge til at f√• et virkede neuralt netv√¶rk.\n",
    "\n",
    "### Installation og Import\n",
    "\n",
    "F√∏rst installerer vi de n√∏dvendige pakker:\n",
    "- **kagglehub**: Til at downloade datas√¶t fra Kaggle\n",
    "- **corner**: Til at lave corner plots for data visualisering\n",
    "- **seaborn**: Til smukke statistiske visualiseringer\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "!pip install -q kagglehub corner seaborn\n",
    "```\n",
    "\n",
    "**Imports:**\n",
    "```python\n",
    "# PyTorch - Det prim√¶re machine learning framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# VIGTIGT: Force CPU mode for kompatibilitet\n",
    "# Udkomment√©r denne linje hvis du vil bruge GPU (hvis tilg√¶ngelig)\n",
    "# torch.set_default_device(\"cpu\")\n",
    "```\n",
    "\n",
    "### CPU vs GPU H√•ndtering\n",
    "\n",
    "**Hvorn√•r skal I udkommentere `torch.set_default_device(\"cpu\")`:**\n",
    "- N√•r I har en GPU og vil bruge den til hurtigere tr√¶ning\n",
    "- N√•r I arbejder med store datas√¶t (>10,000 samples)\n",
    "- N√•r jeres model har mange parametre og tr√¶ning tager lang tid\n",
    "\n",
    "**Hvorn√•r skal I beholde CPU force:**\n",
    "- N√•r I l√¶rer grundl√¶ggende koncepter (mindre kompleksitet)\n",
    "- Hvis I f√•r GPU memory errors\n",
    "- N√•r I debugger kode (CPU er ofte mere stabil)\n",
    "- Hvis jeres GPU drivers ikke virker korrekt\n",
    "\n",
    "**S√•dan tjekker I GPU status:**\n",
    "```python\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "```\n",
    "\n",
    "### Resterende Imports\n",
    "\n",
    "```python\n",
    "# Standard data science biblioteker\n",
    "import numpy as np          # Numeriske beregninger og arrays\n",
    "import matplotlib.pyplot as plt  # Plotting og visualisering\n",
    "import seaborn as sns       # Statistiske plots og smukke grafer\n",
    "\n",
    "# Vores custom import til datas√¶t og tr√¶ning\n",
    "from NN_import import load_dataset, device, train_model\n",
    "```\n",
    "\n",
    "### Pakke Forklaringer\n",
    "\n",
    "#### PyTorch Komponenter\n",
    "- **torch**: Grundl√¶ggende tensor operationer og matematik\n",
    "- **torch.nn**: Neurale netv√¶rk komponenter (lag, aktiveringer, loss funktioner)\n",
    "- **torch.optim**: Optimizers (Adam, SGD, etc.) - importeres automatisk i train_model\n",
    "- **torch.utils.data**: DataLoader og Dataset klasser - bruges internt\n",
    "\n",
    "#### Visualisering og Data\n",
    "- **numpy**: H√•ndtering af numeriske data og arrays\n",
    "- **matplotlib.pyplot**: Grundl√¶ggende plotting funktioner\n",
    "- **seaborn**: Statistiske visualiseringer og smukke plots\n",
    "- **corner**: Specialiseret til corner plots (pairwise scatter plots)\n",
    "\n",
    "#### Custom Imports\n",
    "- **load_dataset**: Vores funktion til at indl√¶se og preprocesse data\n",
    "- **device**: Automatisk CPU/GPU detection\n",
    "- **train_model**: Komplet tr√¶nings pipeline\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Hvis I f√•r CUDA errors:**\n",
    "- Udkomment√©r `torch.set_default_device(\"cpu\")` linjen\n",
    "\n",
    "**Hvis visualiseringer ikke virker:**\n",
    "- S√∏rg for at I har `matplotlib` og `seaborn` installeret\n",
    "- Tjek at I k√∏rer i et milj√∏ der underst√∏tter plotting (Jupyter)\n",
    "\n",
    "### GPU Performance Tips\n",
    "\n",
    "**Hvorn√•r GPU giver st√∏rst fordel:**\n",
    "- Store datas√¶t (>50,000 samples)\n",
    "- Dybe netv√¶rk (>3-4 lag)\n",
    "- Mange tr√¶nings epochs (>100)\n",
    "- Komplekse arkitekturer\n",
    "\n",
    "**Hvorn√•r CPU kan v√¶re hurtigere:**\n",
    "- Meget sm√• datas√¶t (<1,000 samples)\n",
    "- Simple netv√¶rk (1-2 lag)\n",
    "- F√• epochs (<20)\n",
    "- Debugging og eksperimentering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mimodekj/.conda/envs/ML/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install -q kagglehub corner seaborn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Force CPU for compatibility\n",
    "#torch.set_default_device(\"cpu\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from NN_import import load_dataset, device, train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658aedbb",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Her har vi forberedt 5 forskellige datas√¶t som er formateret og klar til brug. Data preprocessing og cleaning er en af de vigtigste dele af Machine Learning, men det er gjort for jer, s√• I kan fokusere p√• at l√¶re om neurale netv√¶rk.\n",
    "\n",
    "### Oversigt over Datas√¶t\n",
    "\n",
    "| Dataset | Type | Target | Anvendelse |\n",
    "|---------|------|--------|------------|\n",
    "| `\"particle\"` | Regression | Invariant masse | Partikelfysik |\n",
    "| `\"weather\"` | Regression | Apparent temperatur | Vejrforudsigelse |\n",
    "| `\"grades\"` | Dual (Regression + Klassifikation) | GPA + GradeClass | Studieresultater |\n",
    "| `\"avocado\"` | Klassifikation | Modenhed | Kvalitetsvurdering |\n",
    "| `\"diabetes\"` | Klassifikation | Diabetes risiko | Sundhedsvurdering |\n",
    "\n",
    "### Detaljerede Beskrivelser\n",
    "\n",
    "#### üî¨ Partikelfysik (`\"particle\"`)\n",
    "**Type:** Regression  \n",
    "**Beskrivelse:** Elektron-par kollisioner i en partikeldetektor  \n",
    "**M√•l:** Forudsig den invariante masse (energi) af partikelparet  \n",
    "**Target:** `M` (masse) - kontinuerlig v√¶rdi  \n",
    "**Features:** Kinetiske egenskaber fra detektorm√•linger  \n",
    "**Anvendelse:** Opdagelse af nye partikler i fysikeksperimenter\n",
    "\n",
    "#### üå§Ô∏è Vejrdata (`\"weather\"`)\n",
    "**Type:** Regression  \n",
    "**Beskrivelse:** Meteorologiske observationer fra Szeged  \n",
    "**M√•l:** Forudsig apparent temperatur ud fra vejrforhold  \n",
    "**Target:** `\"Apparent Temperature (C)\"` - kontinuerlig v√¶rdi  \n",
    "**Features:** Temperatur, luftfugtighed, vindstyrke, barometertryk m.m.  \n",
    "**Anvendelse:** Vejrforudsigelse og komfortindeks\n",
    "\n",
    "#### üéì Studerende Data (`\"grades\"`)\n",
    "**Type:** Dual (b√•de regression og klassifikation)  \n",
    "**Beskrivelse:** Studerendes akademiske pr√¶station  \n",
    "**M√•l:** Forudsig b√•de GPA (regression) og karakterkategori (klassifikation)  \n",
    "**Target:** \n",
    "- `GPA` (regression): Kontinuerligt karaktergennemsnit (0.0-4.0)\n",
    "- `GradeClass` (klassifikation): Kategorisk karakterniveau (A, B, C, D, F) - **one-hot encoded**\n",
    "**Features:** Studietid, for√¶ldrest√∏tte, alder, tidligere karakterer  \n",
    "**Anvendelse:** Uddannelsesplanl√¶gning og tidlig intervention\n",
    "\n",
    "#### ü•ë Avocado Data (`\"avocado\"`)\n",
    "**Type:** Klassifikation  \n",
    "**Beskrivelse:** Kvalitetsvurdering af avocadoer  \n",
    "**M√•l:** Klassificer modenhedsgrad ud fra fysiske egenskaber  \n",
    "**Target:** `\"ripeness\"` - **one-hot encoded** kategorier (moden, fast-moden, umoden osv.)  \n",
    "**Features:** H√•rdhed, farve, st√∏rrelse, v√¶gt  \n",
    "**Anvendelse:** Kvalitetskontrol i f√∏devareindustrien\n",
    "\n",
    "#### üè• Diabetes Data (`\"diabetes\"`)\n",
    "**Type:** Klassifikation  \n",
    "**Beskrivelse:** Sundhedsrisiko vurdering  \n",
    "**M√•l:** Forudsig diabetes risiko ud fra livsstilsfaktorer  \n",
    "**Target:** `\"Diabetes_012\"` - **one-hot encoded** (0=ingen, 1=pre-diabetes, 2=diabetes)  \n",
    "**Features:** BMI, fysisk aktivitet, kost, alder, k√∏n, rygning  \n",
    "**Anvendelse:** Forebyggende sundhedspleje og risikoscreening\n",
    "\n",
    "### Target Encoding Forklaring\n",
    "\n",
    "#### One-Hot Encoding\n",
    "**Hvad er one-hot encoding?**\n",
    "One-hot encoding er en m√•de at repr√¶sentere kategoriske data p√• i en format som neurale netv√¶rk kan forst√•. I stedet for at bruge tal som 0, 1, 2 for kategorier, opretter vi separate bin√¶re kolonner for hver kategori.\n",
    "\n",
    "**Eksempel med 3 klasser (A, B, C):**\n",
    "```\n",
    "Original:  A  ‚Üí  [1, 0, 0]\n",
    "          B  ‚Üí  [0, 1, 0]  \n",
    "          C  ‚Üí  [0, 0, 1]\n",
    "```\n",
    "\n",
    "**Hvorfor bruger vi one-hot encoding?**\n",
    "- **Undg√•r ordin√¶r bias**: Tal som 0, 1, 2 antyder en r√¶kkef√∏lge/hierarki der ikke findes\n",
    "- **Bedre l√¶ring**: Netv√¶rket kan l√¶re forskellige m√∏nstre for hver klasse uafh√¶ngigt\n",
    "- **Matematisk korrekt**: Fungerer optimalt med softmax aktivering og CrossEntropy loss\n",
    "\n",
    "#### Target Format Per Dataset\n",
    "\n",
    "**Regression (Particle, Weather, Grades-GPA):**\n",
    "```python\n",
    "# Kontinuerlige v√¶rdier (normaliserede)\n",
    "train_targets_tensor.shape  # [N, 1] eller [N] - enkelt v√¶rdi per sample\n",
    "```\n",
    "\n",
    "**Bin√¶r Klassifikation (Avocado med ripeness_class=\"ripe\"):**\n",
    "```python\n",
    "# 0 eller 1 for to klasser\n",
    "train_targets_tensor.shape  # [N] - enkelt v√¶rdi (0/1) per sample\n",
    "```\n",
    "\n",
    "**Multiclass Klassifikation (Diabetes, Avocado-all, Grades-GradeClass):**\n",
    "```python\n",
    "# One-hot encoded - en kolonne per klasse\n",
    "train_targets_tensor.shape  # [N, num_classes] - one-hot vektor per sample\n",
    "\n",
    "# Eksempel for 3 klasser:\n",
    "# Sample 0: [1, 0, 0] - tilh√∏rer klasse 0\n",
    "# Sample 1: [0, 1, 0] - tilh√∏rer klasse 1\n",
    "# Sample 2: [0, 0, 1] - tilh√∏rer klasse 2\n",
    "```\n",
    "\n",
    "### Brug af Datas√¶ttene\n",
    "\n",
    "#### Grundl√¶ggende Loading\n",
    "```python\n",
    "# V√¶lg dit datas√¶t\n",
    "dataset_name = \"particle\"  # Skift til: \"weather\", \"grades\", \"avocado\", \"diabetes\"\n",
    "validation = False         # Inkluder validation set (default: False)\n",
    "visualize = False         # Vis data visualiseringer\n",
    "\n",
    "# Load datas√¶ttet\n",
    "data = load_dataset(dataset_name, validation=validation, visualize=visualize)\n",
    "```\n",
    "\n",
    "#### Tilg√¶ngelige Data Variabler\n",
    "Efter loading har du adgang til:\n",
    "- `X_train_tensor`, `X_test_tensor` - Input features (altid normaliserede)\n",
    "- `train_targets_tensor`, `test_targets_tensor` - Target v√¶rdier (format afh√¶nger af task type)  \n",
    "- `X_val_tensor`, `val_targets_tensor` - Validation data (hvis `validation=True`)\n",
    "- `input_size`, `output_size` - Netv√¶rksarkitektur dimensioner\n",
    "- `data['task_type']` - 'regression' eller 'classification'\n",
    "- `data['num_classes']` - Antal klasser (kun for klassifikation)\n",
    "- `data['feature_names']` - Navne p√• input features\n",
    "\n",
    "#### Output Size Guide\n",
    "```python\n",
    "# Regression: output_size = 1\n",
    "if data['task_type'] == 'regression':\n",
    "    print(f\"Output layer should have {data['output_size']} neuron\")\n",
    "\n",
    "# Binary classification: output_size = 1  \n",
    "elif data['num_classes'] == 1:\n",
    "    print(f\"Binary classification - output layer should have {data['output_size']} neuron\")\n",
    "\n",
    "# Multiclass classification: output_size = num_classes\n",
    "else:\n",
    "    print(f\"Multiclass classification - output layer should have {data['output_size']} neurons\")\n",
    "```\n",
    "\n",
    "#### Specielle Tilf√¶lde\n",
    "\n",
    "**Grades Dataset (Dual Target):**\n",
    "```python\n",
    "if dataset_name == \"grades\":\n",
    "    # Regression target (GPA)\n",
    "    gpa_targets = train_targets_tensor  # Shape: [N] - kontinuerligt\n",
    "    # Klassifikation target (GradeClass)  \n",
    "    grade_class_targets = data['train_targets2']  # Shape: [N, num_classes] - one-hot\n",
    "    print(f\"GPA target shape: {gpa_targets.shape}\")\n",
    "    print(f\"GradeClass target shape: {grade_class_targets.shape}\")\n",
    "```\n",
    "\n",
    "**Avocado Dataset (Bin√¶r vs. Multiclass):**\n",
    "```python\n",
    "# Bin√¶r klassifikation (ripe vs. andre)\n",
    "data = load_dataset(\"avocado\", ripeness_class=\"ripe\")\n",
    "print(f\"Binary target shape: {data['train_targets'].shape}\")  # [N]\n",
    "\n",
    "# Multiclass klassifikation (alle kategorier)\n",
    "data = load_dataset(\"avocado\", ripeness_class=\"all\")\n",
    "print(f\"Multiclass target shape: {data['train_targets'].shape}\")  # [N, num_classes]\n",
    "```\n",
    "\n",
    "### Data Preprocessing Pipeline\n",
    "\n",
    "Alle datas√¶t gennemg√•r automatisk standardisering:\n",
    "\n",
    "1. **NaN Removal** - R√¶kker med manglende v√¶rdier fjernes helt\n",
    "2. **Train/Val/Test Split** - Automatisk opdeling (80/10/10% hvis validation=True)\n",
    "3. **Feature Normalization** - StandardScaler p√• input features (mean=0, std=1)\n",
    "4. **Target Processing**:\n",
    "   - **Regression**: StandardScaler normalisering (mean=0, std=1)\n",
    "   - **Classification**: One-hot encoding for multiclass, bin√¶r for binary\n",
    "5. **Tensor Conversion** - Konverteret til PyTorch tensors p√• korrekt device (CPU/GPU)\n",
    "\n",
    "### Visualisering\n",
    "\n",
    "S√¶t `visualize=True` for at se:\n",
    "- **Corner plots** - Pairwise feature relationships\n",
    "- **Target distribution** - Histogram (regression) / bar plots (klassifikation)\n",
    "- **Correlation matrix** - Feature korrelationer\n",
    "- **Basic statistics** - Dataset st√∏rrelse og egenskaber\n",
    "\n",
    "### Eksempel Output\n",
    "```\n",
    "Dataset: diabetes\n",
    "Input size: 21\n",
    "Output size: 3  \n",
    "Task type: classification\n",
    "Number of classes: 3\n",
    "Training samples: 196036\n",
    "Test samples: 21782\n",
    "Feature names: ['HighBP', 'HighChol', 'CholCheck', ...]\n",
    "Target shape: [196036, 3]  # One-hot encoded med 3 klasser\n",
    "```\n",
    "\n",
    "### Vigtige Pointer for Netv√¶rksdesign\n",
    "\n",
    "**Output Layer Design:**\n",
    "- **Regression**: `nn.Linear(hidden_size, 1)` + ingen aktivering\n",
    "- **Bin√¶r klassifikation**: `nn.Linear(hidden_size, 1)` + sigmoid (eller BCEWithLogitsLoss)\n",
    "- **Multiclass**: `nn.Linear(hidden_size, num_classes)` + softmax\n",
    "\n",
    "**Loss Function Matching:**\n",
    "- **Regression**: MSELoss med kontinuerlige targets\n",
    "- **Bin√¶r**: BCELoss med 0/1 targets  \n",
    "- **Multiclass**: BCELoss med one-hot targets (eller CrossEntropyLoss med class indices)\n",
    "\n",
    "Denne struktur sikrer at jeres netv√¶rk f√•r data i det rigtige format og g√∏r det nemt at eksperimentere med forskellige arkitekturer p√• forskellige problemtyper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0dd3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (250, 9)\n",
      "After removing NaN values: (250, 9)\n",
      "Removed 0 rows with NaN values\n",
      "Avocado multiclass mapping: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
      "Task type: classification\n",
      "Number of classes: 5\n",
      "Dataset: avocado\n",
      "Input size: 7\n",
      "Output size: 5\n",
      "Training samples: 225\n",
      "Test samples: 25\n",
      "Feature names: ['firmness', 'hue', 'saturation', 'brightness', 'sound_db', 'weight_g', 'size_cm3']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset with visualization\n",
    "dataset_name = \"avocado\"  # Options: \"particle\", \"weather\", \"grades\", \"avocado\", \"diabetes\"\n",
    "validation = False # Set to True to include validation set\n",
    "visualize = False  # Set to True to see data visualizations\n",
    "\n",
    "data = load_dataset(dataset_name, validation=validation, visualize=visualize)\n",
    "\n",
    "# Extract commonly used variables for convenience\n",
    "X_train_tensor = data['X_train']\n",
    "X_test_tensor = data['X_test']\n",
    "train_targets_tensor = data['train_targets']  \n",
    "test_targets_tensor = data['test_targets']    \n",
    "input_size = data['input_size']\n",
    "output_size = data['output_size']\n",
    "\n",
    "# Get task type and number of classes from data\n",
    "task_type = data.get('task_type', 'regression')\n",
    "num_classes = data.get('num_classes', None)\n",
    "\n",
    "print(f\"Task type: {task_type}\")\n",
    "if task_type == 'classification' and num_classes:\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Check if there are any NaN values in the training data\n",
    "if torch.isnan(X_train_tensor).any():\n",
    "    print(\"Warning: Training data contains NaN values. Consider preprocessing to handle them.\")\n",
    "if torch.isnan(train_targets_tensor).any():\n",
    "    print(\"Warning: Training targets contain NaN values. Consider preprocessing to handle them.\")\n",
    "\n",
    "# Handle grades dataset with two targets\n",
    "if dataset_name == \"grades\" and 'train_targets2' in data:\n",
    "    train_targets_classification = data['train_targets2']  # GradeClass (classification)\n",
    "    test_targets_classification = data['test_targets2']    # GradeClass (classification)\n",
    "    if validation:\n",
    "        val_targets_classification = data['val_targets2']   # GradeClass (classification)\n",
    "    print(f\"Regression target (GPA): {train_targets_tensor.shape}\")\n",
    "    print(f\"Classification target (GradeClass): {train_targets_classification.shape}\")\n",
    "\n",
    "if validation:\n",
    "    X_val_tensor = data['X_val']\n",
    "    val_targets_tensor = data['val_targets']\n",
    "\n",
    "print(f\"Dataset: {data['dataset_name']}\")\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"Training samples: {X_train_tensor.shape[0]}\")\n",
    "if validation:\n",
    "    print(f\"Validation samples: {X_val_tensor.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_tensor.shape[0]}\")\n",
    "print(f\"Feature names: {data['feature_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f22fc",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "\n",
    "Her er det mest simple netv√¶rk I kan lave - det har kun 1 input og 1 output lag. \n",
    "\n",
    "### Netv√¶rksarkitektur\n",
    "\n",
    "N√•r I gerne vil lave flere lag, skal I bare s√∏rge for at st√∏rrelserne passer sammen:\n",
    "\n",
    "**Eksempel p√• lag-dimensioner:**\n",
    "- Input lag: `input_size` ‚Üí `hidden_size` (f.eks. 64)\n",
    "- Hidden lag: `64` ‚Üí `64` (eller anden st√∏rrelse)\n",
    "- Output lag: `64` ‚Üí `output_size`\n",
    "\n",
    "### Vigtige Punkter\n",
    "\n",
    "1. **Aktivering mellem lag**: Brug ReLU mellem hidden lag\n",
    "2. **Output aktivering**: \n",
    "   - **Regression**: Ingen aktivering p√• output (line√¶r)\n",
    "   - **Klassifikation**: Sigmoid (bin√¶r) eller ingen (multiclass med CrossEntropy)\n",
    "3. **Lag st√∏rrelse**: Typiske hidden dimensioner er 32, 64, 128, 256\n",
    "\n",
    "### Aktiveringsfunktioner\n",
    "\n",
    "Aktiveringsfunktioner introducerer ikke-linearitet i netv√¶rket og g√∏r det muligt at l√¶re komplekse m√∏nstre:\n",
    "\n",
    "**Almindelige aktiveringsfunktioner:**\n",
    "- `nn.ReLU()` - Rectified Linear Unit (mest popul√¶re)\n",
    "- `nn.Sigmoid()` - Sigmoid funktion (0 til 1)\n",
    "- `nn.Tanh()` - Tanh funktion (-1 til 1)\n",
    "- `nn.LeakyReLU()` - Modificeret ReLU der ikke \"d√∏r\"\n",
    "\n",
    "**Hvorn√•r bruger vi aktivering:**\n",
    "```python\n",
    "# Mellem lag (altid)\n",
    "x = self.fc1(x)\n",
    "x = self.relu(x)  # Aktivering mellem lag\n",
    "\n",
    "# P√• output lag (kun for klassifikation)\n",
    "x = self.fc_output(x)\n",
    "# For regression: ingen aktivering\n",
    "# For bin√¶r klassifikation: sigmoid\n",
    "# For multiclass: ingen (bruger CrossEntropy loss)\n",
    "```\n",
    "\n",
    "### SimpleNN Forklaring\n",
    "\n",
    "Dette eksempel viser strukturen, men har en fejl - ReLU skal ikke bruges p√• output ved regression:\n",
    "\n",
    "```python\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)  # Direkte forbindelse\n",
    "        # ReLU bruges normalt mellem lag, ikke p√• output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # For regression: ingen aktivering p√• output\n",
    "        return x\n",
    "```\n",
    "\n",
    "### Brug af train_model Funktionen\n",
    "\n",
    "`train_model` funktionen g√∏r tr√¶ning nemt ved at h√•ndtere alle de tekniske detaljer:\n",
    "\n",
    "```python\n",
    "# Grundl√¶ggende brug\n",
    "history, trained_model = train_model(\n",
    "    model=model, \n",
    "    X_train=X_train_tensor, \n",
    "    y_train=train_targets_tensor,\n",
    "    task_type='regression'  # eller 'classification'\n",
    ")\n",
    "```\n",
    "\n",
    "**Avancerede parametre:**\n",
    "```python\n",
    "history, trained_model = train_model(\n",
    "    model=model,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=train_targets_tensor,\n",
    "    X_val=X_val_tensor,           # Validation data (optional)\n",
    "    y_val=val_targets_tensor,     # Validation targets (optional)\n",
    "    X_test=X_test_tensor,         # Test data (optional)\n",
    "    y_test=test_targets_tensor,   # Test targets (optional)\n",
    "    task_type='regression',       # 'regression' eller 'classification'\n",
    "    epochs=100,                   # Antal tr√¶nings epochs\n",
    "    learning_rate=0.001,          # Learning rate for optimizer\n",
    "    batch_size=32,                # Batch st√∏rrelse\n",
    "    early_stopping_patience=10,   # Stop hvis ingen forbedring\n",
    "    print_every=10               # Print status hver 10. epoch\n",
    ")\n",
    "```\n",
    "\n",
    "**Hvad returnerer train_model:**\n",
    "- `history`: Dictionary med tr√¶nings/validation loss over tid\n",
    "- `trained_model`: Det tr√¶nede netv√¶rk klar til brug\n",
    "\n",
    "**Automatiske features:**\n",
    "- V√¶lger \"rigtig\" loss funktion (MSE/CrossEntropy)\n",
    "- Adam optimizer\n",
    "- Device handling (CPU/GPU)\n",
    "- Early stopping ved validation\n",
    "- Progress tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a6efc",
   "metadata": {},
   "source": [
    "### Opgave\n",
    "\n",
    "Nu skal I pr√∏ve at lave jeres eget netv√¶rk og tr√¶ne det p√• et af datas√¶ttene. \n",
    "\n",
    "**Krav til jeres netv√¶rk:**\n",
    "- 2-4 lag total (inkl. input og output lag)\n",
    "- ReLU aktivering mellem hidden lag\n",
    "- Passende output aktivering:\n",
    "  - **Regression**: Ingen aktivering p√• output\n",
    "  - **Bin√¶r klassifikation**: Sigmoid aktivering (kun √©n klasse output)\n",
    "  - **Multiclass klassifikation**: Softmax aktivering (kun √©n klasse kan v√¶re sand)\n",
    "\n",
    "**Om Bin√¶r Klassifikation:**\n",
    "De tilg√¶ngelige datas√¶t er prim√¶rt regression og multiclass klassifikation. For at eksperimentere med bin√¶r klassifikation kan I:\n",
    "- Bruge avocado datas√¶ttet med `ripeness_class=\"ripe\"` (ripe vs. andre)\n",
    "- Modificere et multiclass dataset til bin√¶r (f.eks. konvertere diabetes til \"diabetes vs. ingen diabetes\")\n",
    "\n",
    "**Test jeres netv√¶rk:**\n",
    "Efter I har implementeret jeres netv√¶rk, test det med `train_model`s grundl√¶ggende brug:\n",
    "\n",
    "```python\n",
    "# Grundl√¶ggende tr√¶ning\n",
    "history, trained_model = train_model(\n",
    "    model=model, \n",
    "    X_train=X_train_tensor, \n",
    "    y_train=train_targets_tensor,\n",
    "    task_type='regression'  # eller 'classification'\n",
    ")\n",
    "```\n",
    "\n",
    "**Tips:**\n",
    "- Start simpelt (2-3 lag)\n",
    "- Brug `hidden_size=64` til at starte med\n",
    "- Husk at s√¶tte korrekt `task_type` baseret p√• jeres dataset\n",
    "- Tjek at jeres netv√¶rks dimensioner passer med `input_size` og `output_size`\n",
    "- For bin√¶r klassifikation: brug sigmoid aktivering p√• output (v√¶rdi mellem 0-1)\n",
    "- For multiclass: brug softmax aktivering p√• output (sandsynlighedsfordeling over klasser)\n",
    "\n",
    "**Bin√¶r Klassifikation Eksempel:**\n",
    "```python\n",
    "# Load avocado dataset for binary classification\n",
    "data = load_dataset(\"avocado\", ripeness_class=\"ripe\")\n",
    "# Dette giver binary targets: ripe (1) vs. alle andre (0)\n",
    "\n",
    "# Eller konverter multiclass til binary:\n",
    "# For diabetes: konverter til \"har diabetes\" vs. \"ingen diabetes\"\n",
    "if dataset_name == \"diabetes\":\n",
    "    # Konverter one-hot encoded targets til binary\n",
    "    # [1,0,0] -> 0 (ingen diabetes)\n",
    "    # [0,1,0] eller [0,0,1] -> 1 (har diabetes)\n",
    "    binary_targets = (train_targets_tensor[:, 1] + train_targets_tensor[:, 2]).unsqueeze(1)\n",
    "    print(f\"Binary target shape: {binary_targets.shape}\")  # [N, 1]\n",
    "```\n",
    "\n",
    "**Train_model Loss Functions:**\n",
    "> **Note:** `train_model` funktionen bruger automatisk BCELoss for klassifikation (b√•de bin√¶r og multiclass) og MSELoss for regression. Dette betyder at for klassifikation skal jeres netv√¶rk have passende aktivering p√• output laget (sigmoid for bin√¶r, softmax for multiclass) da BCELoss forventer sandsynligheder mellem 0-1.\n",
    "\n",
    "**Aktivering vs. Loss Function:**\n",
    "- **Multiclass med Softmax**: Bruger BCELoss fordi targets er one-hot encoded\n",
    "- **Bin√¶r med Sigmoid**: Bruger BCELoss med enkelt output  \n",
    "- **Regression**: Bruger MSELoss med kontinuerlige targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686101d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JeresNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dimension=64, hidden_layers=2):\n",
    "        super(JeresNN, self).__init__()\n",
    "        # Her skal du definere de lag, aktiveringsfunktioner og andre komponenter i dit neurale netv√¶rk\n",
    "\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "        # TODO Inds√¶t din kode her\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Her skal du definere, hvordan data passerer gennem netv√¶rket\n",
    "        # TODO Inds√¶t din kode her\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa08a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c373978",
   "metadata": {},
   "source": [
    "### Opgave - Implementer Jeres Eget Tr√¶nings Loop\n",
    "\n",
    "Nu n√•r jeres netv√¶rk virker, skal I skrive jeres eget tr√¶nings loop fra bunden. Dette giver jer en dybere forst√•else af hvad der sker \"under motorhjelmen\" n√•r I bruger `train_model` funktionen.\n",
    "\n",
    "> **üìö L√¶s f√∏rst:** Se afsnittet om \"Matematikken\" i `NN-forklaring.ipynb` for at forst√• hvad der sker under tr√¶ning.\n",
    "\n",
    "#### Step 1: V√¶lg Loss Funktion og Hyperparametre\n",
    "\n",
    "F√∏rst skal I v√¶lge de rigtige komponenter til jeres tr√¶nings setup:\n",
    "\n",
    "**Loss Funktioner:**\n",
    "- **MSE (Mean Squared Error)** for regression problemer\n",
    "- **CrossEntropyLoss** for klassifikation problemer\n",
    "- **BCELoss** for bin√¶r klassifikation med sigmoid output\n",
    "\n",
    "**Hyperparametre I skal v√¶lge:**\n",
    "- `learning_rate` (f.eks. 0.001) - hvor store skridt optimizeren tager\n",
    "- `max_epochs` (f.eks. 100) - hvor mange gange I k√∏rer gennem alt data\n",
    "- `batch_size` (f.eks. 32) - hvor mange samples I behandler ad gangen\n",
    "\n",
    "> **üí° Tip:** Start konservativt med learning_rate=0.001. Hvis tr√¶ning er for langsom, pr√∏v 0.01. Hvis loss eksploderer, pr√∏v 0.0001.\n",
    "\n",
    "#### Step 2: Forst√• Tr√¶nings Loop Strukturen\n",
    "\n",
    "Et tr√¶nings loop har denne struktur:\n",
    "\n",
    "```\n",
    "For hver epoch (1 til max_epochs):\n",
    "    S√¶t model i tr√¶nings mode\n",
    "    For hver batch i tr√¶ningsdata:\n",
    "        1. Flyt data til korrekt device (CPU/GPU)\n",
    "        2. Nulstil gradienter\n",
    "        3. Forward pass (beregn output)\n",
    "        4. Beregn loss\n",
    "        5. Backward pass (beregn gradienter) \n",
    "        6. Optimizer step (opdater v√¶gte)\n",
    "    Print loss for denne epoch\n",
    "```\n",
    "\n",
    "> **üîç Forklaring:** L√¶s afsnittet om \"Epochs\" i `NN-forklaring.ipynb` for at forst√• hvorfor vi gentager denne proces.\n",
    "\n",
    "#### Step 3: Implementer Tr√¶nings Loopet\n",
    "\n",
    "**Ydre loop - Epochs:**\n",
    "- Lav en for-loop som k√∏rer `max_epochs` gange\n",
    "- Kald `model.train()` for at s√¶tte modellen i tr√¶nings mode\n",
    "- Opret en variabel til at holde styr p√• den samlede loss for denne epoch\n",
    "\n",
    "**Indre loop - Batches:**\n",
    "- Lav en for-loop over `train_loader` med `for X, y in train_loader:`\n",
    "- Dette giver jer b√•de input data (`X`) og targets (`y`) for hver batch\n",
    "\n",
    "**Data Handling:**\n",
    "```python\n",
    "# Flyt data til korrekt device\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "```\n",
    "\n",
    "**Den Klassiske Tr√¶nings Sekvens:**\n",
    "```python\n",
    "# 1. Nulstil gradienter fra forrige batch\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 2. Forward pass - hvad forudsiger modellen?\n",
    "outputs = model(X)\n",
    "\n",
    "# 3. Beregn loss mellem forudsigelse og virkelighed\n",
    "loss = criterion(outputs, y)\n",
    "\n",
    "# 4. Backward pass - beregn gradienter\n",
    "loss.backward()\n",
    "\n",
    "# 5. Optimizer step - opdater model v√¶gte\n",
    "optimizer.step()\n",
    "\n",
    "# 6. Akkumuler loss for denne batch\n",
    "epoch_loss += loss.item()\n",
    "```\n",
    "\n",
    "**Print Progress:**\n",
    "Efter hver epoch, print den gennemsnitlige loss:\n",
    "```python\n",
    "avg_loss = epoch_loss / len(train_loader)\n",
    "print(f'Epoch [{epoch+1}/{max_epochs}], Loss: {avg_loss:.4f}')\n",
    "```\n",
    "\n",
    "#### Step 4: Debugging Tips\n",
    "\n",
    "**Hvis jeres loss ikke falder:**\n",
    "- Tjek at I bruger den rigtige loss funktion for jeres problem type\n",
    "- Pr√∏v en lavere learning rate\n",
    "- Kontroller at jeres netv√¶rk har passende aktivering p√• output\n",
    "\n",
    "**Hvis loss eksploderer (bliver meget stor):**\n",
    "- Reducer learning rate betydeligt (f.eks. fra 0.001 til 0.0001)\n",
    "- Tjek at jeres data er normaliseret korrekt\n",
    "\n",
    "**Hvis tr√¶ning er for langsom:**\n",
    "- √òg learning rate forsigtigt\n",
    "- Reducer model st√∏rrelse eller batch size\n",
    "\n",
    "> **‚ö†Ô∏è Almindelige Fejl:**\n",
    "> - Glemme `optimizer.zero_grad()` - dette f√•r gradienter til at akkumulere\n",
    "> - Glemme `loss.backward()` - ingen gradienter bliver beregnet\n",
    "> - Forkert loss funktion for problem type\n",
    "> - Data ikke p√• samme device som model\n",
    "\n",
    "#### Step 5: Sammenlign med train_model\n",
    "\n",
    "Efter I har implementeret jeres eget tr√¶nings loop, sammenlign det med `train_model` funktionen:\n",
    "- F√•r I lignende loss v√¶rdier?\n",
    "- Tr√¶ner jeres model lige s√• hurtigt?\n",
    "- Hvad er forskellen i performance?\n",
    "\n",
    "> **üéØ M√•l:** I skal kunne implementere et funktionelt tr√¶nings loop der giver lignende resultater som `train_model` funktionen. Dette viser at I forst√•r de grundl√¶ggende principper bag neural network tr√¶ning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad7795",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m train_dataset = torch.utils.data.TensorDataset(X_train_tensor, train_targets_tensor)\n\u001b[32m     13\u001b[39m train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m optimizer = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ML/lib/python3.12/site-packages/torch/optim/adam.py:100\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor betas[1] must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m defaults = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     88\u001b[39m     lr=lr,\n\u001b[32m     89\u001b[39m     betas=betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     decoupled_weight_decay=decoupled_weight_decay,\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ML/lib/python3.12/site-packages/torch/optim/optimizer.py:364\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    362\u001b[39m param_groups = \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33moptimizer got an empty parameter list\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[32m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    366\u001b[39m     param_groups = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: param_groups}]\n",
      "\u001b[31mValueError\u001b[39m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "#Define the loss function\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression tasks\n",
    "#criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "max_epochs = 100\n",
    "\n",
    "# Initialize the model, optimizer, and data loader\n",
    "model = JeresNN(input_size, output_size).to(device)  # Initialize your custom model\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, train_targets_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515e0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df2382e5",
   "metadata": {},
   "source": [
    "### Opgave - Evaluering af Jeres Tr√¶nede Model\n",
    "\n",
    "Nu har I det mest basale om hvordan man tr√¶ner et neuralt netv√¶rk, s√• nu skal I evaluere det. Det er som udgangspunkt bare at sige `model(jeres_data_her)`. Det vigtige er at I skriver `model.eval()` f√∏rst for at g√• ud af tr√¶nings mode.\n",
    "\n",
    "Nu skal I pr√∏ve at se hvor langt jeres forudsagte test data er fra de rigtige v√¶rdier.\n",
    "\n",
    "#### For Regression:\n",
    "- **Standard afvigelse**: `torch.std(forudsigelse - rigtig)` \n",
    "- **Root Mean Square Error (RMSE)**: `torch.sqrt(torch.mean((forudsigelse - rigtig)**2))`\n",
    "- **Mean Absolute Error (MAE)**: `torch.mean(torch.abs(forudsigelse - rigtig))`\n",
    "\n",
    "#### For Klassifikation:\n",
    "- **Accuracy (n√∏jagtighed)**: `torch.mean((forudsigelse > 0.5) == rigtig)` for bin√¶r klassifikation\n",
    "- **For multiclass**: `torch.mean(torch.argmax(forudsigelse, dim=1) == torch.argmax(rigtig, dim=1))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401a4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db98475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d04f5a2",
   "metadata": {},
   "source": [
    "### Opgave - Eksperimentering og Videreudvikling\n",
    "\n",
    "Leg med det! Den bedste m√•de at f√• en god id√© om hvad der virker i machine learning er at eksperimentere mange gange. Derfor synes vi bare I skal lege med det og pr√∏ve forskellige ting.\n",
    "\n",
    "> **üìö L√¶s f√∏rst:** Se `NN-forklaring.ipynb` for dybere forst√•else af de koncepter der n√¶vnes herunder.\n",
    "\n",
    "#### Forslag til Eksperimenter\n",
    "\n",
    "**Grundl√¶ggende Forbedringer:**\n",
    "- **Validering og Early Stopping** - Se afsnittet om \"Validation og Early stopping\" i `NN-forklaring.ipynb`\n",
    "  - S√¶t `validation=True` n√•r I loader data\n",
    "  - Brug `train_model` med validation parametre for automatisk early stopping\n",
    "  - Implementer jeres eget early stopping i jeres tr√¶nings loop\n",
    "\n",
    "- **Netv√¶rksarkitektur** - Se afsnittet om \"Matematikken\" for forst√•else af lag-strukturer\n",
    "  - Pr√∏v forskellige antal lag (2-6 lag)\n",
    "  - Eksperiment√©r med forskellige hidden_dimensions (32, 64, 128, 256)\n",
    "  - Test forskellige aktiveringsfunktioner mellem lag\n",
    "\n",
    "**Avancerede Teknikker:**\n",
    "- **Dropout for Regularisering** - Se afsnittet om \"Overtr√¶ning\" ‚Üí \"Dropout\"\n",
    "  - Tilf√∏j `nn.Dropout(p=0.2)` mellem jeres lag\n",
    "  - Pr√∏v forskellige dropout rates (0.1, 0.3, 0.5)\n",
    "  - Sammenlign performance med og uden dropout\n",
    "\n",
    "- **Forskellige Aktiveringsfunktioner** - Se afsnittet om \"Aktiverings Funktioner\"\n",
    "  - Pr√∏v `nn.LeakyReLU()` i stedet for `nn.ReLU()`\n",
    "  - Test `nn.Tanh()` p√• hidden lag\n",
    "  - Eksperiment√©r med forskellige output aktiveringer baseret p√• jeres problem type\n",
    "\n",
    "**Problem-Specifik Eksperimentering:**\n",
    "- **Klassifikation vs. Regression** - Se afsnittene om \"Klassifikation\" og \"Regression\"\n",
    "  - Sammenlign performance p√• forskellige datas√¶t\n",
    "  - Pr√∏v at konvertere multiclass til bin√¶r klassifikation\n",
    "  - Test forskellige loss funktioner og evalueringsmetrikker\n",
    "\n",
    "- **Cross Validation** - Se afsnittet om \"Cross validation\" i `NN-forklaring.ipynb`\n",
    "  - Implementer k-fold cross validation\n",
    "  - Sammenlign med simple train/validation split\n",
    "\n",
    "#### Praktisk Brug af train_model\n",
    "\n",
    "`train_model` funktionen kan h√•ndtere b√•de evaluering og validering automatisk:\n",
    "\n",
    "```python\n",
    "# Fuld setup med validering og test evaluering\n",
    "history, trained_model = train_model(\n",
    "    model=model, \n",
    "    X_train=X_train_tensor, \n",
    "    y_train=train_targets_tensor,\n",
    "    X_val=X_val_tensor if validation else None,     # Automatisk early stopping\n",
    "    y_val=val_targets_tensor if validation else None,\n",
    "    X_test=X_test_tensor,                           # Automatisk test evaluering\n",
    "    y_test=test_targets_tensor,\n",
    "    task_type='regression',  # eller 'classification'\n",
    "    epochs=200,              # H√∏jere da early stopping stopper automatisk\n",
    "    early_stopping_patience=15  # Stop hvis ingen forbedring i 15 epochs\n",
    ")\n",
    "```\n",
    "\n",
    "#### Debugging og Probleml√∏sning\n",
    "\n",
    "**Hvis jeres model ikke l√¶rer:** Se \"Hvad kan g√• galt\" sektionerne i `NN-forklaring.ipynb`\n",
    "- Tjek for overtr√¶ning (h√∏j tr√¶ning accuracy, lav test accuracy)\n",
    "- Kontroller data skala og normalisering\n",
    "- Pr√∏v forskellige learning rates\n",
    "- Unders√∏g om I har nok data til jeres model kompleksitet\n",
    "\n",
    "**Hvis I f√•r m√¶rkelige resultater:**\n",
    "- Sammenlign med baseline (hvad f√•r I hvis I altid g√¶tter gennemsnittet?)\n",
    "- Visualiser jeres predictions vs. targets\n",
    "- Tjek om jeres data har bias eller outliers\n",
    "\n",
    "#### Sp√∏rgsm√•l og Hj√¶lp\n",
    "\n",
    "Sp√∏rg gerne hvis der er noget I har problemer med, eller noget mere I gerne vil pr√∏ve! Nogle gode sp√∏rgsm√•l at stille sig selv:\n",
    "\n",
    "- Hvordan p√•virker antal lag performance?\n",
    "- Hvad sker der hvis I bruger meget sm√• eller store learning rates?\n",
    "- Kan I finde det optimale antal epochs f√∏r overtr√¶ning?\n",
    "- Hvilken aktiverings funktion virker bedst for jeres problem?\n",
    "- Hvordan p√•virker dropout rate performance?\n",
    "\n",
    "> **üí° Tip:** Start med simple √¶ndringer og byg kompleksiteten op gradvist. Document hvad I pr√∏ver s√• I kan sammenligne resultater!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba488f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18717a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab13e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
