{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32ca156",
   "metadata": {},
   "source": [
    "# Opgave Pytorch\n",
    "## Bagtanker\n",
    "\n",
    "Her er opgaverne som skal give jer en introduktion til hvor i kan implementere et simplt neuralt netværk ved hjælp af pytorch.\n",
    "\n",
    "Der kommer til at være noget general struktur i koden for at i hurtigere kan komme igang med at lave nogle netværk og prøve på dataen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e35cda",
   "metadata": {},
   "source": [
    "## Pakker\n",
    "Her er alle pakker som i burde skulle bruge til at få et virkede neuralt netværk.\n",
    "\n",
    "### Installation og Import\n",
    "\n",
    "Først installerer vi de nødvendige pakker:\n",
    "- **kagglehub**: Til at downloade datasæt fra Kaggle\n",
    "- **corner**: Til at lave corner plots for data visualisering\n",
    "- **seaborn**: Til smukke statistiske visualiseringer\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "!pip install -q kagglehub corner seaborn\n",
    "```\n",
    "\n",
    "**Imports:**\n",
    "```python\n",
    "# PyTorch - Det primære machine learning framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# VIGTIGT: Force CPU mode for kompatibilitet\n",
    "# Udkommentér denne linje hvis du vil bruge GPU (hvis tilgængelig)\n",
    "# torch.set_default_device(\"cpu\")\n",
    "```\n",
    "\n",
    "### CPU vs GPU Håndtering\n",
    "\n",
    "**Hvornår skal I udkommentere `torch.set_default_device(\"cpu\")`:**\n",
    "- Når I har en GPU og vil bruge den til hurtigere træning\n",
    "- Når I arbejder med store datasæt (>10,000 samples)\n",
    "- Når jeres model har mange parametre og træning tager lang tid\n",
    "\n",
    "**Hvornår skal I beholde CPU force:**\n",
    "- Når I lærer grundlæggende koncepter (mindre kompleksitet)\n",
    "- Hvis I får GPU memory errors\n",
    "- Når I debugger kode (CPU er ofte mere stabil)\n",
    "- Hvis jeres GPU drivers ikke virker korrekt\n",
    "\n",
    "**Sådan tjekker I GPU status:**\n",
    "```python\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "```\n",
    "\n",
    "### Resterende Imports\n",
    "\n",
    "```python\n",
    "# Standard data science biblioteker\n",
    "import numpy as np          # Numeriske beregninger og arrays\n",
    "import matplotlib.pyplot as plt  # Plotting og visualisering\n",
    "import seaborn as sns       # Statistiske plots og smukke grafer\n",
    "\n",
    "# Vores custom import til datasæt og træning\n",
    "from NN_import import load_dataset, device, train_model\n",
    "```\n",
    "\n",
    "### Pakke Forklaringer\n",
    "\n",
    "#### PyTorch Komponenter\n",
    "- **torch**: Grundlæggende tensor operationer og matematik\n",
    "- **torch.nn**: Neurale netværk komponenter (lag, aktiveringer, loss funktioner)\n",
    "- **torch.optim**: Optimizers (Adam, SGD, etc.) - importeres automatisk i train_model\n",
    "- **torch.utils.data**: DataLoader og Dataset klasser - bruges internt\n",
    "\n",
    "#### Visualisering og Data\n",
    "- **numpy**: Håndtering af numeriske data og arrays\n",
    "- **matplotlib.pyplot**: Grundlæggende plotting funktioner\n",
    "- **seaborn**: Statistiske visualiseringer og smukke plots\n",
    "- **corner**: Specialiseret til corner plots (pairwise scatter plots)\n",
    "\n",
    "#### Custom Imports\n",
    "- **load_dataset**: Vores funktion til at indlæse og preprocesse data\n",
    "- **device**: Automatisk CPU/GPU detection\n",
    "- **train_model**: Komplet trænings pipeline\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Hvis I får CUDA errors:**\n",
    "- Udkommentér `torch.set_default_device(\"cpu\")` linjen\n",
    "\n",
    "**Hvis visualiseringer ikke virker:**\n",
    "- Sørg for at I har `matplotlib` og `seaborn` installeret\n",
    "- Tjek at I kører i et miljø der understøtter plotting (Jupyter)\n",
    "\n",
    "### GPU Performance Tips\n",
    "\n",
    "**Hvornår GPU giver størst fordel:**\n",
    "- Store datasæt (>50,000 samples)\n",
    "- Dybe netværk (>3-4 lag)\n",
    "- Mange trænings epochs (>100)\n",
    "- Komplekse arkitekturer\n",
    "\n",
    "**Hvornår CPU kan være hurtigere:**\n",
    "- Meget små datasæt (<1,000 samples)\n",
    "- Simple netværk (1-2 lag)\n",
    "- Få epochs (<20)\n",
    "- Debugging og eksperimentering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mimodekj/.conda/envs/ML/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install -q kagglehub corner seaborn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Force CPU for compatibility\n",
    "#torch.set_default_device(\"cpu\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from NN_import import load_dataset, device, train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658aedbb",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Her har vi forberedt 5 forskellige datasæt som er formateret og klar til brug. Data preprocessing og cleaning er en af de vigtigste dele af Machine Learning, men det er gjort for jer, så I kan fokusere på at lære om neurale netværk.\n",
    "\n",
    "### Oversigt over Datasæt\n",
    "\n",
    "| Dataset | Type | Target | Anvendelse |\n",
    "|---------|------|--------|------------|\n",
    "| `\"particle\"` | Regression | Invariant masse | Partikelfysik |\n",
    "| `\"weather\"` | Regression | Apparent temperatur | Vejrforudsigelse |\n",
    "| `\"grades\"` | Dual (Regression + Klassifikation) | GPA + GradeClass | Studieresultater |\n",
    "| `\"avocado\"` | Klassifikation | Modenhed | Kvalitetsvurdering |\n",
    "| `\"diabetes\"` | Klassifikation | Diabetes risiko | Sundhedsvurdering |\n",
    "\n",
    "### Detaljerede Beskrivelser\n",
    "\n",
    "#### 🔬 Partikelfysik (`\"particle\"`)\n",
    "**Type:** Regression  \n",
    "**Beskrivelse:** Elektron-par kollisioner i en partikeldetektor  \n",
    "**Mål:** Forudsig den invariante masse (energi) af partikelparet  \n",
    "**Target:** `M` (masse) - kontinuerlig værdi  \n",
    "**Features:** Kinetiske egenskaber fra detektormålinger  \n",
    "**Anvendelse:** Opdagelse af nye partikler i fysikeksperimenter\n",
    "\n",
    "#### 🌤️ Vejrdata (`\"weather\"`)\n",
    "**Type:** Regression  \n",
    "**Beskrivelse:** Meteorologiske observationer fra Szeged  \n",
    "**Mål:** Forudsig apparent temperatur ud fra vejrforhold  \n",
    "**Target:** `\"Apparent Temperature (C)\"` - kontinuerlig værdi  \n",
    "**Features:** Temperatur, luftfugtighed, vindstyrke, barometertryk m.m.  \n",
    "**Anvendelse:** Vejrforudsigelse og komfortindeks\n",
    "\n",
    "#### 🎓 Studerende Data (`\"grades\"`)\n",
    "**Type:** Dual (både regression og klassifikation)  \n",
    "**Beskrivelse:** Studerendes akademiske præstation  \n",
    "**Mål:** Forudsig både GPA (regression) og karakterkategori (klassifikation)  \n",
    "**Target:** \n",
    "- `GPA` (regression): Kontinuerligt karaktergennemsnit (0.0-4.0)\n",
    "- `GradeClass` (klassifikation): Kategorisk karakterniveau (A, B, C, D, F) - **one-hot encoded**\n",
    "**Features:** Studietid, forældrestøtte, alder, tidligere karakterer  \n",
    "**Anvendelse:** Uddannelsesplanlægning og tidlig intervention\n",
    "\n",
    "#### 🥑 Avocado Data (`\"avocado\"`)\n",
    "**Type:** Klassifikation  \n",
    "**Beskrivelse:** Kvalitetsvurdering af avocadoer  \n",
    "**Mål:** Klassificer modenhedsgrad ud fra fysiske egenskaber  \n",
    "**Target:** `\"ripeness\"` - **one-hot encoded** kategorier (moden, fast-moden, umoden osv.)  \n",
    "**Features:** Hårdhed, farve, størrelse, vægt  \n",
    "**Anvendelse:** Kvalitetskontrol i fødevareindustrien\n",
    "\n",
    "#### 🏥 Diabetes Data (`\"diabetes\"`)\n",
    "**Type:** Klassifikation  \n",
    "**Beskrivelse:** Sundhedsrisiko vurdering  \n",
    "**Mål:** Forudsig diabetes risiko ud fra livsstilsfaktorer  \n",
    "**Target:** `\"Diabetes_012\"` - **one-hot encoded** (0=ingen, 1=pre-diabetes, 2=diabetes)  \n",
    "**Features:** BMI, fysisk aktivitet, kost, alder, køn, rygning  \n",
    "**Anvendelse:** Forebyggende sundhedspleje og risikoscreening\n",
    "\n",
    "### Target Encoding Forklaring\n",
    "\n",
    "#### One-Hot Encoding\n",
    "**Hvad er one-hot encoding?**\n",
    "One-hot encoding er en måde at repræsentere kategoriske data på i en format som neurale netværk kan forstå. I stedet for at bruge tal som 0, 1, 2 for kategorier, opretter vi separate binære kolonner for hver kategori.\n",
    "\n",
    "**Eksempel med 3 klasser (A, B, C):**\n",
    "```\n",
    "Original:  A  →  [1, 0, 0]\n",
    "          B  →  [0, 1, 0]  \n",
    "          C  →  [0, 0, 1]\n",
    "```\n",
    "\n",
    "**Hvorfor bruger vi one-hot encoding?**\n",
    "- **Undgår ordinær bias**: Tal som 0, 1, 2 antyder en rækkefølge/hierarki der ikke findes\n",
    "- **Bedre læring**: Netværket kan lære forskellige mønstre for hver klasse uafhængigt\n",
    "- **Matematisk korrekt**: Fungerer optimalt med softmax aktivering og CrossEntropy loss\n",
    "\n",
    "#### Target Format Per Dataset\n",
    "\n",
    "**Regression (Particle, Weather, Grades-GPA):**\n",
    "```python\n",
    "# Kontinuerlige værdier (normaliserede)\n",
    "train_targets_tensor.shape  # [N, 1] eller [N] - enkelt værdi per sample\n",
    "```\n",
    "\n",
    "**Binær Klassifikation (Avocado med ripeness_class=\"ripe\"):**\n",
    "```python\n",
    "# 0 eller 1 for to klasser\n",
    "train_targets_tensor.shape  # [N] - enkelt værdi (0/1) per sample\n",
    "```\n",
    "\n",
    "**Multiclass Klassifikation (Diabetes, Avocado-all, Grades-GradeClass):**\n",
    "```python\n",
    "# One-hot encoded - en kolonne per klasse\n",
    "train_targets_tensor.shape  # [N, num_classes] - one-hot vektor per sample\n",
    "\n",
    "# Eksempel for 3 klasser:\n",
    "# Sample 0: [1, 0, 0] - tilhører klasse 0\n",
    "# Sample 1: [0, 1, 0] - tilhører klasse 1\n",
    "# Sample 2: [0, 0, 1] - tilhører klasse 2\n",
    "```\n",
    "\n",
    "### Brug af Datasættene\n",
    "\n",
    "#### Grundlæggende Loading\n",
    "```python\n",
    "# Vælg dit datasæt\n",
    "dataset_name = \"particle\"  # Skift til: \"weather\", \"grades\", \"avocado\", \"diabetes\"\n",
    "validation = False         # Inkluder validation set (default: False)\n",
    "visualize = False         # Vis data visualiseringer\n",
    "\n",
    "# Load datasættet\n",
    "data = load_dataset(dataset_name, validation=validation, visualize=visualize)\n",
    "```\n",
    "\n",
    "#### Tilgængelige Data Variabler\n",
    "Efter loading har du adgang til:\n",
    "- `X_train_tensor`, `X_test_tensor` - Input features (altid normaliserede)\n",
    "- `train_targets_tensor`, `test_targets_tensor` - Target værdier (format afhænger af task type)  \n",
    "- `X_val_tensor`, `val_targets_tensor` - Validation data (hvis `validation=True`)\n",
    "- `input_size`, `output_size` - Netværksarkitektur dimensioner\n",
    "- `data['task_type']` - 'regression' eller 'classification'\n",
    "- `data['num_classes']` - Antal klasser (kun for klassifikation)\n",
    "- `data['feature_names']` - Navne på input features\n",
    "\n",
    "#### Output Size Guide\n",
    "```python\n",
    "# Regression: output_size = 1\n",
    "if data['task_type'] == 'regression':\n",
    "    print(f\"Output layer should have {data['output_size']} neuron\")\n",
    "\n",
    "# Binary classification: output_size = 1  \n",
    "elif data['num_classes'] == 1:\n",
    "    print(f\"Binary classification - output layer should have {data['output_size']} neuron\")\n",
    "\n",
    "# Multiclass classification: output_size = num_classes\n",
    "else:\n",
    "    print(f\"Multiclass classification - output layer should have {data['output_size']} neurons\")\n",
    "```\n",
    "\n",
    "#### Specielle Tilfælde\n",
    "\n",
    "**Grades Dataset (Dual Target):**\n",
    "```python\n",
    "if dataset_name == \"grades\":\n",
    "    # Regression target (GPA)\n",
    "    gpa_targets = train_targets_tensor  # Shape: [N] - kontinuerligt\n",
    "    # Klassifikation target (GradeClass)  \n",
    "    grade_class_targets = data['train_targets2']  # Shape: [N, num_classes] - one-hot\n",
    "    print(f\"GPA target shape: {gpa_targets.shape}\")\n",
    "    print(f\"GradeClass target shape: {grade_class_targets.shape}\")\n",
    "```\n",
    "\n",
    "**Avocado Dataset (Binær vs. Multiclass):**\n",
    "```python\n",
    "# Binær klassifikation (ripe vs. andre)\n",
    "data = load_dataset(\"avocado\", ripeness_class=\"ripe\")\n",
    "print(f\"Binary target shape: {data['train_targets'].shape}\")  # [N]\n",
    "\n",
    "# Multiclass klassifikation (alle kategorier)\n",
    "data = load_dataset(\"avocado\", ripeness_class=\"all\")\n",
    "print(f\"Multiclass target shape: {data['train_targets'].shape}\")  # [N, num_classes]\n",
    "```\n",
    "\n",
    "### Data Preprocessing Pipeline\n",
    "\n",
    "Alle datasæt gennemgår automatisk standardisering:\n",
    "\n",
    "1. **NaN Removal** - Rækker med manglende værdier fjernes helt\n",
    "2. **Train/Val/Test Split** - Automatisk opdeling (80/10/10% hvis validation=True)\n",
    "3. **Feature Normalization** - StandardScaler på input features (mean=0, std=1)\n",
    "4. **Target Processing**:\n",
    "   - **Regression**: StandardScaler normalisering (mean=0, std=1)\n",
    "   - **Classification**: One-hot encoding for multiclass, binær for binary\n",
    "5. **Tensor Conversion** - Konverteret til PyTorch tensors på korrekt device (CPU/GPU)\n",
    "\n",
    "### Visualisering\n",
    "\n",
    "Sæt `visualize=True` for at se:\n",
    "- **Corner plots** - Pairwise feature relationships\n",
    "- **Target distribution** - Histogram (regression) / bar plots (klassifikation)\n",
    "- **Correlation matrix** - Feature korrelationer\n",
    "- **Basic statistics** - Dataset størrelse og egenskaber\n",
    "\n",
    "### Eksempel Output\n",
    "```\n",
    "Dataset: diabetes\n",
    "Input size: 21\n",
    "Output size: 3  \n",
    "Task type: classification\n",
    "Number of classes: 3\n",
    "Training samples: 196036\n",
    "Test samples: 21782\n",
    "Feature names: ['HighBP', 'HighChol', 'CholCheck', ...]\n",
    "Target shape: [196036, 3]  # One-hot encoded med 3 klasser\n",
    "```\n",
    "\n",
    "### Vigtige Pointer for Netværksdesign\n",
    "\n",
    "**Output Layer Design:**\n",
    "- **Regression**: `nn.Linear(hidden_size, 1)` + ingen aktivering\n",
    "- **Binær klassifikation**: `nn.Linear(hidden_size, 1)` + sigmoid (eller BCEWithLogitsLoss)\n",
    "- **Multiclass**: `nn.Linear(hidden_size, num_classes)` + softmax\n",
    "\n",
    "**Loss Function Matching:**\n",
    "- **Regression**: MSELoss med kontinuerlige targets\n",
    "- **Binær**: BCELoss med 0/1 targets  \n",
    "- **Multiclass**: BCELoss med one-hot targets (eller CrossEntropyLoss med class indices)\n",
    "\n",
    "Denne struktur sikrer at jeres netværk får data i det rigtige format og gør det nemt at eksperimentere med forskellige arkitekturer på forskellige problemtyper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0dd3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (250, 9)\n",
      "After removing NaN values: (250, 9)\n",
      "Removed 0 rows with NaN values\n",
      "Avocado multiclass mapping: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
      "Task type: classification\n",
      "Number of classes: 5\n",
      "Dataset: avocado\n",
      "Input size: 7\n",
      "Output size: 5\n",
      "Training samples: 225\n",
      "Test samples: 25\n",
      "Feature names: ['firmness', 'hue', 'saturation', 'brightness', 'sound_db', 'weight_g', 'size_cm3']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset with visualization\n",
    "dataset_name = \"avocado\"  # Options: \"particle\", \"weather\", \"grades\", \"avocado\", \"diabetes\"\n",
    "validation = False # Set to True to include validation set\n",
    "visualize = False  # Set to True to see data visualizations\n",
    "\n",
    "data = load_dataset(dataset_name, validation=validation, visualize=visualize)\n",
    "\n",
    "# Extract commonly used variables for convenience\n",
    "X_train_tensor = data['X_train']\n",
    "X_test_tensor = data['X_test']\n",
    "train_targets_tensor = data['train_targets']  \n",
    "test_targets_tensor = data['test_targets']    \n",
    "input_size = data['input_size']\n",
    "output_size = data['output_size']\n",
    "\n",
    "# Get task type and number of classes from data\n",
    "task_type = data.get('task_type', 'regression')\n",
    "num_classes = data.get('num_classes', None)\n",
    "\n",
    "print(f\"Task type: {task_type}\")\n",
    "if task_type == 'classification' and num_classes:\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Check if there are any NaN values in the training data\n",
    "if torch.isnan(X_train_tensor).any():\n",
    "    print(\"Warning: Training data contains NaN values. Consider preprocessing to handle them.\")\n",
    "if torch.isnan(train_targets_tensor).any():\n",
    "    print(\"Warning: Training targets contain NaN values. Consider preprocessing to handle them.\")\n",
    "\n",
    "# Handle grades dataset with two targets\n",
    "if dataset_name == \"grades\" and 'train_targets2' in data:\n",
    "    train_targets_classification = data['train_targets2']  # GradeClass (classification)\n",
    "    test_targets_classification = data['test_targets2']    # GradeClass (classification)\n",
    "    if validation:\n",
    "        val_targets_classification = data['val_targets2']   # GradeClass (classification)\n",
    "    print(f\"Regression target (GPA): {train_targets_tensor.shape}\")\n",
    "    print(f\"Classification target (GradeClass): {train_targets_classification.shape}\")\n",
    "\n",
    "if validation:\n",
    "    X_val_tensor = data['X_val']\n",
    "    val_targets_tensor = data['val_targets']\n",
    "\n",
    "print(f\"Dataset: {data['dataset_name']}\")\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"Training samples: {X_train_tensor.shape[0]}\")\n",
    "if validation:\n",
    "    print(f\"Validation samples: {X_val_tensor.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_tensor.shape[0]}\")\n",
    "print(f\"Feature names: {data['feature_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f22fc",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "\n",
    "Her er det mest simple netværk I kan lave - det har kun 1 input og 1 output lag. \n",
    "\n",
    "### Netværksarkitektur\n",
    "\n",
    "Når I gerne vil lave flere lag, skal I bare sørge for at størrelserne passer sammen:\n",
    "\n",
    "**Eksempel på lag-dimensioner:**\n",
    "- Input lag: `input_size` → `hidden_size` (f.eks. 64)\n",
    "- Hidden lag: `64` → `64` (eller anden størrelse)\n",
    "- Output lag: `64` → `output_size`\n",
    "\n",
    "### Vigtige Punkter\n",
    "\n",
    "1. **Aktivering mellem lag**: Brug ReLU mellem hidden lag\n",
    "2. **Output aktivering**: \n",
    "   - **Regression**: Ingen aktivering på output (lineær)\n",
    "   - **Klassifikation**: Sigmoid (binær) eller ingen (multiclass med CrossEntropy)\n",
    "3. **Lag størrelse**: Typiske hidden dimensioner er 32, 64, 128, 256\n",
    "\n",
    "### Aktiveringsfunktioner\n",
    "\n",
    "Aktiveringsfunktioner introducerer ikke-linearitet i netværket og gør det muligt at lære komplekse mønstre:\n",
    "\n",
    "**Almindelige aktiveringsfunktioner:**\n",
    "- `nn.ReLU()` - Rectified Linear Unit (mest populære)\n",
    "- `nn.Sigmoid()` - Sigmoid funktion (0 til 1)\n",
    "- `nn.Tanh()` - Tanh funktion (-1 til 1)\n",
    "- `nn.LeakyReLU()` - Modificeret ReLU der ikke \"dør\"\n",
    "\n",
    "**Hvornår bruger vi aktivering:**\n",
    "```python\n",
    "# Mellem lag (altid)\n",
    "x = self.fc1(x)\n",
    "x = self.relu(x)  # Aktivering mellem lag\n",
    "\n",
    "# På output lag (kun for klassifikation)\n",
    "x = self.fc_output(x)\n",
    "# For regression: ingen aktivering\n",
    "# For binær klassifikation: sigmoid\n",
    "# For multiclass: ingen (bruger CrossEntropy loss)\n",
    "```\n",
    "\n",
    "### SimpleNN Forklaring\n",
    "\n",
    "Dette eksempel viser strukturen, men har en fejl - ReLU skal ikke bruges på output ved regression:\n",
    "\n",
    "```python\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)  # Direkte forbindelse\n",
    "        # ReLU bruges normalt mellem lag, ikke på output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # For regression: ingen aktivering på output\n",
    "        return x\n",
    "```\n",
    "\n",
    "### Brug af train_model Funktionen\n",
    "\n",
    "`train_model` funktionen gør træning nemt ved at håndtere alle de tekniske detaljer:\n",
    "\n",
    "```python\n",
    "# Grundlæggende brug\n",
    "history, trained_model = train_model(\n",
    "    model=model, \n",
    "    X_train=X_train_tensor, \n",
    "    y_train=train_targets_tensor,\n",
    "    task_type='regression'  # eller 'classification'\n",
    ")\n",
    "```\n",
    "\n",
    "**Avancerede parametre:**\n",
    "```python\n",
    "history, trained_model = train_model(\n",
    "    model=model,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=train_targets_tensor,\n",
    "    X_val=X_val_tensor,           # Validation data (optional)\n",
    "    y_val=val_targets_tensor,     # Validation targets (optional)\n",
    "    X_test=X_test_tensor,         # Test data (optional)\n",
    "    y_test=test_targets_tensor,   # Test targets (optional)\n",
    "    task_type='regression',       # 'regression' eller 'classification'\n",
    "    epochs=100,                   # Antal trænings epochs\n",
    "    learning_rate=0.001,          # Learning rate for optimizer\n",
    "    batch_size=32,                # Batch størrelse\n",
    "    early_stopping_patience=10,   # Stop hvis ingen forbedring\n",
    "    print_every=10               # Print status hver 10. epoch\n",
    ")\n",
    "```\n",
    "\n",
    "**Hvad returnerer train_model:**\n",
    "- `history`: Dictionary med trænings/validation loss over tid\n",
    "- `trained_model`: Det trænede netværk klar til brug\n",
    "\n",
    "**Automatiske features:**\n",
    "- Vælger \"rigtig\" loss funktion (MSE/CrossEntropy)\n",
    "- Adam optimizer\n",
    "- Device handling (CPU/GPU)\n",
    "- Early stopping ved validation\n",
    "- Progress tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a6efc",
   "metadata": {},
   "source": [
    "### Opgave\n",
    "\n",
    "Nu skal I prøve at lave jeres eget netværk og træne det på et af datasættene. \n",
    "\n",
    "**Krav til jeres netværk:**\n",
    "- 2-4 lag total (inkl. input og output lag)\n",
    "- ReLU aktivering mellem hidden lag\n",
    "- Passende output aktivering:\n",
    "  - **Regression**: Ingen aktivering på output\n",
    "  - **Binær klassifikation**: Sigmoid aktivering (kun én klasse output)\n",
    "  - **Multiclass klassifikation**: Softmax aktivering (kun én klasse kan være sand)\n",
    "\n",
    "**Om Binær Klassifikation:**\n",
    "De tilgængelige datasæt er primært regression og multiclass klassifikation. For at eksperimentere med binær klassifikation kan I:\n",
    "- Bruge avocado datasættet med `ripeness_class=\"ripe\"` (ripe vs. andre)\n",
    "- Modificere et multiclass dataset til binær (f.eks. konvertere diabetes til \"diabetes vs. ingen diabetes\")\n",
    "\n",
    "**Test jeres netværk:**\n",
    "Efter I har implementeret jeres netværk, test det med `train_model`s grundlæggende brug:\n",
    "\n",
    "```python\n",
    "# Grundlæggende træning\n",
    "history, trained_model = train_model(\n",
    "    model=model, \n",
    "    X_train=X_train_tensor, \n",
    "    y_train=train_targets_tensor,\n",
    "    task_type='regression'  # eller 'classification'\n",
    ")\n",
    "```\n",
    "\n",
    "**Tips:**\n",
    "- Start simpelt (2-3 lag)\n",
    "- Brug `hidden_size=64` til at starte med\n",
    "- Husk at sætte korrekt `task_type` baseret på jeres dataset\n",
    "- Tjek at jeres netværks dimensioner passer med `input_size` og `output_size`\n",
    "- For binær klassifikation: brug sigmoid aktivering på output (værdi mellem 0-1)\n",
    "- For multiclass: brug softmax aktivering på output (sandsynlighedsfordeling over klasser)\n",
    "\n",
    "**Binær Klassifikation Eksempel:**\n",
    "```python\n",
    "# Load avocado dataset for binary classification\n",
    "data = load_dataset(\"avocado\", ripeness_class=\"ripe\")\n",
    "# Dette giver binary targets: ripe (1) vs. alle andre (0)\n",
    "\n",
    "# Eller konverter multiclass til binary:\n",
    "# For diabetes: konverter til \"har diabetes\" vs. \"ingen diabetes\"\n",
    "if dataset_name == \"diabetes\":\n",
    "    # Konverter one-hot encoded targets til binary\n",
    "    # [1,0,0] -> 0 (ingen diabetes)\n",
    "    # [0,1,0] eller [0,0,1] -> 1 (har diabetes)\n",
    "    binary_targets = (train_targets_tensor[:, 1] + train_targets_tensor[:, 2]).unsqueeze(1)\n",
    "    print(f\"Binary target shape: {binary_targets.shape}\")  # [N, 1]\n",
    "```\n",
    "\n",
    "**Train_model Loss Functions:**\n",
    "> **Note:** `train_model` funktionen bruger automatisk BCELoss for klassifikation (både binær og multiclass) og MSELoss for regression. Dette betyder at for klassifikation skal jeres netværk have passende aktivering på output laget (sigmoid for binær, softmax for multiclass) da BCELoss forventer sandsynligheder mellem 0-1.\n",
    "\n",
    "**Aktivering vs. Loss Function:**\n",
    "- **Multiclass med Softmax**: Bruger BCELoss fordi targets er one-hot encoded\n",
    "- **Binær med Sigmoid**: Bruger BCELoss med enkelt output  \n",
    "- **Regression**: Bruger MSELoss med kontinuerlige targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686101d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JeresNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dimension=64, hidden_layers=2):\n",
    "        super(JeresNN, self).__init__()\n",
    "        # Her skal du definere de lag, aktiveringsfunktioner og andre komponenter i dit neurale netværk\n",
    "\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "        # TODO Indsæt din kode her\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Her skal du definere, hvordan data passerer gennem netværket\n",
    "        # TODO Indsæt din kode her\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa08a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c373978",
   "metadata": {},
   "source": [
    "### Opgave - Implementer Jeres Eget Trænings Loop\n",
    "\n",
    "Nu når jeres netværk virker, skal I skrive jeres eget trænings loop fra bunden. Dette giver jer en dybere forståelse af hvad der sker \"under motorhjelmen\" når I bruger `train_model` funktionen.\n",
    "\n",
    "> **📚 Læs først:** Se afsnittet om \"Matematikken\" i `NN-forklaring.ipynb` for at forstå hvad der sker under træning.\n",
    "\n",
    "#### Step 1: Vælg Loss Funktion og Hyperparametre\n",
    "\n",
    "Først skal I vælge de rigtige komponenter til jeres trænings setup:\n",
    "\n",
    "**Loss Funktioner:**\n",
    "- **MSE (Mean Squared Error)** for regression problemer\n",
    "- **CrossEntropyLoss** for klassifikation problemer\n",
    "- **BCELoss** for binær klassifikation med sigmoid output\n",
    "\n",
    "**Hyperparametre I skal vælge:**\n",
    "- `learning_rate` (f.eks. 0.001) - hvor store skridt optimizeren tager\n",
    "- `max_epochs` (f.eks. 100) - hvor mange gange I kører gennem alt data\n",
    "- `batch_size` (f.eks. 32) - hvor mange samples I behandler ad gangen\n",
    "\n",
    "> **💡 Tip:** Start konservativt med learning_rate=0.001. Hvis træning er for langsom, prøv 0.01. Hvis loss eksploderer, prøv 0.0001.\n",
    "\n",
    "#### Step 2: Forstå Trænings Loop Strukturen\n",
    "\n",
    "Et trænings loop har denne struktur:\n",
    "\n",
    "```\n",
    "For hver epoch (1 til max_epochs):\n",
    "    Sæt model i trænings mode\n",
    "    For hver batch i træningsdata:\n",
    "        1. Flyt data til korrekt device (CPU/GPU)\n",
    "        2. Nulstil gradienter\n",
    "        3. Forward pass (beregn output)\n",
    "        4. Beregn loss\n",
    "        5. Backward pass (beregn gradienter) \n",
    "        6. Optimizer step (opdater vægte)\n",
    "    Print loss for denne epoch\n",
    "```\n",
    "\n",
    "> **🔍 Forklaring:** Læs afsnittet om \"Epochs\" i `NN-forklaring.ipynb` for at forstå hvorfor vi gentager denne proces.\n",
    "\n",
    "#### Step 3: Implementer Trænings Loopet\n",
    "\n",
    "**Ydre loop - Epochs:**\n",
    "- Lav en for-loop som kører `max_epochs` gange\n",
    "- Kald `model.train()` for at sætte modellen i trænings mode\n",
    "- Opret en variabel til at holde styr på den samlede loss for denne epoch\n",
    "\n",
    "**Indre loop - Batches:**\n",
    "- Lav en for-loop over `train_loader` med `for X, y in train_loader:`\n",
    "- Dette giver jer både input data (`X`) og targets (`y`) for hver batch\n",
    "\n",
    "**Data Handling:**\n",
    "```python\n",
    "# Flyt data til korrekt device\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "```\n",
    "\n",
    "**Den Klassiske Trænings Sekvens:**\n",
    "```python\n",
    "# 1. Nulstil gradienter fra forrige batch\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 2. Forward pass - hvad forudsiger modellen?\n",
    "outputs = model(X)\n",
    "\n",
    "# 3. Beregn loss mellem forudsigelse og virkelighed\n",
    "loss = criterion(outputs, y)\n",
    "\n",
    "# 4. Backward pass - beregn gradienter\n",
    "loss.backward()\n",
    "\n",
    "# 5. Optimizer step - opdater model vægte\n",
    "optimizer.step()\n",
    "\n",
    "# 6. Akkumuler loss for denne batch\n",
    "epoch_loss += loss.item()\n",
    "```\n",
    "\n",
    "**Print Progress:**\n",
    "Efter hver epoch, print den gennemsnitlige loss:\n",
    "```python\n",
    "avg_loss = epoch_loss / len(train_loader)\n",
    "print(f'Epoch [{epoch+1}/{max_epochs}], Loss: {avg_loss:.4f}')\n",
    "```\n",
    "\n",
    "#### Step 4: Debugging Tips\n",
    "\n",
    "**Hvis jeres loss ikke falder:**\n",
    "- Tjek at I bruger den rigtige loss funktion for jeres problem type\n",
    "- Prøv en lavere learning rate\n",
    "- Kontroller at jeres netværk har passende aktivering på output\n",
    "\n",
    "**Hvis loss eksploderer (bliver meget stor):**\n",
    "- Reducer learning rate betydeligt (f.eks. fra 0.001 til 0.0001)\n",
    "- Tjek at jeres data er normaliseret korrekt\n",
    "\n",
    "**Hvis træning er for langsom:**\n",
    "- Øg learning rate forsigtigt\n",
    "- Reducer model størrelse eller batch size\n",
    "\n",
    "> **⚠️ Almindelige Fejl:**\n",
    "> - Glemme `optimizer.zero_grad()` - dette får gradienter til at akkumulere\n",
    "> - Glemme `loss.backward()` - ingen gradienter bliver beregnet\n",
    "> - Forkert loss funktion for problem type\n",
    "> - Data ikke på samme device som model\n",
    "\n",
    "#### Step 5: Sammenlign med train_model\n",
    "\n",
    "Efter I har implementeret jeres eget trænings loop, sammenlign det med `train_model` funktionen:\n",
    "- Får I lignende loss værdier?\n",
    "- Træner jeres model lige så hurtigt?\n",
    "- Hvad er forskellen i performance?\n",
    "\n",
    "> **🎯 Mål:** I skal kunne implementere et funktionelt trænings loop der giver lignende resultater som `train_model` funktionen. Dette viser at I forstår de grundlæggende principper bag neural network træning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad7795",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m train_dataset = torch.utils.data.TensorDataset(X_train_tensor, train_targets_tensor)\n\u001b[32m     13\u001b[39m train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m optimizer = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ML/lib/python3.12/site-packages/torch/optim/adam.py:100\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor betas[1] must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m defaults = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     88\u001b[39m     lr=lr,\n\u001b[32m     89\u001b[39m     betas=betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     decoupled_weight_decay=decoupled_weight_decay,\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ML/lib/python3.12/site-packages/torch/optim/optimizer.py:364\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    362\u001b[39m param_groups = \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33moptimizer got an empty parameter list\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[32m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    366\u001b[39m     param_groups = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: param_groups}]\n",
      "\u001b[31mValueError\u001b[39m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "#Define the loss function\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression tasks\n",
    "#criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "max_epochs = 100\n",
    "\n",
    "# Initialize the model, optimizer, and data loader\n",
    "model = JeresNN(input_size, output_size).to(device)  # Initialize your custom model\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, train_targets_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515e0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df2382e5",
   "metadata": {},
   "source": [
    "### Opgave - Evaluering af Jeres Trænede Model\n",
    "\n",
    "Nu har I det mest basale om hvordan man træner et neuralt netværk, så nu skal I evaluere det. Det er som udgangspunkt bare at sige `model(jeres_data_her)`. Det vigtige er at I skriver `model.eval()` først for at gå ud af trænings mode.\n",
    "\n",
    "Nu skal I prøve at se hvor langt jeres forudsagte test data er fra de rigtige værdier.\n",
    "\n",
    "#### For Regression:\n",
    "- **Standard afvigelse**: `torch.std(forudsigelse - rigtig)` \n",
    "- **Root Mean Square Error (RMSE)**: `torch.sqrt(torch.mean((forudsigelse - rigtig)**2))`\n",
    "- **Mean Absolute Error (MAE)**: `torch.mean(torch.abs(forudsigelse - rigtig))`\n",
    "\n",
    "#### For Klassifikation:\n",
    "- **Accuracy (nøjagtighed)**: `torch.mean((forudsigelse > 0.5) == rigtig)` for binær klassifikation\n",
    "- **For multiclass**: `torch.mean(torch.argmax(forudsigelse, dim=1) == torch.argmax(rigtig, dim=1))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401a4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db98475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d04f5a2",
   "metadata": {},
   "source": [
    "### Opgave - Eksperimentering og Videreudvikling\n",
    "\n",
    "Leg med det! Den bedste måde at få en god idé om hvad der virker i machine learning er at eksperimentere mange gange. Derfor synes vi bare I skal lege med det og prøve forskellige ting.\n",
    "\n",
    "> **📚 Læs først:** Se `NN-forklaring.ipynb` for dybere forståelse af de koncepter der nævnes herunder.\n",
    "\n",
    "#### Forslag til Eksperimenter\n",
    "\n",
    "**Grundlæggende Forbedringer:**\n",
    "- **Validering og Early Stopping** - Se afsnittet om \"Validation og Early stopping\" i `NN-forklaring.ipynb`\n",
    "  - Sæt `validation=True` når I loader data\n",
    "  - Brug `train_model` med validation parametre for automatisk early stopping\n",
    "  - Implementer jeres eget early stopping i jeres trænings loop\n",
    "\n",
    "- **Netværksarkitektur** - Se afsnittet om \"Matematikken\" for forståelse af lag-strukturer\n",
    "  - Prøv forskellige antal lag (2-6 lag)\n",
    "  - Eksperimentér med forskellige hidden_dimensions (32, 64, 128, 256)\n",
    "  - Test forskellige aktiveringsfunktioner mellem lag\n",
    "\n",
    "**Avancerede Teknikker:**\n",
    "- **Dropout for Regularisering** - Se afsnittet om \"Overtræning\" → \"Dropout\"\n",
    "  - Tilføj `nn.Dropout(p=0.2)` mellem jeres lag\n",
    "  - Prøv forskellige dropout rates (0.1, 0.3, 0.5)\n",
    "  - Sammenlign performance med og uden dropout\n",
    "\n",
    "- **Forskellige Aktiveringsfunktioner** - Se afsnittet om \"Aktiverings Funktioner\"\n",
    "  - Prøv `nn.LeakyReLU()` i stedet for `nn.ReLU()`\n",
    "  - Test `nn.Tanh()` på hidden lag\n",
    "  - Eksperimentér med forskellige output aktiveringer baseret på jeres problem type\n",
    "\n",
    "**Problem-Specifik Eksperimentering:**\n",
    "- **Klassifikation vs. Regression** - Se afsnittene om \"Klassifikation\" og \"Regression\"\n",
    "  - Sammenlign performance på forskellige datasæt\n",
    "  - Prøv at konvertere multiclass til binær klassifikation\n",
    "  - Test forskellige loss funktioner og evalueringsmetrikker\n",
    "\n",
    "- **Cross Validation** - Se afsnittet om \"Cross validation\" i `NN-forklaring.ipynb`\n",
    "  - Implementer k-fold cross validation\n",
    "  - Sammenlign med simple train/validation split\n",
    "\n",
    "#### Praktisk Brug af train_model\n",
    "\n",
    "`train_model` funktionen kan håndtere både evaluering og validering automatisk:\n",
    "\n",
    "```python\n",
    "# Fuld setup med validering og test evaluering\n",
    "history, trained_model = train_model(\n",
    "    model=model, \n",
    "    X_train=X_train_tensor, \n",
    "    y_train=train_targets_tensor,\n",
    "    X_val=X_val_tensor if validation else None,     # Automatisk early stopping\n",
    "    y_val=val_targets_tensor if validation else None,\n",
    "    X_test=X_test_tensor,                           # Automatisk test evaluering\n",
    "    y_test=test_targets_tensor,\n",
    "    task_type='regression',  # eller 'classification'\n",
    "    epochs=200,              # Højere da early stopping stopper automatisk\n",
    "    early_stopping_patience=15  # Stop hvis ingen forbedring i 15 epochs\n",
    ")\n",
    "```\n",
    "\n",
    "#### Debugging og Problemløsning\n",
    "\n",
    "**Hvis jeres model ikke lærer:** Se \"Hvad kan gå galt\" sektionerne i `NN-forklaring.ipynb`\n",
    "- Tjek for overtræning (høj træning accuracy, lav test accuracy)\n",
    "- Kontroller data skala og normalisering\n",
    "- Prøv forskellige learning rates\n",
    "- Undersøg om I har nok data til jeres model kompleksitet\n",
    "\n",
    "**Hvis I får mærkelige resultater:**\n",
    "- Sammenlign med baseline (hvad får I hvis I altid gætter gennemsnittet?)\n",
    "- Visualiser jeres predictions vs. targets\n",
    "- Tjek om jeres data har bias eller outliers\n",
    "\n",
    "#### Spørgsmål og Hjælp\n",
    "\n",
    "Spørg gerne hvis der er noget I har problemer med, eller noget mere I gerne vil prøve! Nogle gode spørgsmål at stille sig selv:\n",
    "\n",
    "- Hvordan påvirker antal lag performance?\n",
    "- Hvad sker der hvis I bruger meget små eller store learning rates?\n",
    "- Kan I finde det optimale antal epochs før overtræning?\n",
    "- Hvilken aktiverings funktion virker bedst for jeres problem?\n",
    "- Hvordan påvirker dropout rate performance?\n",
    "\n",
    "> **💡 Tip:** Start med simple ændringer og byg kompleksiteten op gradvist. Document hvad I prøver så I kan sammenligne resultater!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba488f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18717a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab13e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
