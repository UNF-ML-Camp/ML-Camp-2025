{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d157dc81",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Matematikken bag Neurale Netværk\n",
    "## Bagtanker\n",
    "\n",
    "Her er der opgaver som er lavet til at give jer en fuldstændig forståelse for hvordan Neurale Netværk virker. Det sker ved at I skal bygge et og optimere med numpy som ikke er bygget til det så i skal selv lave beregningerne med matricerne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de238480",
   "metadata": {},
   "source": [
    "## Pakker\n",
    "Her er numpy og matplot som er alle de pakker i har brug for, for at løse opgaverne her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bfc2b",
   "metadata": {},
   "source": [
    "## Vægte\n",
    "\n",
    "Her skal i lave to funktioner, en som initialisere tilfældige vægte og en som laver et netværk udfra de vægte.\n",
    "\n",
    "**Hvad er vægte og bias:**\n",
    "- **Vægte (W):** Bestemmer hvor stærk forbindelsen er mellem neuroner i forskellige lag\n",
    "- **Bias (b):** Tillader netværket at forskyde aktiveringsgrænsen for hver neuron\n",
    "- Vægte er matricer med dimensioner (input_size, output_size)\n",
    "- Bias er vektorer med længde output_size\n",
    "\n",
    "**Initialisering:**\n",
    "- Vægte initialiseres med tilfældige værdier (brug `np.random.randn()`)\n",
    "- Bias kan initialiseres til nul (brug `np.zeros()`)\n",
    "- Korrekte dimensioner er kritiske for matrix multiplikation\n",
    "\n",
    "**Tips:**\n",
    "- For `init_weight`: returner en vægt-matrix (dim_in × dim_out) og bias-vektor (dim_out)\n",
    "- For `init_NN`: brug et loop til at oprette vægte for hvert lag\n",
    "- Husk at det første lag tager X_dim som input, derefter bruger du L værdierne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(dim_in: int, dim_out: int):\n",
    "    # TODO: Her skal du initialisere vægtene som matrice og bias som vektor for et lag i et neuralt netværk.\n",
    "    W = np.random.randn(dim_out, dim_in)\n",
    "    b = np.zeros(dim_out)\n",
    "    return W, b\n",
    "\n",
    "def init_NN(X_dim: int, L: list[int]):\n",
    "    weights, biases = [], []\n",
    "    prev_dim = X_dim\n",
    "\n",
    "    for out_dim in L:\n",
    "        W, b = init_weight(prev_dim, out_dim)\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "        prev_dim = out_dim\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_NN_params(weights: list, biases: list):\n",
    "    print(f\"Netværket har {len(weights)} lag, med hhv. {', '.join([f'{weights[i].shape[1]} neuroner i lag ({i+1})' for i in range(len(weights))])}\")\n",
    "    print()\n",
    "    for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "        print(f'W^({i+1}):')\n",
    "        print(weight)\n",
    "        print(f'b^({i+1}):')\n",
    "        print(bias)\n",
    "        print()\n",
    "\n",
    "print_NN_params(*init_NN(X_dim=3, L = [2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73305f1b",
   "metadata": {},
   "source": [
    "## Aktiveringsfunktioner\n",
    "\n",
    "Her skal I implementere de forskellige aktiveringsfunktioner. Aktiveringsfunktioner bestemmer om en neuron skal \"aktiveres\" eller ej, og introducerer non-linearitet i netværket.\n",
    "\n",
    "**Hvad gør aktiveringsfunktioner:**\n",
    "- Tager input z (linear transformation) og transformerer det til en aktivering a\n",
    "- Introducerer non-linearitet - uden dem ville netværket kun kunne lære lineære sammenhænge\n",
    "- Forskellige funktioner har forskellige egenskaber og anvendelser\n",
    "\n",
    "**De forskellige funktioner:**\n",
    "- **ReLU:** Simpel og hurtig, men kan \"dø\" (gradient bliver 0)\n",
    "- **Leaky ReLU:** Som ReLU men undgår \"døde\" neuroner\n",
    "- **Sigmoid:** Squasher output til [0,1], men kan have vanishing gradient problem\n",
    "- **Tanh:** Som sigmoid men output er [-1,1]\n",
    "- **Softmax:** Bruges til klassifikation - konverterer til sandsynligheder\n",
    "\n",
    "Vi har lavet de afledte for jer som i kan tage inspiration fra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(z: np.ndarray, return_derivative: bool = False) -> np.ndarray:\n",
    "    if return_derivative:\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    else:\n",
    "        # TODO: Implementer ReLu\n",
    "        return ...\n",
    "\n",
    "def tanh(z: np.ndarray, return_derivative: bool = False) -> np.ndarray:\n",
    "    if return_derivative:\n",
    "        return 1 - tanh(z)**2\n",
    "    else:\n",
    "        # TODO: Implementer tanh\n",
    "        return ...\n",
    "\n",
    "def sigmoid(z: np.ndarray, return_derivative: bool = False) -> np.ndarray:\n",
    "    if return_derivative:\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "    else:\n",
    "        # TODO: Implementer sigmoid\n",
    "        return ...\n",
    "\n",
    "def leaky_ReLu(z: np.ndarray, alpha: float = 0.1, return_derivative: bool = False) -> np.ndarray:\n",
    "    if return_derivative:\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "    else:\n",
    "        # TODO: Implementer leaky ReLu\n",
    "        return ...\n",
    "\n",
    "def softmax(z: np.ndarray, return_derivative: bool = False) -> np.ndarray:\n",
    "    if return_derivative:\n",
    "        return softmax(z) * (np.ones(z.shape) - softmax(z))\n",
    "    else:\n",
    "        # TODO: Implementer softmax\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f68110",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_funcs = [tanh, sigmoid, ReLu, leaky_ReLu]\n",
    "fig, ax = plt.subplots(1, len(activation_funcs), figsize=(15, 4), sharey=True, layout='tight')\n",
    "ax[0].set_ylabel('$f(z)$')\n",
    "z = np.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "for i, f in enumerate(activation_funcs):\n",
    "    ax[i].plot(z, f(z), label=f.__name__)\n",
    "    ax[i].plot(z, f(z, return_derivative=True), '--', color='tab:blue')\n",
    "    ax[i].set_title(f.__name__)\n",
    "    ax[i].plot(z, np.zeros_like(z), 'k--', linewidth=0.5)\n",
    "    ax[i].plot(np.zeros_like(z), z, 'k--', linewidth=0.5)\n",
    "    ax[i].set_ylim([-2, 2])\n",
    "    ax[i].set_xlim([-5, 5])\n",
    "    ax[i].grid(True)\n",
    "    ax[i].set_xlabel('$z$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67518d7a",
   "metadata": {},
   "source": [
    "## Fremadpropagering (Forward Propagation)\n",
    "\n",
    "Her skal I færdiggøre fremadpropagering, som er processen hvor data flyder gennem netværket fra input til output.\n",
    "\n",
    "**Hvad sker der i hvert lag:**\n",
    "1. **Linear transformation:** $$z = X \\cdot W + b$$\n",
    "2. **Aktivering:** $$a = f(z)$$\n",
    "\n",
    "**Proces:**\n",
    "- Tag input data (X) \n",
    "- Gang med vægte (W) og læg bias (b) til → dette giver z\n",
    "- Anvend aktiveringsfunktion på z → dette giver a (aktivering)\n",
    "- a bliver input til næste lag\n",
    "\n",
    "**Tips:**\n",
    "- Brug `np.dot` for matrix multiplikation\n",
    "- Husk at tilføje bias: `+ b`\n",
    "- Anvend aktiveringsfunktionen\n",
    "- Den sidste aktivering er dit output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X: np.ndarray, weights: list, biases: list, activation_funcs: list) -> np.ndarray:\n",
    "    a = X.T\n",
    "    for i, (W, b, f) in enumerate(zip(weights, biases, activation_funcs)):\n",
    "        # TODO: Implementer fremadpropagering i det neurale netværk.\n",
    "        ...\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.2, 0.8], [2.5, 3], [0.3, -0.9],[4, 3.5]])\n",
    "weights, biases = init_NN(X_dim=2, L=[3, 3, 3, 3, 2])\n",
    "print(biases[0].shape)\n",
    "fs = [tanh, ReLu, leaky_ReLu, ReLu, softmax]\n",
    "print(forward(X, weights, biases, fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ad664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(prob: np.ndarray) -> np.ndarray:\n",
    "    return np.argmax(prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97446cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = forward(X, weights, biases, fs)\n",
    "y = get_label(y_prob)\n",
    "print(f\"Klassifikation: {y}\")\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8af153",
   "metadata": {},
   "source": [
    "## Loss funktioner\n",
    "Her skal i implementere nogle forskellige loss funktioner\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "**Matematik:** $$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_{true} - y_{pred})^2$$\n",
    "- Bruges til regression problemer\n",
    "- Straffer store fejl mere end små fejl (kvadratisk)\n",
    "\n",
    "### Mean Absolute Error (MAE) \n",
    "**Matematik:** $$MAE = \\frac{1}{n} \\sum_{i=1}^{n}|y_{true} - y_{pred}|$$\n",
    "- Bruges til regression problemer\n",
    "- Mere robust overfor outliers end MSE\n",
    "\n",
    "### Categorical Cross-Entropy (CCE)\n",
    "**Matematik:** $$CCE = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{C} y_{true,i,j} \\log(y_{pred,i,j})$$\n",
    "- Bruges til multi-class klassifikation\n",
    "- y_true skal være one-hot encoded\n",
    "- y_pred skal være sandsynligheder (softmax output)\n",
    "\n",
    "### Binary Cross-Entropy (BCE)\n",
    "**Matematik:** $$BCE = -\\frac{1}{n} \\sum_{i=1}^{n}[y_{true,i} \\log(y_{pred,i}) + (1-y_{true,i}) \\log(1-y_{pred,i})]$$\n",
    "- Bruges til binær klassifikation  \n",
    "- y_true skal være 0 eller 1\n",
    "- y_pred skal være sandsynlighed mellem 0 og 1 (sigmoid output)\n",
    "\n",
    "**Tips til implementering:**\n",
    "- Brug `np.mean()` for gennemsnit\n",
    "- Tilføj en lille epsilon (f.eks. 1e-15) til log for at undgå log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    # TODO:\n",
    "    return ...\n",
    "\n",
    "def MAE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    # TODO:\n",
    "    return 1/len(y_true) * np.sum(abs(y_true - y_pred))\n",
    "\n",
    "def CCE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    # TODO:\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-15, 1.0)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred_clipped), axis=1))\n",
    "\n",
    "def BCE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    # TODO:\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-15, 1.0 - 1e-15)\n",
    "    return -np.mean(\n",
    "        y_true * np.log(y_pred_clipped) +\n",
    "        (1 - y_true) * np.log(1 - y_pred_clipped)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0aabb",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "**Ingen opgave her - bare kør koden!**\n",
    "\n",
    "Her genererer vi noget syntetisk data som I kan klassificere. Funktionen `data_generator()` laver 3 klasser af datapunkter, hvor hver klasse er centreret omkring forskellige punkter med lidt støj tilføjet. \n",
    "\n",
    "**Hvad sker der:**\n",
    "- Klasse 0: Data centreret omkring (0, 0)\n",
    "- Klasse 1: Data centreret omkring (1, 1) \n",
    "- Klasse 2: Data centreret omkring (2, 2)\n",
    "- Støj tilføjes for at gøre klassifikationen mere udfordrende\n",
    "- Data opdeles i træning (60%), validering (20%) og test (20%)\n",
    "\n",
    "Dette giver jer et simpelt klassifikationsproblem at teste jeres neurale netværk på."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(n_datapunkter: int = 3000, n_klasser: int = 3, n_dim: int = 2, støj: float = 0.9):\n",
    "    for klasse in range(n_klasser):\n",
    "        X_ = np.random.normal(klasse, støj, (n_datapunkter // n_klasser, n_dim))\n",
    "        y_ = np.full(X_.shape[0], klasse)\n",
    "        if klasse == 0:\n",
    "            X = X_\n",
    "            y = y_\n",
    "        else:\n",
    "            X = np.vstack([X, X_])\n",
    "            y = np.hstack([y, y_])\n",
    "\n",
    "    idx = np.random.permutation(X.shape[0])\n",
    "    X, y = X[idx], y[idx]\n",
    "    X_train, y_train = X[:int(0.6 * X.shape[0])], y[:int(0.6 * X.shape[0])]\n",
    "    X_val, y_val = X[int(0.6 * X.shape[0]):int(0.8 * X.shape[0])], y[int(0.6 * X.shape[0]):int(0.8 * X.shape[0])]\n",
    "    X_test, y_test = X[int(0.8 * X.shape[0]):], y[int(0.8 * X.shape[0]):]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test  \n",
    "\n",
    "def plot_data(X: np.ndarray, y: np.ndarray, axs: plt.Axes = None):\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    for klasse in np.unique(y):\n",
    "        axs.scatter(X[y == klasse, 0], X[y == klasse, 1], label=f'Klasse {klasse}')\n",
    "    axs.legend()\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = data_generator()\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, x, y in zip(range(3), [X_train, X_val, X_test], [y_train, y_val, y_test]):\n",
    "    plot_data(x, y, ax[i])\n",
    "    ax[i].set_title(['Træningsdata', 'Valideringsdata', 'Testdata'][i])\n",
    "    ax[i].set_xlabel('$x_1$')\n",
    "    ax[i].set_ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993e562",
   "metadata": {},
   "source": [
    "# Træning\n",
    "\n",
    "Her skal i færdiggøre træningen af jeres neurale netværk. Træning består af tre hovedkomponenter:\n",
    "\n",
    "## Backpropagation (Baglæns propagering)\n",
    "**Formål:** Beregne hvor meget hver vægt og bias skal ændres for at minimere fejlen.\n",
    "\n",
    "**Matematik:** Backpropagation bruger kædereglen til at beregne gradienter:\n",
    "- **Output lag:** $$\\delta^{(L)} = \\frac{dL}{dz^{(L)}} = a^{(L)} - y_{true}$$ (for softmax + cross-entropy)\n",
    "- **Skjulte lag:** $$\\delta^{(l)} = (W^{(l+1)})^T \\delta^{(l+1)} \\times f'(z^{(l)})$$ (element-wise multiplikation)\n",
    "\n",
    "**Gradienter:**\n",
    "- **Vægte:** $$\\frac{dL}{dW^{(l)}} = a^{(l-1)} (\\delta^{(l)})^T$$\n",
    "- **Bias:** $$\\frac{dL}{db^{(l)}} = \\delta^{(l)}$$\n",
    "\n",
    "## Parameterudatering\n",
    "**Gradient Descent:** Opdater parametre i den modsatte retning af gradienten:\n",
    "- $$W^{(l)} = W^{(l)} - \\alpha \\frac{dL}{dW^{(l)}}$$\n",
    "- $$b^{(l)} = b^{(l)} - \\alpha \\frac{dL}{db^{(l)}}$$\n",
    "\n",
    "hvor $\\alpha$ er learning rate (læringsraten).\n",
    "\n",
    "## Træningsloop\n",
    "1. **Forward pass:** Beregn output og aktivationer\n",
    "2. **Beregn loss:** Sammenlign output med rigtige labels\n",
    "3. **Backward pass:** Beregn gradienter\n",
    "4. **Opdater parametre:** Anvend gradient descent\n",
    "5. **Gentag** for hver epoch\n",
    "\n",
    "**Tips til implementering:**\n",
    "- Husk at matricerne skal have de rigtige dimensioner for matrix multiplikation\n",
    "- Brug `np.dot()` for matrix multiplikation mellem aktivationer og vægte\n",
    "- Forward funktionen skal returnere både aktivationer og z-værdier for backpropagation\n",
    "- For accuracy: sammenlign `np.argmax(y_pred, axis=1)` med rigtige labels\n",
    "- Print shapes af matricer for at debugge dimensionsfejl\n",
    "- Start med simple netværk (få lag) for at teste implementeringen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51168a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dim = X_train.shape[1]\n",
    "L = [10, 10, 10, 3] # TODO: Definer lagstørrelserne for det neurale netværk, f.eks. [10, 10, 10, 3] for et netværk med 3 skjulte lag og 3 output neuroner.\n",
    "weights, biases = init_NN(X_dim, L)\n",
    "fs = [ReLu, leaky_ReLu, ReLu, softmax] # TODO: Definer aktiveringsfunktionerne for hvert lag i det neurale netværk, f.eks. [ReLu, leaky_ReLu, ReLu, softmax] for et netværk med 3 skjulte lag og 1 output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sæt weights her\n",
    "def backward(y_true, activations, zs, weights, biases, activation_funcs):\n",
    "    deltas = [None] * len(weights)\n",
    "    grads_w = [None] * len(weights)\n",
    "    grads_b = [None] * len(biases)\n",
    "\n",
    "    # Compute the delta for the last layer\n",
    "    delta = # TODO: Beregn delta for det sidste lag, f.eks. ved at bruge softmax og CCE.\n",
    "    deltas[-1] = delta\n",
    "\n",
    "    # Backpropagate the error\n",
    "    for l in range(len(weights)-2, -1, -1):\n",
    "        delta = # TODO: beregn delta for det aktuelle lag.\n",
    "        deltas[l] = delta\n",
    "\n",
    "    # Compute gradients\n",
    "    for l in range(len(weights)):\n",
    "        grads_w[l] = # TODO: beregn gradienten for vægtene i det aktuelle lag.\n",
    "        grads_b[l] = # TODO: beregn gradienten for bias i det aktuelle lag.\n",
    "\n",
    "    return grads_w, grads_b\n",
    "\n",
    "def update_parameters(weights, biases, grads_w, grads_b, learning_rate):\n",
    "    for l in range(len(weights)):\n",
    "        # TODO: Opdater vægte og bias for hvert lag i det neurale netværk. Hvor de er vaegtet med læringsraten.\n",
    "        pass\n",
    "    return weights, biases\n",
    "\n",
    "def train(X, y, weights, biases, activation_funcs, epochs, learning_rate):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # One-hot encode y_true_train and y_true_val\n",
    "    N_klasser = np.unique(y).shape[0]\n",
    "    N_samples = y.shape[0]\n",
    "\n",
    "    y_true_one_hot = np.zeros((N_samples, N_klasser))\n",
    "    y_true_one_hot[np.arange(len(y)), y] = 1\n",
    "    y_val_one_hot = np.zeros((y_val.shape[0], N_klasser))\n",
    "    y_val_one_hot[np.arange(len(y_val)), y_val] = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        activations, zs = # TODO: Implementer fremadpropagering i det neurale netværk for træningsdata.\n",
    "        grads_w, grads_b = backward(y_true_one_hot, activations, zs, weights, biases, activation_funcs)\n",
    "\n",
    "        # Evaluate the loss\n",
    "        train_loss = # TODO implementer en loss funktion, f.eks. CCE\n",
    "        train_losses.append(train_loss)\n",
    "        # TODO: Tilføj en accuracy funktion for træningsdata og tilføj den til train_accuracies\n",
    "\n",
    "        val_activations, _ = # TODO: Implementer fremadpropagering i det neurale netværk for valideringsdata.\n",
    "        val_loss = # TODO implementer en loss funktion, f.eks. CCE\n",
    "        val_losses.append(val_loss)\n",
    "        # TODO: Tilføj en accuracy funktion for valideringsdata og tilføj den til val_accuracies\n",
    "\n",
    "        weights, biases = update_parameters(weights, biases, grads_w, grads_b, learning_rate)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch}{\" \" if len(str(epoch)) == 1 else \"\"} ### Train Loss: {train_loss} ### Val Loss: {val_loss}')\n",
    "    \n",
    "    metrics = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "    return weights, biases, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51146a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(weight.shape) for weight in weights]\n",
    "\n",
    "# Choose hyperparameters\n",
    "epochs = 30\n",
    "learning_rate = 0.0001\n",
    "\n",
    "weights, biases, metrics = train(X_train, y_train, weights, biases, fs, epochs=epochs, learning_rate=learning_rate)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5), layout='tight')\n",
    "ax[0].plot(metrics['train_losses'], label='Training', linestyle='--', color='tab:blue')\n",
    "ax[0].plot(metrics['val_losses'], label='Validation', color='tab:orange')\n",
    "ax[0].set_title(\"Loss\")\n",
    "\n",
    "ax[1].plot(metrics['train_accuracies'], label='Training', linestyle='--', color='tab:blue')\n",
    "ax[1].plot(metrics['val_accuracies'], label='Validation', color='tab:orange')\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel('Epoch')\n",
    "\n",
    "# shared legend below the plots\n",
    "lines, labels = ax[0].get_legend_handles_labels()\n",
    "fig.legend(lines, labels, loc='lower center', ncol=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
