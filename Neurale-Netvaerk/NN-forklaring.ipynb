{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63124e31",
   "metadata": {},
   "source": [
    "# Neurale Netværk\n",
    "\n",
    "## Introduktion\n",
    "\n",
    "Der er to sæt intro opgaver til neurale netværk, den første er for at lære jer hvordan I bruger pytorch, og hvordan i skriver de forskellige beregninger ind. Mens den anden er en low level implementation, hvor I selv skal lave et forward step skrevet kun med numpy. Dette prøver at give jer en bedre forståelse for hvad et Neuralt Netværk faktisk gør. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd24d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Der er to store felter af Machine Learning, supervised og unsupervised learning. Forskellen kommer om hvad du bruger til at træne, hvis du ikke har et konkret svar, er det du bruger unsupervised learning, det bliver f.eks. ofte brugt inden for clustering, hvor KNN er en måde at lave supervised clustering.\n",
    "\n",
    "Alle Neurale Netværk er en del af supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b23a1",
   "metadata": {},
   "source": [
    "## Matematikken\n",
    "\n",
    "\n",
    "Et neuralt netværk består af lag af \"neuroner\", hvor hvert neuron laver en simpel matematisk beregning. Hvert neuron modtager nogle tal (inputs), ganger dem med nogle vægte, derefter lægger dem sammen og sender resultatet videre gennem en aktiveringsfunktion.\n",
    "\n",
    "Hvis vi kalder inputtene $( x_1, x_2, ..., x_n )$ og vægtene $( w_1, w_2, ..., w_n )$, så regner et neuron sådan her:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i \\cdot x_i + b\n",
    "$$\n",
    "\n",
    "Her er:\n",
    "- $( w_i )$ vægtene (hvor vigtige de forskellige inputs er)\n",
    "- $( x_i )$ inputtene\n",
    "- $( b )$ er en bias (en slags ekstra justering)\n",
    "- $( z )$ er summen, som sendes videre\n",
    "\n",
    "Derefter bruger vi en aktiveringsfunktion, fx den populære \"sigmoid\":\n",
    "\n",
    "$$\n",
    "a = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Her er $( a )$ outputtet fra neuronet.\n",
    "\n",
    "\n",
    "Det kan du også se som en graf, hvor alle neuronerne er forbundet med vægte, til dem på det tidligere eller senere lag. Dette koncept kan du se på figuren under her.\n",
    "\n",
    "<img src=\"1_YL2a2dbDQ5754h_ktDj8mQ.webp\" style=\"max-width:600px;\">\n",
    "\n",
    "*Billedet er fra [denne artikel om neurale netværk](https://medium.com/ravenprotocol/everything-you-need-to-know-about-neural-networks-6fcc7a15cb4).*\n",
    "\n",
    "Alt hvad der er forklaret tidligere her, er for et lag, dog har de fleste netværk flere lag som er lagt i serie. Det betyder at når jeg regner outputtet fra det første lag, vil det direkte kunne bruges som input til det næste. De to eneste lag som er specille er input og output. Input laget er der hvor den orignalle data bliver givet ind. Hvor output laget er der hvor du for det ud du prøver at forudsige. Der er diskussioner om hvad Dybe Neurale Netværk(Deep Neural Networks) er, det som jeg har hørt fra en som har arbejdet med det i mange år, er at dybe er når der er mere en et lag, da du bliver nødt til at splitte backpropergation op for at optimisere værdierne. \n",
    "\n",
    "Vi kommer ikke rigtig til at gå mere ind i backpropergation, men I kan tænke på at du prøver at regne hvad den ville give ud også ændre vægtene tilfældigt en lille smule også ser om det virker bedre. Så kommer resten bare hvordan du vælger hvilke vægte og hvor meget du skal ændre dem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53334a5",
   "metadata": {},
   "source": [
    "## Aktiverings Funktioner\n",
    "\n",
    "Vi skal bruge en aktiverings funktion for at kunne forudsige noget der ikke er linaert. Hvis relationen vi vil forudsige var en ret linje så ville vi ikke have brug for denne ekstra compleksitet, men de fleste problemer er ikke en linaer sammenhæng derfor er der aktiveringsfunktioner.\n",
    "\n",
    "De aktiverings funktioner vi føler i burde kende er:\n",
    "\n",
    "- **Sigmoid**  \n",
    "  Sigmoid-funktionen bruges ofte i outputlaget, når vi vil have et output mellem 0 og 1. Den ser sådan ud:\n",
    "  $$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "  Den gør store negative tal til noget tæt på 0, og store positive tal til noget tæt på 1.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**  \n",
    "  ReLU er meget populær i skjulte lag, fordi den er simpel og virker godt i praksis:\n",
    "  $$\n",
    "  \\text{ReLU}(z) = \\max(0, z)\n",
    "  $$\n",
    "  Det betyder, at hvis $z$ er negativt, bliver output 0, ellers er det bare $z$.\n",
    "\n",
    "- **Leaky ReLU**  \n",
    "  Leaky ReLU minder om ReLU, men hvis $z$ er negativt, får vi en lille negativ værdi i stedet for 0. Det kan hjælpe netværket med at lære bedre:\n",
    "  $$\n",
    "  \\text{Leaky ReLU}(z) = \n",
    "  \\begin{cases}\n",
    "    z & \\text{hvis } z > 0 \\\\\n",
    "    0.01 \\cdot z & \\text{hvis } z \\leq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Tanh**  \n",
    "  Tanh-funktionen ligner sigmoid, men outputtet går fra -1 til 1:\n",
    "  $$\n",
    "  \\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "  $$\n",
    "  Den bruges nogle gange, hvis man vil have både negative og positive værdier ud.\n",
    "\n",
    "- **Softmax**  \n",
    "  Softmax bruges ofte i outputlaget, når vi har flere klasser og vil have sandsynligheder for hver klasse (f.eks. billedgenkendelse med flere kategorier). Softmax laver alle outputs om til tal mellem 0 og 1, som tilsammen giver 1:\n",
    "  $$\n",
    "  \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "  $$\n",
    "  hvor $z_i$ er outputtet for klasse $i$. Det gør det nemt at vælge den klasse med højest sandsynlighed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf0dd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Der er to forskellige typer af problemer hvor neurale netværk ofte bliver brugt, det er klassifikation og regression.\n",
    "\n",
    "Der er opgaver i begge dele da det er meget ligende hvordan man laver de to. For de to typer at problemer, giver det mening at bruge for hvis vi starter med klassifikation, så bruger du ofte Softmax eller sigmoid som din aktivierings funktion. Softmax bliver brugt til at give noget der minder om en sandsynlighed ud, så for vær klasse du har i din klassifikation vil den give sandsyndlighedden for at det er den klasse, mens sigmoid er brugt til det samme hvis du kun har en klasse.\n",
    "\n",
    "For regression, bruger du så sigmoid eller tanh som din output aktiverings funktion, dette kommer an på hvordan din data er spredt, foresemple hvis den går fra 0, til 1000, ville jeg normalisere det til 0 til 1 og bruge sigmoid, mens hvis der også er negative værdier, ville jeg bruge tanh.\n",
    "\n",
    "De sidste er ReLU og Leaky ReLU de er ofte brugt på hidden layers du kan dog også bruge de andre på hidden layers, den eneste som måske ikke giver så meget mening der er softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a543dc9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
