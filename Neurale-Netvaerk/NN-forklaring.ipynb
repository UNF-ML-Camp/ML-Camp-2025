{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63124e31",
   "metadata": {},
   "source": [
    "# Neurale Netværk\n",
    "\n",
    "## Introduktion\n",
    "\n",
    "Der er to sæt intro opgaver til neurale netværk, den første er for at lære jer hvordan I bruger pytorch, og hvordan i skriver de forskellige beregninger ind. Mens den anden er en low level implementation, hvor I selv skal lave et forward step skrevet kun med numpy. Dette prøver at give jer en bedre forståelse for hvad et Neuralt Netværk faktisk gør. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd24d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Der er to store felter af Machine Learning, supervised og unsupervised learning. Forskellen kommer om hvad du bruger til at træne, hvis du ikke har et konkret svar, er det du bruger unsupervised learning, det bliver f.eks. ofte brugt inden for clustering, hvor KNN er en måde at lave supervised clustering.\n",
    "\n",
    "Alle Neurale Netværk er en del af supervised learning.\n",
    "\n",
    "Lige et par ord du burde vide\n",
    "Loss: Det tal du bruger til at vudere hvor god din model er, og det du minimere for at gøre modellen bedre. Du kan også tale om en loss funktion, som er den måde du regner din loss.\n",
    "Epoch: Epochs er din optimerings cyklus så hver epoch laver du en optimering hvor du beregner lossen og optimere vægtene i din model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b23a1",
   "metadata": {},
   "source": [
    "## Matematikken\n",
    "\n",
    "\n",
    "Et neuralt netværk består af lag af \"neuroner\", hvor hvert neuron laver en simpel matematisk beregning. Hvert neuron modtager nogle tal (inputs), ganger dem med nogle vægte, derefter lægger dem sammen og sender resultatet videre gennem en aktiveringsfunktion.\n",
    "\n",
    "Hvis vi kalder inputtene $( x_1, x_2, ..., x_n )$ og vægtene $( w_1, w_2, ..., w_n )$, så regner et neuron sådan her:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i \\cdot x_i + b\n",
    "$$\n",
    "\n",
    "Her er:\n",
    "- $( w_i )$ vægtene (hvor vigtige de forskellige inputs er)\n",
    "- $( x_i )$ inputtene\n",
    "- $( b )$ er en bias (en slags ekstra justering)\n",
    "- $( z )$ er summen, som sendes videre\n",
    "\n",
    "Derefter bruger vi en aktiveringsfunktion, fx den populære \"sigmoid\":\n",
    "\n",
    "$$\n",
    "a = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Her er $( a )$ outputtet fra neuronet.\n",
    "\n",
    "\n",
    "Det kan du også se som en graf, hvor alle neuronerne er forbundet med vægte, til dem på det tidligere eller senere lag. Dette koncept kan du se på figuren under her.\n",
    "\n",
    "<img src=\"1_YL2a2dbDQ5754h_ktDj8mQ.webp\" style=\"max-width:600px;\">\n",
    "\n",
    "*Billedet er fra [denne artikel om neurale netværk](https://medium.com/ravenprotocol/everything-you-need-to-know-about-neural-networks-6fcc7a15cb4).*\n",
    "\n",
    "Alt hvad der er forklaret tidligere her, er for et lag, dog har de fleste netværk flere lag som er lagt i serie. Det betyder at når jeg regner outputtet fra det første lag, vil det direkte kunne bruges som input til det næste. De to eneste lag som er specille er input og output. Input laget er der hvor den orignalle data bliver givet ind. Hvor output laget er der hvor du for det ud du prøver at forudsige. Der er diskussioner om hvad Dybe Neurale Netværk(Deep Neural Networks) er, den defination jeg har hørt fra den mest pårlidlige kilde er: et dybt neuralt netværk er når der er mere en et lag. Det kommer af at i gamle dag kunne man kun beregne det backpropergation så optimeringen for et lag. Derfor er det historisk dybt når der er mere end et lag. Nu kan du smide lag på som du vil og selv det dårligste hardware kan sagtens kører 15-20 lag.\n",
    "\n",
    "Vi kommer ikke rigtig til at gå mere ind i backpropergation, men I kan tænke på at du prøver at regne hvad den ville give ud også ændre vægtene tilfældigt en lille smule også ser om det virker bedre. Så kommer resten bare hvordan du vælger hvilke vægte og hvor meget du skal ændre dem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53334a5",
   "metadata": {},
   "source": [
    "## Aktiverings Funktioner\n",
    "\n",
    "Vi skal bruge en aktiverings funktion for at kunne forudsige noget der ikke er linaert. Hvis relationen vi vil forudsige var en ret linje så ville vi ikke have brug for denne ekstra compleksitet, men de fleste problemer er ikke en linaer sammenhæng derfor er der aktiveringsfunktioner.\n",
    "\n",
    "De aktiverings funktioner vi føler i burde kende er:\n",
    "\n",
    "- **Sigmoid**  \n",
    "  Sigmoid-funktionen bruges ofte i outputlaget, når vi vil have et output mellem 0 og 1. Den ser sådan ud:\n",
    "  $$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "  Den gør store negative tal til noget tæt på 0, og store positive tal til noget tæt på 1.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**  \n",
    "  ReLU er meget populær i skjulte lag, fordi den er simpel og virker godt i praksis:\n",
    "  $$\n",
    "  \\text{ReLU}(z) = \\max(0, z)\n",
    "  $$\n",
    "  Det betyder, at hvis $z$ er negativt, bliver output 0, ellers er det bare $z$.\n",
    "\n",
    "- **Leaky ReLU**  \n",
    "  Leaky ReLU minder om ReLU, men hvis $z$ er negativt, får vi en lille negativ værdi i stedet for 0. Det kan hjælpe netværket med at lære bedre:\n",
    "  $$\n",
    "  \\text{Leaky ReLU}(z) = \n",
    "  \\begin{cases}\n",
    "    z & \\text{hvis } z > 0 \\\\\n",
    "    0.01 \\cdot z & \\text{hvis } z \\leq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Tanh**  \n",
    "  Tanh-funktionen ligner sigmoid, men outputtet går fra -1 til 1:\n",
    "  $$\n",
    "  \\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "  $$\n",
    "  Den bruges nogle gange, hvis man vil have både negative og positive værdier ud.\n",
    "\n",
    "- **Softmax**  \n",
    "  Softmax bruges ofte i outputlaget, når vi har flere klasser og vil have sandsynligheder for hver klasse (f.eks. billedgenkendelse med flere kategorier). Softmax laver alle outputs om til tal mellem 0 og 1, som tilsammen giver 1:\n",
    "  $$\n",
    "  \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "  $$\n",
    "  hvor $z_i$ er outputtet for klasse $i$. Det gør det nemt at vælge den klasse med højest sandsynlighed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf0dd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Der er to forskellige typer af problemer hvor neurale netværk ofte bliver brugt, det er klassifikation og regression.\n",
    "\n",
    "Der er opgaver i begge dele da det er meget ligende hvordan man laver de to. For de to typer at problemer, giver det mening at bruge for hvis vi starter med klassifikation, så bruger du ofte Softmax eller sigmoid som din aktivierings funktion. Softmax bliver brugt til at give noget der minder om en sandsynlighed ud, så for vær klasse du har i din klassifikation vil den give sandsyndlighedden for at det er den klasse, mens sigmoid er brugt til det samme hvis du kun har en klasse.\n",
    "\n",
    "For regression, bruger du så sigmoid eller tanh som din output aktiverings funktion, dette kommer an på hvordan din data er spredt, foresemple hvis den går fra 0, til 1000, ville jeg normalisere det til 0 til 1 og bruge sigmoid, mens hvis der også er negative værdier, ville jeg bruge tanh.\n",
    "\n",
    "De sidste er ReLU og Leaky ReLU de er ofte brugt på hidden layers du kan dog også bruge de andre på hidden layers, den eneste som måske ikke giver så meget mening der er softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a543dc9",
   "metadata": {},
   "source": [
    "## Overtræning\n",
    "\n",
    "På et tidspunkt kommer i nok til at støde ind i overtræning. Overtræning er når jeg model mister konteksten, så når model er virkelig god på træningsættet, men ikke virker så godt på jeres test set. Der er mange måder at komme uden om overtræning på, hvor jeg vil gennem gå nogle af dem i kan vælge at arbejde med her. Det skal siges at ikke alle de her metoder er lige til men det er ikke mening i skal kunne det hele, der er heller ikke nogen logisk rækkefølge på hvordan de kommer men de kan være brugbare.\n",
    "\n",
    "### Dropout\n",
    "Dropout er et lag som du kan tilføje under træningen som gør at noget af din trænings data bliver skiftet ud med nul, med den sandsynlighed som du giver ind. Det bliver brugt til at få netværket til at generalisere bedre, som er tilsvarende til at minske overtræningen. Dropout virker bedst når der er meget data.\n",
    "\n",
    "### Validation og Early stopping\n",
    "Den letteste måde at mindske overtræning er at stoppe træningen tidligere. En god måde at gøre det på er ved hjælp af et validerings set. Det gør at du kan regne din loss på validerings settet som holdes seperat fra træning, og ser om den optimering som du har lavet denne epoch, er god eller ej. Den måde du oftest bruger validering er at du vælger hvor mange gang træning ikke må være blevet bedre i validering settet til at stoppe tidligt også vælger du model med den bedste valideringsloss som din trænende model. Det kræver dog at du har en del data.\n",
    "\n",
    "### Cross validation\n",
    "Cross validation eller krydsvalidering, er en måde at bruge validering men uden du har ligeså meget data. Det du gør er at du splitter din data f.eks. i 5 dele, også laver du backpropagation(optimeringen), med en af dele som er validering i vær. Så bruger du alle 5 til at vudere hvordan du skal optimere modellen ved at tage gennemsnittet det din estimering. Det her er en kompleks måde at lave validering på men den virker godt hvis du kun har lidt data.\n",
    "\n",
    "\n",
    "Husk dog at den letteste måde at kommer over overtræning er at træne mindre, så du kan altid optimere hvor mange epoch du kører. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a7ba0",
   "metadata": {},
   "source": [
    "## Klassifikation\n",
    "\n",
    "Klassifikation handler om at forudsige hvilken kategori eller klasse et datapunkt tilhører. For eksempel:\n",
    "- Billedgenkendelse: Er dette billede en kat, hund eller fugl?\n",
    "- E-mail filtrering: Er denne e-mail spam eller ikke spam?\n",
    "- Sygdomsdiagnose: Har patienten sygdom A, B eller er rask?\n",
    "\n",
    "**Hvordan fungerer det:**\n",
    "- Dit neurale netværk outputter sandsynligheder for hver klasse\n",
    "- Du vælger klassen med højest sandsynlighed som dit svar\n",
    "- For binær klassifikation (2 klasser): brug sigmoid aktivering\n",
    "- For multi-class klassifikation (3+ klasser): brug softmax aktivering\n",
    "\n",
    "**Loss funktioner til klassifikation:**\n",
    "- **Binary Cross-Entropy** for 2 klasser\n",
    "- **Categorical Cross-Entropy** for flere klasser\n",
    "- Disse loss funktioner straffer modellen mere jo længere væk den er fra det rigtige svar\n",
    "\n",
    "**Hvad kan gå galt:**\n",
    "\n",
    "### 1. Ubalanceret data\n",
    "Hvis du har 95% klasse A og kun 5% klasse B, vil modellen lære at gætte klasse A hver gang og opnå 95% nøjagtighed, men være ubrugelig til at finde klasse B.\n",
    "- **Løsning:** Brug class weights, oversampling eller undersampling\n",
    "\n",
    "### 2. Overfitting til træningsdata\n",
    "Modellen husker træningseksemplerne i stedet for at lære mønstre.\n",
    "- **Symptom:** Høj træningsnøjagtighed, lav test-nøjagtighed\n",
    "- **Løsning:** Dropout, early stopping, eller mere data\n",
    "\n",
    "### 3. For få træningsdata\n",
    "Komplekse neurale netværk har brug for meget data for at lære ordentligt.\n",
    "- **Løsning:** Data augmentation, transfer learning, eller simplere model\n",
    "\n",
    "### 4. Forkerte labels\n",
    "Hvis dine træningsdata har forkerte labels, lærer modellen forkerte mønstre.\n",
    "- **Løsning:** Manuel gennemgang af data, crowd-sourcing af labels\n",
    "\n",
    "### 5. Bias i data\n",
    "Hvis træningsdata ikke repræsenterer den virkelige verden, vil modellen fejle på nye data.\n",
    "- **Eksempel:** Træne på billeder kun fra sommeren, teste på vinterbilleder\n",
    "- **Løsning:** Mere repræsentativ data indsamling\n",
    "\n",
    "### 6. Threshold problemer\n",
    "Ved binær klassifikation skal du vælge en grænse (f.eks. 0.5) for hvornår du klassificerer som klasse 1.\n",
    "- Standard 0.5 er ikke altid optimalt\n",
    "- **Løsning:** Juster threshold baseret på validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9394648f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "Regression handler om at forudsige kontinuerlige numeriske værdier i stedet for kategorier. For eksempel:\n",
    "- Huspriser: Hvad koster et hus med disse egenskaber?\n",
    "- Temperaturer: Hvor varmt bliver det i morgen?\n",
    "- Aktiekurser: Hvad vil aktien koste næste uge?\n",
    "- Alder: Hvor gammel er personen på dette billede?\n",
    "\n",
    "**Hvordan fungerer det:**\n",
    "- Dit neurale netværk outputter en eller flere numeriske værdier\n",
    "- Ingen sandsynligheder - bare direkte tal\n",
    "- For output mellem 0-1: brug sigmoid aktivering\n",
    "- For output mellem -1 og 1: brug tanh aktivering  \n",
    "- For ubegrænset output: brug linear aktivering (ingen aktivering)\n",
    "\n",
    "**Loss funktioner til regression:**\n",
    "- **Mean Squared Error (MSE)** - mest almindelige\n",
    "- **Mean Absolute Error (MAE)** - mere robust overfor outliers\n",
    "\n",
    "**Hvad kan gå galt:**\n",
    "\n",
    "### 1. Forkert skala på data\n",
    "Hvis dine input features har vidt forskellige skalaer (f.eks. alder 0-100 vs indkomst 0-1.000.000), kan modellen fokusere for meget på de store tal.\n",
    "- **Løsning:** Normaliser eller standardiser dine data\n",
    "\n",
    "### 2. Outliers i data\n",
    "Ekstreme værdier kan forvrænge hele modellen, især med MSE loss.\n",
    "- **Symptom:** Modellen underpræsterer på \"normale\" eksempler\n",
    "- **Løsning:** Fjern outliers, brug MAE loss, eller robust preprocessing\n",
    "\n",
    "### 3. Non-lineære sammenhænge\n",
    "Hvis forholdet mellem input og output er meget komplekst, kan simple netværk ikke fange det.\n",
    "- **Løsning:** Dybere netværk, flere neuroner, eller feature engineering\n",
    "\n",
    "### 4. Begrænsede output værdier\n",
    "Hvis dit target altid er positivt (f.eks. priser), men modellen kan outputte negative tal.\n",
    "- **Løsning:** Brug sigmoid/ReLU aktivering, eller log-transform targets\n",
    "\n",
    "### 5. Heteroskedastisk støj\n",
    "Hvis fejlen varierer afhængigt af input-værdien (f.eks. dyrere huse har mere variable priser).\n",
    "- **Symptom:** Modellen er god til nogle værdier, dårlig til andre\n",
    "- **Løsning:** Weighted loss functions eller separate modeller for forskellige områder\n",
    "\n",
    "### 6. Temporal dependencies\n",
    "Hvis dine data har tidsafhængigheder (f.eks. aktiekurser), ignorerer simple netværk historik.\n",
    "- **Løsning:** Brug recurrent neural networks (RNN/LSTM) eller time-series features\n",
    "\n",
    "### 7. Multi-output regression\n",
    "Når du forudsiger flere værdier samtidig, kan nogle outputs dominere træningen.\n",
    "- **Symptom:** God på nogle outputs, dårlig på andre\n",
    "- **Løsning:** Balancer loss weights eller separate modeller\n",
    "\n",
    "**Evaluering af regression:**\n",
    "- **R² score:** Hvor meget varians forklarer modellen? (1.0 = perfekt)\n",
    "- **RMSE:** Root Mean Squared Error - gennemsnitlig fejl i samme enheder som target\n",
    "- **MAE:** Mean Absolute Error - mere intuitivt end RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e26622",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
