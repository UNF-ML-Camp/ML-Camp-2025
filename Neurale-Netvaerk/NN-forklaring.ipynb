{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63124e31",
   "metadata": {},
   "source": [
    "# Neurale Netværk\n",
    "\n",
    "Dette modul er lavet for at give en bedre indblik til hvordan Neurale Netværk fungerer, og hvordan de kan implementeres i Python. Neurale netværke er en vigtig værktøj i maskinlæring, og de bruges i mange forskellige applikationer, fra billedgenkendelse til naturlig sprogbehandling. Man kan sige at neurale netværk består af en masse små matematiske operationer, som tilsammen kan løse komplekse opgaver.\n",
    "\n",
    "Udover dette, så snakker vi også om hvad for nogle ting man skal have i mente når man træner et neuralt nætværk.\n",
    "\n",
    "## Opgaver\n",
    "\n",
    "Til dette modul, er der to sæt intro opgaver til neurale netværk, den første er for at lære jer hvordan I bruger pytorch, og hvordan i skriver de forskellige beregninger ind. Mens den anden er en low level implementation, hvor I selv skal lave et forward step skrevet kun med numpy. Dette prøver at give jer en bedre forståelse for hvad et Neuralt Netværk faktisk gør. \n",
    "\n",
    "## Hvad kan Neurale Netværk bruges til?\n",
    "Indenfor maskinlæring er der 2 primære typer af opgaver, som neurale netværk kan bruges til:\n",
    "- Supervised Learning: Her lærer neurale netværk at forudsige en output baseret på en input. Det kan være alt fra at klassificere billeder til at forudsige priser på aktier. En karakteristik ved supervised learning er at der er et kendt 'label' for hver input som beskriver hvad inputtet reelt set er.\n",
    "- Unsupervised Learning: Her lærer neurale netværk at finde mønstre i data uden at have et kendt label. Det kan være alt fra at gruppere billeder til at finde skjulte strukturer i data. En karakteristik ved unsupervised learning er at der ikke er et kendt 'label' for hver input, og netværket skal selv finde ud af hvad der er vigtigt.\n",
    "\n",
    "Der er også andre underfelter, såsom self-supervised learning, generative modellering og andet, men de to ovenstående er de mest almindelige.\n",
    "\n",
    "## Matematikken\n",
    "\n",
    "For at forstå hvordan neurale netværk fungerer, er det vigtigt at have en grundlæggende forståelse for den matematik, der ligger bag. Neurale netværk er baseret på lineær algebra og calculus, og de bruger begreber som matrixmultiplikation, vektorer og aktiveringsfunktioner.\n",
    "\n",
    "Et neuralt netværk består af lag af \"neuroner\", hvor hvert neuron laver en simpel matematisk beregning. Hvert neuron modtager nogle tal (inputs), ganger dem med nogle vægte, derefter lægger dem sammen og sender resultatet videre gennem en aktiveringsfunktion.\n",
    "\n",
    "Hvis vi kalder inputtene $( x_1, x_2, ..., x_n )$ og vægtene $( w_1, w_2, ..., w_n )$, så regner et neuron sådan her:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i \\cdot x_i + b\n",
    "$$\n",
    "\n",
    "Her er:\n",
    "- $( x_i )$ inputtene\n",
    "- $( w_i )$ vægtene (hvor vigtige de forskellige inputs er)\n",
    "- $( b )$ er en bias (en slags ekstra justering)\n",
    "- $( z )$ er summen, som sendes videre\n",
    "\n",
    "Derefter bruger vi en aktiveringsfunktion, fx den populære \"sigmoid\":\n",
    "\n",
    "$$\n",
    "a = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Her er $( a )$ outputtet fra neuronet.\n",
    "\n",
    "Sigmoid funktionen ser sådan her ud:\n",
    "\n",
    "<img src=\"billeder/1694183259537.png\" style=\"max-width:600px;\">\n",
    "\n",
    "# Netværket\n",
    "\n",
    "Alt det tidligere beskriver en enkelt neuron, hvori et neural netværk består af mange neuroner struktureret i lag. Hvert neuron har deres egne vægte og bias, som tillader hvert enkelt neuron at lære forskellige ting, som tillader netværket at lære komplekse mønstre i data.\n",
    "\n",
    "De fleste netværk flere lag som er lagt i serie. Det betyder at når man regner outputtet fra det første lag, vil det direkte kunne bruges som input til det næste. De to eneste lag som er specille er input og output. Input laget er der hvor den orignalle data bliver givet ind. Hvor output laget er der hvor du for det ud du prøver at forudsige. Et Deep Neural netværk er et netværk med flere lag, ergo at den er 'dyb'.\n",
    "\n",
    "Herunder ses et billede af et neuralt netværk:\n",
    "\n",
    "<img src=\"billeder/1_YL2a2dbDQ5754h_ktDj8mQ.webp\" style=\"max-width:600px;\">\n",
    "\n",
    "*Billedet er fra [denne artikel om neurale netværk](https://medium.com/ravenprotocol/everything-you-need-to-know-about-neural-networks-6fcc7a15cb4).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53334a5",
   "metadata": {},
   "source": [
    "## Deep Dive ind på Aktiverings Funktioner\n",
    "\n",
    "Vi skal bruge en aktiverings funktion for at kunne forudsige noget der ikke er linear. Hvis relationen vi vil forudsige var en ret linje så ville vi ikke have brug for denne ekstra compleksitet, men de fleste problemer er ikke en linaer sammenhæng derfor er der aktiveringsfunktioner.\n",
    "\n",
    "De aktiverings funktioner vi føler i burde kende er:\n",
    "\n",
    "- **Sigmoid**  \n",
    "  Sigmoid-funktionen bruges ofte i outputlaget, når vi vil have et output mellem 0 og 1. Den ser sådan ud:\n",
    "  $$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "  Den gør store negative tal til noget tæt på 0, og store positive tal til noget tæt på 1.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**  \n",
    "  ReLU er meget populær i skjulte lag, fordi den er simpel og virker godt i praksis:\n",
    "  $$\n",
    "  \\text{ReLU}(z) = \\max(0, z)\n",
    "  $$\n",
    "  Det betyder, at hvis $z$ er negativt, bliver output 0, ellers er det bare $z$.\n",
    "\n",
    "- **Leaky ReLU**  \n",
    "  Leaky ReLU minder om ReLU, men hvis $z$ er negativt, får vi en lille negativ værdi i stedet for 0. Det kan hjælpe netværket med at lære bedre:\n",
    "  $$\n",
    "  \\text{Leaky ReLU}(z) = \n",
    "  \\begin{cases}\n",
    "    z & \\text{hvis } z > 0 \\\\\n",
    "    0.01 \\cdot z & \\text{hvis } z \\leq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Tanh**  \n",
    "  Tanh-funktionen ligner sigmoid, men outputtet går fra -1 til 1:\n",
    "  $$\n",
    "  \\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "  $$\n",
    "  Den bruges nogle gange, hvis man vil have både negative og positive værdier ud.\n",
    "\n",
    "- **Softmax**  \n",
    "  Softmax bruges ofte i outputlaget, når vi har flere klasser og vil have sandsynligheder for hver klasse (f.eks. billedgenkendelse med flere kategorier). Softmax laver alle outputs om til tal mellem 0 og 1, som tilsammen giver 1:\n",
    "  $$\n",
    "  \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "  $$\n",
    "  hvor $z_i$ er outputtet for klasse $i$. Det gør det nemt at vælge den klasse med højest sandsynlighed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf0dd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Der er to forskellige typer af problemer hvor neurale netværk ofte bliver brugt, det er klassifikation og regression.\n",
    "\n",
    "Der er opgaver i begge dele da det er meget ligende hvordan man laver de to. Hvis vi starter med klassifikation, så bruger du ofte Softmax eller Sigmoid som din aktiverings funktion. Softmax bliver brugt til at give noget der minder om en sandsynlighed ud, så for hver klasse som man vil klassifer så vil den give sandsyndlighederne for at inputtet tilhører hvert klasse, mens Sigmoid er brugt til det samme hvis du kun har en klasse.\n",
    "\n",
    "For regression, bruger man oftest Sigmoid eller Tanh som din output aktiverings funktion, dette kommer an på hvordan din data er spredt, fx hvis den går fra 0, til 1000, så kan man normalisere det til 0 til 1 og bruge Sigmoid, mens hvis der også er negative værdier, ville jeg bruge Tanh.\n",
    "\n",
    "De sidste er ReLU og Leaky ReLU de er ofte brugt på hidden layers du kan dog også bruge de andre på hidden layers, den eneste som måske ikke giver så meget mening der er softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a543dc9",
   "metadata": {},
   "source": [
    "## Træning af Neurale Netværk\n",
    "\n",
    "Når man træner et neurale netværk, så er det værd at have følgende i mente, ellers kan man ende med suboptimale resultater.\n",
    "\n",
    "### Overtræning\n",
    "På et tidspunkt kommer i nok til at støde ind i overtræning. Overtræning er når en model mister konteksten omkring dataen, som sker når modellen bliver virkelig god til at løse træningsdataen, men ikke virker så godt på testdataen. Der er mange måder at komme uden om overtræning på, hvor jeg vil gennem gå nogle af dem i kan vælge at arbejde med her. Det skal siges at ikke alle de her metoder er lige til men det er ikke mening i skal kunne det hele, der er heller ikke nogen logisk rækkefølge på hvordan de kommer men de kan være brugbare.\n",
    "\n",
    "#### Dropout\n",
    "Dropout er et lag som du kan tilføje under træningen som gør at noget af din trænings data bliver skiftet ud med nul, med den sandsynlighed som du giver ind. Det bliver brugt til at få netværket til at generalisere bedre, som er tilsvarende til at minske overtræningen. Dropout virker bedst når der er meget data.\n",
    "\n",
    "#### Validation og Early stopping\n",
    "Den letteste måde at mindske overtræning er at stoppe træningen tidligere. En god måde at gøre det på er ved hjælp af et validerings set. Det gør at du kan regne din loss på validerings settet som holdes seperat fra træning, og ser om den optimering som du har lavet denne epoch, er god eller ej. Den måde du oftest bruger validering er at du vælger hvor mange gang træning ikke må være blevet bedre i validering settet til at stoppe tidligt også vælger du model med den bedste valideringsloss som din trænende model. Det kræver dog at du har en del data.\n",
    "\n",
    "#### Cross validation\n",
    "Cross validation eller krydsvalidering, er en måde at bruge validering men uden du har ligeså meget data. Det du gør er at du splitter din data f.eks. i 5 dele, også laver du backpropagation(optimeringen), med en af dele som er validering i vær. Så bruger du alle 5 til at vudere hvordan du skal optimere modellen ved at tage gennemsnittet det din estimering. Det her er en kompleks måde at lave validering på men den virker godt hvis du kun har lidt data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a7ba0",
   "metadata": {},
   "source": [
    "## Klassifikation\n",
    "\n",
    "Klassifikation handler om at forudsige hvilken kategori eller klasse et datapunkt tilhører. For eksempel:\n",
    "- Billedgenkendelse: Er dette billede en kat, hund eller fugl?\n",
    "- E-mail filtrering: Er denne e-mail spam eller ikke spam?\n",
    "- Sygdomsdiagnose: Har patienten sygdom A, B eller er rask?\n",
    "\n",
    "**Hvordan fungerer det:**\n",
    "- Dit neurale netværk outputter sandsynligheder for hver klasse\n",
    "- Du vælger klassen med højest sandsynlighed som dit svar\n",
    "- For binær klassifikation (2 klasser): brug sigmoid aktivering\n",
    "- For multi-class klassifikation (3+ klasser): brug softmax aktivering\n",
    "\n",
    "**Loss funktioner til klassifikation:**\n",
    "- **Binary Cross-Entropy** for 2 klasser\n",
    "- **Categorical Cross-Entropy** for flere klasser\n",
    "- Disse loss funktioner straffer modellen mere jo længere væk den er fra det rigtige svar\n",
    "\n",
    "**Hvad kan gå galt:**\n",
    "\n",
    "### 1. Ubalanceret data\n",
    "Hvis du har 95% klasse A og kun 5% klasse B, vil modellen lære at gætte klasse A hver gang og opnå 95% nøjagtighed, men være ubrugelig til at finde klasse B.\n",
    "- **Løsning:** Brug class weights, oversampling eller undersampling\n",
    "\n",
    "### 2. Overfitting til træningsdata\n",
    "Modellen husker træningseksemplerne i stedet for at lære mønstre.\n",
    "- **Symptom:** Høj træningsnøjagtighed, lav test-nøjagtighed\n",
    "- **Løsning:** Dropout, early stopping, eller mere data\n",
    "\n",
    "### 3. For få træningsdata\n",
    "Komplekse neurale netværk har brug for meget data for at lære ordentligt.\n",
    "- **Løsning:** Data augmentation, transfer learning, eller simplere model\n",
    "\n",
    "### 4. Forkerte labels\n",
    "Hvis dine træningsdata har forkerte labels, lærer modellen forkerte mønstre.\n",
    "- **Løsning:** Manuel gennemgang af data, crowd-sourcing af labels\n",
    "\n",
    "### 5. Bias i data\n",
    "Hvis træningsdata ikke repræsenterer den virkelige verden, vil modellen fejle på nye data.\n",
    "- **Eksempel:** Træne på billeder kun fra sommeren, teste på vinterbilleder\n",
    "- **Løsning:** Mere repræsentativ data indsamling\n",
    "\n",
    "### 6. Threshold problemer\n",
    "Ved binær klassifikation skal du vælge en grænse (f.eks. 0.5) for hvornår du klassificerer som klasse 1.\n",
    "- Standard 0.5 er ikke altid optimalt\n",
    "- **Løsning:** Juster threshold baseret på validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9394648f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "Regression handler om at forudsige kontinuerlige numeriske værdier i stedet for kategorier. For eksempel:\n",
    "- Huspriser: Hvad koster et hus med disse egenskaber?\n",
    "- Temperaturer: Hvor varmt bliver det i morgen?\n",
    "- Aktiekurser: Hvad vil aktien koste næste uge?\n",
    "- Alder: Hvor gammel er personen på dette billede?\n",
    "\n",
    "**Hvordan fungerer det:**\n",
    "- Dit neurale netværk outputter en eller flere numeriske værdier\n",
    "- Ingen sandsynligheder - bare direkte tal\n",
    "- For output mellem 0-1: brug sigmoid aktivering\n",
    "- For output mellem -1 og 1: brug tanh aktivering  \n",
    "- For ubegrænset output: brug linear aktivering (ingen aktivering)\n",
    "\n",
    "**Loss funktioner til regression:**\n",
    "- **Mean Squared Error (MSE)** - mest almindelige\n",
    "- **Mean Absolute Error (MAE)** - mere robust overfor outliers\n",
    "\n",
    "**Hvad kan gå galt:**\n",
    "\n",
    "### 1. Forkert skala på data\n",
    "Hvis dine input features har vidt forskellige skalaer (f.eks. alder 0-100 vs indkomst 0-1.000.000), kan modellen fokusere for meget på de store tal.\n",
    "- **Løsning:** Normaliser eller standardiser dine data\n",
    "\n",
    "### 2. Outliers i data\n",
    "Ekstreme værdier kan forvrænge hele modellen, især med MSE loss.\n",
    "- **Symptom:** Modellen underpræsterer på \"normale\" eksempler\n",
    "- **Løsning:** Fjern outliers, brug MAE loss, eller robust preprocessing\n",
    "\n",
    "### 3. Non-lineære sammenhænge\n",
    "Hvis forholdet mellem input og output er meget komplekst, kan simple netværk ikke fange det.\n",
    "- **Løsning:** Dybere netværk, flere neuroner, eller feature engineering\n",
    "\n",
    "### 4. Begrænsede output værdier\n",
    "Hvis dit target altid er positivt (f.eks. priser), men modellen kan outputte negative tal.\n",
    "- **Løsning:** Brug sigmoid/ReLU aktivering, eller log-transform targets\n",
    "\n",
    "### 5. Heteroskedastisk støj\n",
    "Hvis fejlen varierer afhængigt af input-værdien (f.eks. dyrere huse har mere variable priser).\n",
    "- **Symptom:** Modellen er god til nogle værdier, dårlig til andre\n",
    "- **Løsning:** Weighted loss functions eller separate modeller for forskellige områder\n",
    "\n",
    "### 6. Temporal dependencies\n",
    "Hvis dine data har tidsafhængigheder (f.eks. aktiekurser), ignorerer simple netværk historik.\n",
    "- **Løsning:** Brug recurrent neural networks (RNN/LSTM) eller time-series features\n",
    "\n",
    "### 7. Multi-output regression\n",
    "Når du forudsiger flere værdier samtidig, kan nogle outputs dominere træningen.\n",
    "- **Symptom:** God på nogle outputs, dårlig på andre\n",
    "- **Løsning:** Balancer loss weights eller separate modeller\n",
    "\n",
    "**Evaluering af regression:**\n",
    "- **R² score:** Hvor meget varians forklarer modellen? (1.0 = perfekt)\n",
    "- **RMSE:** Root Mean Squared Error - gennemsnitlig fejl i samme enheder som target\n",
    "- **MAE:** Mean Absolute Error - mere intuitivt end RMSE"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
