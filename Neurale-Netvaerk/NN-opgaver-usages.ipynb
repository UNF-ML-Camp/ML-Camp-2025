{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32ca156",
   "metadata": {},
   "source": [
    "# Bagtanker\n",
    "\n",
    "Her er opgaverne som skal give jer en introduktion til hvor i kan implementere et simplt neuralt netværk ved hjælp af pytorch.\n",
    "\n",
    "Der kommer til at være noget general struktur i koden for at i hurtigere kan komme igang med at lave nogle netværk og prøve på dataen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e35cda",
   "metadata": {},
   "source": [
    "## Pakker\n",
    "Her er alle pakker som i burde skulle bruge til at få et virkede neuralt netværk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ec683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mimodekj/.conda/envs/ML/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "!pip install -q kagglehub corner seaborn\n",
    "from NN_import import load_dataset, device, train_model\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658aedbb",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Her har vi sadt 5 dataset ind som er formateret til I bare kan bruge det. En af de vigstige ting inden for Machine Learning er at formatere og rengør sin data. Her er gjort det for jer fordi ellers skulle vi bruge det meste af campen på hvordan man gør det.\n",
    "\n",
    "De 5 dataset er:\n",
    "\n",
    "### Partikle fysik\n",
    "Det her dataset beskriver et electron par der bevæger sig igennem en partikle detektor. Her er ideen at i skal bestemme den invariante masse(energi) som partikle par'et har. Dette er et regressions dataset\n",
    "\n",
    "### Vejr data\n",
    "Det her er et dataset for at i skal kunne forud sige hvad den aparent tempertur er ud fra temperturen, luftfugtigheden og andre variabler. Det her er også et regressions dataset.\n",
    "\n",
    "### Studerende data\n",
    "Her er et dataset hvor studerendes karaktere er givet udfra nogle katagoriere som hvor meget hjælp de for at deres forældre, hvor meget de studere og deres alder. Det her kan både bruges som regression og klassifikation. Regressions delen er fra GPA, mens klassifikations dele er fra hvilken katagri deres GPA er i.\n",
    "\n",
    "### Avocado data\n",
    "Dette er et småt dataset hvor avocadoer er vuderet på deres modenhed, udfra ekterne faktore som hårdhed. Det her er et klassifikation dataset.\n",
    "\n",
    "### Diabetes data\n",
    "Diabetes datasetet handler om at forudsige om en person har diabetes, udfra deres levestil. Dette er et klassifikations dataset. \n",
    "\n",
    "\n",
    "\n",
    "### Kørslsen af dataen\n",
    "For at vælge hvilket dataset i vil bruge skal i skrive navnet på datasættet og sætte variablen dataset, enten \"particle\", \"weather\", \"grades\", \"avocado\" eller \"diabetes\". Den er sadt til at være \"particle\" som default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d7c445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (100000, 19)\n",
      "After removing NaN values: (99915, 19)\n",
      "Removed 85 rows with NaN values\n",
      "Dataset: particle\n",
      "Input size: 16\n",
      "Output size: 1\n",
      "Training samples: 80930\n",
      "Validation samples: 9992\n",
      "Test samples: 8993\n",
      "Feature names: ['E1', 'px1 ', 'py1', 'pz1', 'pt1', 'eta1', 'phi1', 'Q1', 'E2', 'px2', 'py2', 'pz2', 'pt2', 'eta2', 'phi2', 'Q2']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset with visualization\n",
    "dataset_name = \"particle\"  # Options: \"particle\", \"weather\", \"grades\", \"avocado\", \"diabetes\"\n",
    "validation = True # Set to True to include validation set\n",
    "visualize = False  # Set to True to see data visualizations\n",
    "\n",
    "data = load_dataset(dataset_name, validation=validation, visualize=visualize)\n",
    "\n",
    "# Extract commonly used variables for convenience\n",
    "X_train_tensor = data['X_train']\n",
    "X_test_tensor = data['X_test']\n",
    "train_targets_tensor = data['train_targets']  \n",
    "test_targets_tensor = data['test_targets']    \n",
    "input_size = data['input_size']\n",
    "output_size = data['output_size']\n",
    "\n",
    "# Check if there are any NaN values in the training data\n",
    "if torch.isnan(X_train_tensor).any():\n",
    "    print(\"Warning: Training data contains NaN values. Consider preprocessing to handle them.\")\n",
    "if torch.isnan(train_targets_tensor).any():\n",
    "    print(\"Warning: Training targets contain NaN values. Consider preprocessing to handle them.\")\n",
    "\n",
    "# Handle grades dataset with two targets\n",
    "if dataset_name == \"grades\":\n",
    "    train_targets_classification = data['train_targets2']  # GradeClass (classification)\n",
    "    test_targets_classification = data['test_targets2']    # GradeClass (classification)\n",
    "    if validation:\n",
    "        val_targets_classification = data['val_targets2']   # GradeClass (classification)\n",
    "    print(f\"Regression target (GPA): {train_targets_tensor.shape}\")\n",
    "    print(f\"Classification target (GradeClass): {train_targets_classification.shape}\")\n",
    "\n",
    "if validation:\n",
    "    X_val_tensor = data['X_val']\n",
    "    val_targets_tensor = data['val_targets']  # GPA (regression)\n",
    "\n",
    "print(f\"Dataset: {data['dataset_name']}\")\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"Training samples: {X_train_tensor.shape[0]}\")\n",
    "if validation:\n",
    "    print(f\"Validation samples: {X_val_tensor.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_tensor.shape[0]}\")\n",
    "print(f\"Feature names: {data['feature_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f22fc",
   "metadata": {},
   "source": [
    "Her er det mest simple netværk I kan lave det har kun 1 input og 1 output lag. Når du gerne vil lave flere lag skal du bare sørger for at størrelserne passer, så i stedet for output_size kan du bruge 64 også lave et lag til som er 64, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0bbf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Her skal du definere de lag, aktiveringsfunktioner og andre komponenter i dit neurale netværk\n",
    "        # For eksempel:\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Her skal du definere, hvordan data passerer gennem netværket\n",
    "        # For regression, we don't want ReLU on the output layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229bbb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training regression model for 100 epochs...\n",
      "Device: cuda\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model = SimpleNN(input_size, output_size).to(device)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m history, model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mregression\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/ML_Camp/ML-Camp-2025/Neurale-Netvaerk/NN_import.py:412\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, X_train, y_train, X_val, y_val, X_test, y_test, task_type, epochs, learning_rate, batch_size, early_stopping_patience, print_every)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m    411\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# Handle output dimensions\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task_type == \u001b[33m'\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSimpleNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Her skal du definere, hvordan data passerer gennem netværket\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# For regression, we don't want ReLU on the output layer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(x)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ML/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example usage cell - FIXED VERSION:\n",
    "# Create a simple model\n",
    "model = SimpleNN(input_size, output_size).to(device)\n",
    "\n",
    "# Train the model\n",
    "history, model = train_model(model, X_train_tensor, train_targets_tensor, task_type='regression',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a6efc",
   "metadata": {},
   "source": [
    "### Opgave\n",
    "\n",
    "Nu skal I proeve at lave jeres eget netvaerk, og proeve at traene det paa et af datasetne. Jeres foeste netvaerk skal vaere to til fire lag, med en ReLU mellem vaert lag, ogsaa en output aktiverings funktion som I synes der giver mening. Ogsaa skal i bruge train_model funktionen til at traene jeres netvaerk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686101d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JeresNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dimension=64, hidden_layers=2):\n",
    "        super(JeresNN, self).__init__()\n",
    "        # Her skal du definere de lag, aktiveringsfunktioner og andre komponenter i dit neurale netværk\n",
    "\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "        # TODO Indsæt din kode her\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Her skal du definere, hvordan data passerer gennem netværket\n",
    "        # TODO Indsæt din kode her\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf5883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c373978",
   "metadata": {},
   "source": [
    "### Opgave\n",
    "\n",
    "Nu naar jeres netvaerk virker skal i skrive jeres eget traenings loop. \n",
    "foerst skal I vaelge hvilken loss funktion i skal bruge, her kan i vaelge mellem MSE for regression eller Cross Entropy for klassifikation.\n",
    "Her efter skal i vaelge en learning rate, max_epochs og batch size.\n",
    "Saa har vi valgt at i skal bruge Adam som optimizer her.\n",
    "\n",
    "Ideen bag et traenings loop er at vi gerne vil koere alt dataen igennem et antal gange lig hvor mange epoch der er.\n",
    "Derfor til at starte med skal i lave en for lykke som koere max epochs gange.\n",
    "\n",
    "Det foerste i skal koere er model.train() for at faa jeres model til at gaa ind i traening mode.\n",
    "\n",
    "Her skal i lave en variable som er den nuvaerende loss hvis I gerne vil have en ide om hvor godt det virker.\n",
    "\n",
    "Saa skal i koere endnu et loop her for alle X og y i train_loader. Det betyder en for lykke hvor der er to variabler X, y der hvor du normalt kun ville have en f.eks. i.\n",
    "\n",
    "Nu skal vi sikre at jeres data X og y er paa det rigtige device, som I goer ved at sige X = X.to(device) og ligende for y.\n",
    "\n",
    "Nu kommer den mest klassike del af et traenings loop.\n",
    "\n",
    "Foerst bruger vi optimizer.zero_grad(), for at genstarte eventuelle tidligere optimizeringere med de nye vardier, som er fundet.\n",
    "\n",
    "Saa skal vi vudere hvad modellen goer lige nu, ved at koere model(vores X).\n",
    "\n",
    "Nu skal vi bruge loss = criterion(modellens output, vores y) som er batches loss.\n",
    "\n",
    "Saa bruger vi loss.backwards() for at finde hvilken optimizering der giver mening ved at differencere loss funktionen.\n",
    "\n",
    "Saa bruger vi optimizer.step() til at lave en optimizering.\n",
    "\n",
    "Hvis du gerne vil have et output saa skal du plusse loss.item() paa din variable\n",
    "\n",
    "Her skal du saa ud i det stoerre loop, hvor du kan skrive ud hvad din loss er per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the loss function\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression tasks\n",
    "#criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "max_epochs = 100\n",
    "\n",
    "# Initialize the model, optimizer, and data loader\n",
    "model = JeresNN(input_size, output_size).to(device)  # Initialize your custom model\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, train_targets_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515e0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df2382e5",
   "metadata": {},
   "source": [
    "### Opgave\n",
    "\n",
    "Nu har i det mest basalse om hvordan man koere traener et neuralt netvaerk, ssaa nu skal vi koere det. det er som udgangs punkt bare at sige model(Din data her). Hvor det vigtige er i skriver model.eval() inden for at gaa ud af traenings mode.\n",
    "\n",
    "Nu skal i proeve se hvor lang jeres forud sagte test data er fra jeres data.\n",
    "For regression, ville jer bruge standard afvielse ved torch.std(forudsigelse, rigtig) og for lidt mere granualistet saa torch.sqrt((forudsiglese - rigtig)**2).\n",
    "\n",
    "For Klassifikation er der kun et godt maal, som er torch.mean( (forudsigelse > 0.5) == rigtig ) for at give hvor mange faktionen af rigtigt klassifikeret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401a4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db98475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d04f5a2",
   "metadata": {},
   "source": [
    "### Opgave\n",
    "\n",
    "Leg, den bedste maade at faa en god idee om hvad der virker i machine learning er at goere 42 millader gange saa derfor synes jeg bare i skal lege med det. Her kan i proeve at implemetnere validering, early stopping, lave traening med en masse netvaerk, proeve nogle nye lag, tilfoeje dropout, jeres mulighedder er endeloese. I skal bare sproegere hvis der er noget I har problemer med, eller noget mere I gerne vil proeve.\n",
    "\n",
    "Hvis i gerne vil kan train_model baade vudere og har validering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b28b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training regression model for 100 epochs...\n",
      "Device: cuda\n",
      "--------------------------------------------------\n",
      "Epoch 10/100 - Train Loss: 0.6141, Train RMSE: 0.7837, Val Loss: 0.5941, Val RMSE: 0.7708\n",
      "Early stopping at epoch 19\n",
      "Loaded best model from validation\n",
      "\n",
      "Test Results - Loss: 0.6171, RMSE: 0.7856\n"
     ]
    }
   ],
   "source": [
    "history, model = train_model(model, X_train_tensor, train_targets_tensor, X_val=X_val_tensor if validation else None,\n",
    "                             y_val=val_targets_tensor if validation else None,\n",
    "                             X_test=X_test_tensor, y_test=test_targets_tensor,\n",
    "                             task_type='regression',  # Change to 'classification' if needed\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba488f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18717a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab13e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
